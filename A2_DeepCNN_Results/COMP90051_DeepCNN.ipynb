{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-10T07:26:47.881128Z",
     "iopub.status.busy": "2025-10-10T07:26:47.880734Z",
     "iopub.status.idle": "2025-10-10T07:26:47.891093Z",
     "shell.execute_reply": "2025-10-10T07:26:47.890467Z",
     "shell.execute_reply.started": "2025-10-10T07:26:47.881104Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/images-shoes/train_B.npz\n",
      "/kaggle/input/images-shoes/test.npz\n",
      "/kaggle/input/images-shoes/train_A.npz\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T00:15:27.871545Z",
     "iopub.status.busy": "2025-10-11T00:15:27.871001Z",
     "iopub.status.idle": "2025-10-11T00:15:27.875539Z",
     "shell.execute_reply": "2025-10-11T00:15:27.874814Z",
     "shell.execute_reply.started": "2025-10-11T00:15:27.871525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, random, numpy as np\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import pickle\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T00:15:30.472819Z",
     "iopub.status.busy": "2025-10-11T00:15:30.472186Z",
     "iopub.status.idle": "2025-10-11T00:15:30.476818Z",
     "shell.execute_reply": "2025-10-11T00:15:30.476261Z",
     "shell.execute_reply.started": "2025-10-11T00:15:30.472796Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:33:14.309188Z",
     "iopub.status.busy": "2025-10-10T22:33:14.308677Z",
     "iopub.status.idle": "2025-10-10T22:33:14.314834Z",
     "shell.execute_reply": "2025-10-10T22:33:14.314072Z",
     "shell.execute_reply.started": "2025-10-10T22:33:14.309165Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#creates stratified k-fold splits, that preserve class distribution\n",
    "#INPUTS: \n",
    "#k - number of folds\n",
    "#seed - random seed\n",
    "#y - class labels for stratification\n",
    "#RETURNS: \n",
    "#list of k arrays, containing indices for each fold\n",
    "\n",
    "def make_folds(k=10, seed=42, y=None):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    unique_classes = np.unique(y)\n",
    "    folds = [[] for _ in range(k)]\n",
    "    \n",
    "    #split samples across k folds for each class, stratification\n",
    "    for cls in unique_classes:\n",
    "        cls_indices = np.where(y == cls)[0]\n",
    "        np.random.shuffle(cls_indices)\n",
    "        cls_splits = np.array_split(cls_indices, k)\n",
    "        \n",
    "        #add class samples to each fold\n",
    "        for fold_idx, split in enumerate(cls_splits):\n",
    "            folds[fold_idx].extend(split)\n",
    "    \n",
    "    #shuffle within each fold and convert to numpy arrays\n",
    "    for i in range(k):\n",
    "        np.random.shuffle(folds[i])\n",
    "        folds[i] = np.array(folds[i])\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:33:18.149134Z",
     "iopub.status.busy": "2025-10-10T22:33:18.148806Z",
     "iopub.status.idle": "2025-10-10T22:33:18.154112Z",
     "shell.execute_reply": "2025-10-10T22:33:18.153369Z",
     "shell.execute_reply.started": "2025-10-10T22:33:18.149113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix_manual(y_true, y_pred, labels):\n",
    "    n = len(labels)\n",
    "    label_to_idx = {lab: i for i, lab in enumerate(labels)}\n",
    "    cm = np.zeros((n, n), dtype=int)\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        i = label_to_idx[yt]\n",
    "        j = label_to_idx[yp]\n",
    "        cm[i, j] += 1\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:33:19.988401Z",
     "iopub.status.busy": "2025-10-10T22:33:19.987514Z",
     "iopub.status.idle": "2025-10-10T22:33:19.992540Z",
     "shell.execute_reply": "2025-10-10T22:33:19.991954Z",
     "shell.execute_reply.started": "2025-10-10T22:33:19.988376Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calc_metrics(cm):\n",
    "    TP = np.diag(cm)\n",
    "    FP = cm.sum(0) - TP\n",
    "    FN = cm.sum(1) - TP\n",
    "    precision = np.mean(TP / (TP + FP + 1e-9))\n",
    "    recall    = np.mean(TP / (TP + FN + 1e-9))\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "    acc = TP.sum() / cm.sum()\n",
    "    return acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:33:26.362085Z",
     "iopub.status.busy": "2025-10-10T22:33:26.361758Z",
     "iopub.status.idle": "2025-10-10T22:33:26.369437Z",
     "shell.execute_reply": "2025-10-10T22:33:26.368628Z",
     "shell.execute_reply.started": "2025-10-10T22:33:26.362062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#training one fold, and returning model+predictions on validation set\n",
    "def train_one_fold(X_train, y_train, X_val, y_val, model_builder,\n",
    "                   lr=1e-3, epochs=5, batch=64, device=\"cpu\"):\n",
    "    \n",
    "    train_ds = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "    val_ds   = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch, shuffle=True)\n",
    "    val_dl   = DataLoader(val_ds, batch_size=batch, shuffle=False)\n",
    "\n",
    "    model = model_builder().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            loss = loss_fn(out, yb)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_dl)\n",
    "        print(f\"  Epoch {ep+1}/{epochs}, Train loss={avg_loss:.4f}\")\n",
    "\n",
    "    #validation predictions\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in val_dl:\n",
    "            xb = xb.to(device)\n",
    "            probs = torch.softmax(model(xb), 1)\n",
    "            preds.append(torch.argmax(probs, 1).cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    #memory cleanup\n",
    "    del model, loss_fn, opt, train_dl, val_dl\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return None, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:33:28.117539Z",
     "iopub.status.busy": "2025-10-10T22:33:28.116790Z",
     "iopub.status.idle": "2025-10-10T22:33:28.126849Z",
     "shell.execute_reply": "2025-10-10T22:33:28.126251Z",
     "shell.execute_reply.started": "2025-10-10T22:33:28.117516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model_nested_cv(\n",
    "    X, y, model_builder,\n",
    "    candidate_lr=[1e-3, 3e-4, 1e-4],\n",
    "    k_outer=10, k_inner=3, epochs=5,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    folds = make_folds(len(X), k_outer, seed=42, y=y)\n",
    "    metrics_all = []\n",
    "\n",
    "    for i in range(k_outer):\n",
    "        print(f\"\\n=== Outer Fold {i+1}/{k_outer} ===\")\n",
    "\n",
    "        test_idx = folds[i]\n",
    "        train_idx = np.concatenate([folds[j] for j in range(k_outer) if j != i])\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_test,  y_test  = X[test_idx],  y[test_idx]\n",
    "\n",
    "        #inner loop does hyperparameter tuning\n",
    "        inner_folds = make_folds(len(X_train), k_inner, seed=42, y=y_train)\n",
    "        mean_accs = []\n",
    "\n",
    "        for lr in candidate_lr:\n",
    "            inner_scores = []\n",
    "            for j in range(k_inner):\n",
    "                val_idx = inner_folds[j]\n",
    "                tr_idx  = np.concatenate([inner_folds[m] for m in range(k_inner) if m != j])\n",
    "\n",
    "                _, y_pred_val = train_one_fold(\n",
    "                    X_train[tr_idx], y_train[tr_idx],\n",
    "                    X_train[val_idx], y_train[val_idx],\n",
    "                    model_builder=model_builder,\n",
    "                    lr=lr, epochs=2, device=device\n",
    "                )\n",
    "\n",
    "                cm = confusion_matrix_manual(y_train[val_idx], y_pred_val, labels=np.unique(y))\n",
    "                acc, prec, rec, f1 = calc_metrics(cm)\n",
    "                inner_scores.append(acc)\n",
    "\n",
    "            mean_accs.append(np.mean(inner_scores))\n",
    "\n",
    "        best_lr = candidate_lr[int(np.argmax(mean_accs))]\n",
    "        print(f\"Best LR = {best_lr:.0e}\")\n",
    "\n",
    "        #outer test fold, calculates validation performance\n",
    "        _, y_pred = train_one_fold(\n",
    "            X_train, y_train, X_test, y_test,\n",
    "            model_builder=model_builder,\n",
    "            lr=best_lr, epochs=epochs, device=device\n",
    "        )\n",
    "\n",
    "        cm = confusion_matrix_manual(y_test, y_pred, labels=np.unique(y))\n",
    "        acc, prec, rec, f1 = calc_metrics(cm)\n",
    "        metrics_all.append([acc, prec, rec, f1])\n",
    "\n",
    "        print(f\"Fold {i+1}: Acc={acc:.3f}, P={prec:.3f}, R={rec:.3f}, F1={f1:.3f}\")\n",
    "        \n",
    "        #memory cleanup\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    #summary statistics\n",
    "    metrics_all = np.array(metrics_all)\n",
    "    mean, std = metrics_all.mean(0), metrics_all.std(0)\n",
    "\n",
    "    print(\"\\n=== Nested CV Results ===\")\n",
    "    print(f\"Accuracy : {mean[0]:.3f} ± {std[0]:.3f}\")\n",
    "    print(f\"Precision: {mean[1]:.3f} ± {std[1]:.3f}\")\n",
    "    print(f\"Recall   : {mean[2]:.3f} ± {std[2]:.3f}\")\n",
    "    print(f\"F1-score : {mean[3]:.3f} ± {std[3]:.3f}\")\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:33:30.727978Z",
     "iopub.status.busy": "2025-10-10T22:33:30.727706Z",
     "iopub.status.idle": "2025-10-10T22:33:30.734616Z",
     "shell.execute_reply": "2025-10-10T22:33:30.734069Z",
     "shell.execute_reply.started": "2025-10-10T22:33:30.727959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        #feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),  #32 filters to start off with, padding = 1 to maintain filter dimensions coming out of each conv layer\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                 #downsampling\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "\n",
    "        self.flatten_dim = None\n",
    "        self.classifier = None\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def _get_flatten_dim(self, x):\n",
    "        with torch.no_grad():\n",
    "            f = self.features(x)\n",
    "            return f.view(f.size(0), -1).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.classifier is None:\n",
    "            flat_dim = self._get_flatten_dim(x)\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(flat_dim, 128), nn.ReLU(), \n",
    "                nn.Dropout(0.4),  #dropout between linear layers, for regularisation\n",
    "                nn.Linear(128, self.n_classes)\n",
    "            ).to(x.device)\n",
    "        out = self.features(x)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on train_B.npz (dirty images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T00:04:18.594904Z",
     "iopub.status.busy": "2025-10-11T00:04:18.594568Z",
     "iopub.status.idle": "2025-10-11T00:04:37.621441Z",
     "shell.execute_reply": "2025-10-11T00:04:37.620783Z",
     "shell.execute_reply.started": "2025-10-11T00:04:18.594879Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (12000, 224, 224, 3) (12000,)\n",
      "Label mapping: {'Boot': 0, 'Sandal': 1, 'Shoe': 2}\n",
      "Final tensors: (12000, 3, 224, 224) Classes: 3\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/kaggle/input/images-shoes\"\n",
    "data = np.load(os.path.join(data_dir, \"train_B.npz\"))  # or train_A.npz\n",
    "X, y = data[\"X\"], data[\"y\"]\n",
    "print(\"Loaded:\", X.shape, y.shape)\n",
    "\n",
    "#encode string labels to int values\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)            #e.g. Boot=0, Sandal=1, Shoe=2\n",
    "\n",
    "#save encoder\n",
    "with open('/kaggle/working/encoder_trainB.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder, f)\n",
    "\n",
    "print(\"Label mapping:\", dict(zip(encoder.classes_,\n",
    "                                 range(len(encoder.classes_)))))\n",
    "\n",
    "#normalise images\n",
    "X = X.astype(\"float32\") / 255.0\n",
    "\n",
    "#reshape for pytorch\n",
    "X = np.transpose(X, (0,3,1,2))\n",
    "y = y.astype(\"int64\")\n",
    "num_classes = len(np.unique(y))\n",
    "print(\"Final tensors:\", X.shape, \"Classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T07:27:06.872130Z",
     "iopub.status.busy": "2025-10-10T07:27:06.871897Z",
     "iopub.status.idle": "2025-10-10T08:50:45.892756Z",
     "shell.execute_reply": "2025-10-10T08:50:45.892155Z",
     "shell.execute_reply.started": "2025-10-10T07:27:06.872113Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Evaluating DeepCNN ###\n",
      "\n",
      "=== Outer Fold 1/10 ===\n",
      "  Epoch 1/2, Train loss=0.9186\n",
      "  Epoch 2/2, Train loss=0.7659\n",
      "  Epoch 1/2, Train loss=0.9090\n",
      "  Epoch 2/2, Train loss=0.7549\n",
      "  Epoch 1/2, Train loss=0.9340\n",
      "  Epoch 2/2, Train loss=0.7718\n",
      "  Epoch 1/2, Train loss=0.9266\n",
      "  Epoch 2/2, Train loss=0.7782\n",
      "  Epoch 1/2, Train loss=0.9270\n",
      "  Epoch 2/2, Train loss=0.7841\n",
      "  Epoch 1/2, Train loss=0.9262\n",
      "  Epoch 2/2, Train loss=0.7785\n",
      "  Epoch 1/2, Train loss=0.9811\n",
      "  Epoch 2/2, Train loss=0.8617\n",
      "  Epoch 1/2, Train loss=0.9634\n",
      "  Epoch 2/2, Train loss=0.8492\n",
      "  Epoch 1/2, Train loss=0.9903\n",
      "  Epoch 2/2, Train loss=0.8747\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.8735\n",
      "  Epoch 2/3, Train loss=0.7241\n",
      "  Epoch 3/3, Train loss=0.6613\n",
      "Fold 1: Acc=0.816, P=0.819, R=0.816, F1=0.817\n",
      "\n",
      "=== Outer Fold 2/10 ===\n",
      "  Epoch 1/2, Train loss=0.9078\n",
      "  Epoch 2/2, Train loss=0.7662\n",
      "  Epoch 1/2, Train loss=0.9118\n",
      "  Epoch 2/2, Train loss=0.7604\n",
      "  Epoch 1/2, Train loss=0.9299\n",
      "  Epoch 2/2, Train loss=0.7572\n",
      "  Epoch 1/2, Train loss=0.9094\n",
      "  Epoch 2/2, Train loss=0.7652\n",
      "  Epoch 1/2, Train loss=0.9373\n",
      "  Epoch 2/2, Train loss=0.7936\n",
      "  Epoch 1/2, Train loss=0.9235\n",
      "  Epoch 2/2, Train loss=0.7740\n",
      "  Epoch 1/2, Train loss=0.9718\n",
      "  Epoch 2/2, Train loss=0.8392\n",
      "  Epoch 1/2, Train loss=0.9828\n",
      "  Epoch 2/2, Train loss=0.8678\n",
      "  Epoch 1/2, Train loss=0.9868\n",
      "  Epoch 2/2, Train loss=0.8601\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.8960\n",
      "  Epoch 2/3, Train loss=0.7360\n",
      "  Epoch 3/3, Train loss=0.6731\n",
      "Fold 2: Acc=0.826, P=0.825, R=0.826, F1=0.826\n",
      "\n",
      "=== Outer Fold 3/10 ===\n",
      "  Epoch 1/2, Train loss=0.9284\n",
      "  Epoch 2/2, Train loss=0.7784\n",
      "  Epoch 1/2, Train loss=0.9397\n",
      "  Epoch 2/2, Train loss=0.7650\n",
      "  Epoch 1/2, Train loss=0.9170\n",
      "  Epoch 2/2, Train loss=0.7479\n",
      "  Epoch 1/2, Train loss=0.9276\n",
      "  Epoch 2/2, Train loss=0.7739\n",
      "  Epoch 1/2, Train loss=0.9087\n",
      "  Epoch 2/2, Train loss=0.7679\n",
      "  Epoch 1/2, Train loss=0.9245\n",
      "  Epoch 2/2, Train loss=0.7657\n",
      "  Epoch 1/2, Train loss=0.9487\n",
      "  Epoch 2/2, Train loss=0.8355\n",
      "  Epoch 1/2, Train loss=0.9909\n",
      "  Epoch 2/2, Train loss=0.8664\n",
      "  Epoch 1/2, Train loss=0.9691\n",
      "  Epoch 2/2, Train loss=0.8424\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.8998\n",
      "  Epoch 2/3, Train loss=0.7375\n",
      "  Epoch 3/3, Train loss=0.6757\n",
      "Fold 3: Acc=0.800, P=0.806, R=0.800, F1=0.803\n",
      "\n",
      "=== Outer Fold 4/10 ===\n",
      "  Epoch 1/2, Train loss=0.9373\n",
      "  Epoch 2/2, Train loss=0.7715\n",
      "  Epoch 1/2, Train loss=0.9412\n",
      "  Epoch 2/2, Train loss=0.7820\n",
      "  Epoch 1/2, Train loss=0.9307\n",
      "  Epoch 2/2, Train loss=0.7690\n",
      "  Epoch 1/2, Train loss=0.9452\n",
      "  Epoch 2/2, Train loss=0.7950\n",
      "  Epoch 1/2, Train loss=0.9194\n",
      "  Epoch 2/2, Train loss=0.7708\n",
      "  Epoch 1/2, Train loss=0.8967\n",
      "  Epoch 2/2, Train loss=0.7586\n",
      "  Epoch 1/2, Train loss=0.9745\n",
      "  Epoch 2/2, Train loss=0.8620\n",
      "  Epoch 1/2, Train loss=0.9922\n",
      "  Epoch 2/2, Train loss=0.8700\n",
      "  Epoch 1/2, Train loss=0.9510\n",
      "  Epoch 2/2, Train loss=0.8183\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.8729\n",
      "  Epoch 2/3, Train loss=0.7257\n",
      "  Epoch 3/3, Train loss=0.6658\n",
      "Fold 4: Acc=0.798, P=0.808, R=0.798, F1=0.803\n",
      "\n",
      "=== Outer Fold 5/10 ===\n",
      "  Epoch 1/2, Train loss=0.9280\n",
      "  Epoch 2/2, Train loss=0.7584\n",
      "  Epoch 1/2, Train loss=0.9158\n",
      "  Epoch 2/2, Train loss=0.7577\n",
      "  Epoch 1/2, Train loss=0.9149\n",
      "  Epoch 2/2, Train loss=0.7534\n",
      "  Epoch 1/2, Train loss=0.9379\n",
      "  Epoch 2/2, Train loss=0.7935\n",
      "  Epoch 1/2, Train loss=0.9407\n",
      "  Epoch 2/2, Train loss=0.7859\n",
      "  Epoch 1/2, Train loss=0.9230\n",
      "  Epoch 2/2, Train loss=0.7779\n",
      "  Epoch 1/2, Train loss=0.9937\n",
      "  Epoch 2/2, Train loss=0.8663\n",
      "  Epoch 1/2, Train loss=0.9807\n",
      "  Epoch 2/2, Train loss=0.8667\n",
      "  Epoch 1/2, Train loss=0.9713\n",
      "  Epoch 2/2, Train loss=0.8478\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.8990\n",
      "  Epoch 2/3, Train loss=0.7537\n",
      "  Epoch 3/3, Train loss=0.6842\n",
      "Fold 5: Acc=0.811, P=0.815, R=0.811, F1=0.813\n",
      "\n",
      "=== Outer Fold 6/10 ===\n",
      "  Epoch 1/2, Train loss=0.9161\n",
      "  Epoch 2/2, Train loss=0.7621\n",
      "  Epoch 1/2, Train loss=0.9020\n",
      "  Epoch 2/2, Train loss=0.7513\n",
      "  Epoch 1/2, Train loss=0.9281\n",
      "  Epoch 2/2, Train loss=0.7737\n",
      "  Epoch 1/2, Train loss=0.9306\n",
      "  Epoch 2/2, Train loss=0.7866\n",
      "  Epoch 1/2, Train loss=0.9405\n",
      "  Epoch 2/2, Train loss=0.7973\n",
      "  Epoch 1/2, Train loss=0.9328\n",
      "  Epoch 2/2, Train loss=0.7832\n",
      "  Epoch 1/2, Train loss=0.9759\n",
      "  Epoch 2/2, Train loss=0.8432\n",
      "  Epoch 1/2, Train loss=0.9806\n",
      "  Epoch 2/2, Train loss=0.8575\n",
      "  Epoch 1/2, Train loss=0.9749\n",
      "  Epoch 2/2, Train loss=0.8674\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.8915\n",
      "  Epoch 2/3, Train loss=0.7462\n",
      "  Epoch 3/3, Train loss=0.6832\n",
      "Fold 6: Acc=0.800, P=0.824, R=0.800, F1=0.812\n",
      "\n",
      "=== Outer Fold 7/10 ===\n",
      "  Epoch 1/2, Train loss=0.9096\n",
      "  Epoch 2/2, Train loss=0.7480\n",
      "  Epoch 1/2, Train loss=0.9573\n",
      "  Epoch 2/2, Train loss=0.8083\n",
      "  Epoch 1/2, Train loss=0.9400\n",
      "  Epoch 2/2, Train loss=0.7754\n",
      "  Epoch 1/2, Train loss=0.9423\n",
      "  Epoch 2/2, Train loss=0.7932\n",
      "  Epoch 1/2, Train loss=0.9311\n",
      "  Epoch 2/2, Train loss=0.7826\n",
      "  Epoch 1/2, Train loss=0.9015\n",
      "  Epoch 2/2, Train loss=0.7433\n",
      "  Epoch 1/2, Train loss=0.9817\n",
      "  Epoch 2/2, Train loss=0.8597\n",
      "  Epoch 1/2, Train loss=0.9957\n",
      "  Epoch 2/2, Train loss=0.8809\n",
      "  Epoch 1/2, Train loss=0.9806\n",
      "  Epoch 2/2, Train loss=0.8385\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.8899\n",
      "  Epoch 2/3, Train loss=0.7334\n",
      "  Epoch 3/3, Train loss=0.6693\n",
      "Fold 7: Acc=0.792, P=0.799, R=0.792, F1=0.795\n",
      "\n",
      "=== Outer Fold 8/10 ===\n",
      "  Epoch 1/2, Train loss=0.9108\n",
      "  Epoch 2/2, Train loss=0.7549\n",
      "  Epoch 1/2, Train loss=0.9104\n",
      "  Epoch 2/2, Train loss=0.7448\n",
      "  Epoch 1/2, Train loss=0.9049\n",
      "  Epoch 2/2, Train loss=0.7423\n",
      "  Epoch 1/2, Train loss=0.9288\n",
      "  Epoch 2/2, Train loss=0.7794\n",
      "  Epoch 1/2, Train loss=0.9331\n",
      "  Epoch 2/2, Train loss=0.7861\n",
      "  Epoch 1/2, Train loss=0.9179\n",
      "  Epoch 2/2, Train loss=0.7766\n",
      "  Epoch 1/2, Train loss=0.9852\n",
      "  Epoch 2/2, Train loss=0.8619\n",
      "  Epoch 1/2, Train loss=0.9743\n",
      "  Epoch 2/2, Train loss=0.8637\n",
      "  Epoch 1/2, Train loss=0.9855\n",
      "  Epoch 2/2, Train loss=0.8506\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.8887\n",
      "  Epoch 2/3, Train loss=0.7375\n",
      "  Epoch 3/3, Train loss=0.6921\n",
      "Fold 8: Acc=0.747, P=0.765, R=0.747, F1=0.756\n",
      "\n",
      "=== Outer Fold 9/10 ===\n",
      "  Epoch 1/2, Train loss=0.9296\n",
      "  Epoch 2/2, Train loss=0.7690\n",
      "  Epoch 1/2, Train loss=0.8945\n",
      "  Epoch 2/2, Train loss=0.7304\n",
      "  Epoch 1/2, Train loss=0.9410\n",
      "  Epoch 2/2, Train loss=0.7901\n",
      "  Epoch 1/2, Train loss=0.9279\n",
      "  Epoch 2/2, Train loss=0.7860\n",
      "  Epoch 1/2, Train loss=0.9133\n",
      "  Epoch 2/2, Train loss=0.7642\n",
      "  Epoch 1/2, Train loss=0.9565\n",
      "  Epoch 2/2, Train loss=0.8234\n",
      "  Epoch 1/2, Train loss=0.9638\n",
      "  Epoch 2/2, Train loss=0.8389\n",
      "  Epoch 1/2, Train loss=0.9606\n",
      "  Epoch 2/2, Train loss=0.8267\n",
      "  Epoch 1/2, Train loss=0.9699\n",
      "  Epoch 2/2, Train loss=0.8468\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.8813\n",
      "  Epoch 2/3, Train loss=0.7242\n",
      "  Epoch 3/3, Train loss=0.6602\n",
      "Fold 9: Acc=0.808, P=0.815, R=0.808, F1=0.812\n",
      "\n",
      "=== Outer Fold 10/10 ===\n",
      "  Epoch 1/2, Train loss=0.8999\n",
      "  Epoch 2/2, Train loss=0.7530\n",
      "  Epoch 1/2, Train loss=0.9056\n",
      "  Epoch 2/2, Train loss=0.7406\n",
      "  Epoch 1/2, Train loss=0.9102\n",
      "  Epoch 2/2, Train loss=0.7433\n",
      "  Epoch 1/2, Train loss=0.9235\n",
      "  Epoch 2/2, Train loss=0.7729\n",
      "  Epoch 1/2, Train loss=0.9323\n",
      "  Epoch 2/2, Train loss=0.7769\n",
      "  Epoch 1/2, Train loss=0.9442\n",
      "  Epoch 2/2, Train loss=0.7979\n",
      "  Epoch 1/2, Train loss=0.9729\n",
      "  Epoch 2/2, Train loss=0.8379\n",
      "  Epoch 1/2, Train loss=0.9589\n",
      "  Epoch 2/2, Train loss=0.8242\n",
      "  Epoch 1/2, Train loss=0.9862\n",
      "  Epoch 2/2, Train loss=0.8468\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.9017\n",
      "  Epoch 2/3, Train loss=0.7497\n",
      "  Epoch 3/3, Train loss=0.6752\n",
      "Fold 10: Acc=0.788, P=0.787, R=0.788, F1=0.788\n",
      "\n",
      "=== Nested CV Results ===\n",
      "Accuracy : 0.799 ± 0.020\n",
      "Precision: 0.806 ± 0.018\n",
      "Recall   : 0.799 ± 0.020\n",
      "F1-score : 0.802 ± 0.019\n",
      "CV results saved!\n"
     ]
    }
   ],
   "source": [
    "#cross validation, train B (distorted images)\n",
    "print(\"\\n### Evaluating DeepCNN ###\")\n",
    "deep_mean_B, deep_std_B = evaluate_model_nested_cv(\n",
    "    X, y,\n",
    "    model_builder=lambda: DeepCNN(num_classes),\n",
    "    candidate_lr=[3e-3, 1e-3, 3e-4],\n",
    "    k_outer=10,\n",
    "    k_inner=3,\n",
    "    epochs=3,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "#saving results\n",
    "import pickle\n",
    "\n",
    "cv_results_B = {\n",
    "    'mean': deep_mean_B,\n",
    "    'std': deep_std_B,\n",
    "    'accuracy': deep_mean_B[0],\n",
    "    'precision': deep_mean_B[1],\n",
    "    'recall': deep_mean_B[2],\n",
    "    'f1': deep_mean_B[3]\n",
    "}\n",
    "\n",
    "with open('/kaggle/working/cv_results_trainB.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_results_B, f)\n",
    "\n",
    "print(\"CV results saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T23:58:58.423463Z",
     "iopub.status.busy": "2025-10-10T23:58:58.423207Z",
     "iopub.status.idle": "2025-10-10T23:58:58.429123Z",
     "shell.execute_reply": "2025-10-10T23:58:58.428393Z",
     "shell.execute_reply.started": "2025-10-10T23:58:58.423445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#training final model, using the whole training set\n",
    "#lr - set to the value determined via inner CV (see output above)\n",
    "def train_final_model(X, y, model_builder, lr=1e-3, epochs=5, batch=64, device=\"cpu\"):   \n",
    "    train_ds = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch, shuffle=True)\n",
    "    \n",
    "    model = model_builder().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            loss = loss_fn(out, yb)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_dl)\n",
    "        print(f\"  Epoch {ep+1}/{epochs}, Train loss={avg_loss:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T00:05:21.355435Z",
     "iopub.status.busy": "2025-10-11T00:05:21.354943Z",
     "iopub.status.idle": "2025-10-11T00:07:56.533808Z",
     "shell.execute_reply": "2025-10-11T00:07:56.533164Z",
     "shell.execute_reply.started": "2025-10-11T00:05:21.355412Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Final Model on Full Dataset (Dirty Images) ===\n",
      "  Epoch 1/5, Train loss=0.8796\n",
      "  Epoch 2/5, Train loss=0.7210\n",
      "  Epoch 3/5, Train loss=0.6518\n",
      "  Epoch 4/5, Train loss=0.6117\n",
      "  Epoch 5/5, Train loss=0.5837\n",
      "Final model saved!\n"
     ]
    }
   ],
   "source": [
    "#training final model, using hyperparameter values chosen via nested CV\n",
    "print(\"\\n=== Training Final Model on Full Dataset (Dirty Images) ===\")\n",
    "\n",
    "final_model = train_final_model(\n",
    "    X, y,\n",
    "    model_builder=lambda: DeepCNN(num_classes),\n",
    "    lr=1e-3,\n",
    "    epochs=5,\n",
    "    batch=64,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "#save final model\n",
    "torch.save(final_model.state_dict(), '/kaggle/working/deepcnn_final_B.pth')\n",
    "print(\"Final model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T09:16:20.905534Z",
     "iopub.status.busy": "2025-10-10T09:16:20.905157Z",
     "iopub.status.idle": "2025-10-10T09:16:28.554618Z",
     "shell.execute_reply": "2025-10-10T09:16:28.553860Z",
     "shell.execute_reply.started": "2025-10-10T09:16:20.905509Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Test Data ===\n",
      "Loaded test data: (3000, 224, 224, 3) (3000,)\n",
      "Test data prepared: (3000, 3, 224, 224)\n",
      "\n",
      "=== Evaluating Final Model on Test Set ===\n",
      "\n",
      "=== Test Set Results ===\n",
      "Accuracy : 0.843\n",
      "Precision: 0.842\n",
      "Recall   : 0.843\n",
      "F1-score : 0.842\n",
      "\n",
      "Confusion Matrix:\n",
      "[[946  30  24]\n",
      " [108 757 135]\n",
      " [ 46 128 826]]\n",
      "Classes: ['Boot' 'Sandal' 'Shoe']\n",
      "\n",
      "=== Performance Comparison ===\n",
      "CV Performance:   0.799 ± 0.020\n",
      "Test Performance: 0.843\n",
      "Difference:       0.044\n"
     ]
    }
   ],
   "source": [
    "#loading/preparing test set\n",
    "print(\"\\n=== Loading Test Data ===\")\n",
    "test_data = np.load(os.path.join(data_dir, \"test.npz\"))\n",
    "X_test, y_test = test_data[\"X\"], test_data[\"y\"]\n",
    "print(\"Loaded test data:\", X_test.shape, y_test.shape)\n",
    "\n",
    "#loading encoder\n",
    "with open('/kaggle/working/encoder_trainB.pkl', 'rb') as f:\n",
    "    encoder = pickle.load(f)\n",
    "\n",
    "#encode labels\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "#normalising and reshaping test images\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "X_test = np.transpose(X_test, (0,3,1,2))\n",
    "y_test = y_test.astype(\"int64\")\n",
    "\n",
    "print(\"Test data prepared:\", X_test.shape)\n",
    "\n",
    "#making predictions\n",
    "print(\"\\n=== Evaluating Final Model on Test Set ===\")\n",
    "\n",
    "test_ds = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "final_model.eval()\n",
    "y_pred_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, _ in test_dl:\n",
    "        xb = xb.to(device)\n",
    "        probs = torch.softmax(final_model(xb), 1)\n",
    "        y_pred_test.append(torch.argmax(probs, 1).cpu().numpy())\n",
    "\n",
    "y_pred_test = np.concatenate(y_pred_test)\n",
    "\n",
    "#calculating metrics\n",
    "cm_test = confusion_matrix_manual(y_test, y_pred_test, labels=np.unique(y_test))\n",
    "acc_test, prec_test, rec_test, f1_test = calc_metrics(cm_test)\n",
    "\n",
    "print(f\"\\n=== Test Set Results ===\")\n",
    "print(f\"Accuracy : {acc_test:.3f}\")\n",
    "print(f\"Precision: {prec_test:.3f}\")\n",
    "print(f\"Recall   : {rec_test:.3f}\")\n",
    "print(f\"F1-score : {f1_test:.3f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm_test)\n",
    "print(f\"Classes: {encoder.classes_}\")\n",
    "\n",
    "#comparison with cv results\n",
    "#opening saved cv results\n",
    "with open('/kaggle/working/cv_results_trainB.pkl', 'rb') as f:\n",
    "    cv_results_B = pickle.load(f)\n",
    "\n",
    "#extracting relevant variables\n",
    "deep_mean_B = cv_results_B['mean']\n",
    "deep_std_B = cv_results_B['std']\n",
    "\n",
    "print(f\"\\n=== Performance Comparison ===\")\n",
    "print(f\"CV Performance:   {deep_mean_B[0]:.3f} ± {deep_std_B[0]:.3f}\") \n",
    "print(f\"Test Performance: {acc_test:.3f}\")\n",
    "print(f\"Difference:       {acc_test - deep_mean_B[0]:.3f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating the above, but this time using train_A (which is the image set with only clean images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:33:41.800682Z",
     "iopub.status.busy": "2025-10-10T22:33:41.799934Z",
     "iopub.status.idle": "2025-10-10T22:33:58.832988Z",
     "shell.execute_reply": "2025-10-10T22:33:58.832215Z",
     "shell.execute_reply.started": "2025-10-10T22:33:41.800660Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (12000, 224, 224, 3) (12000,)\n",
      "Label mapping: {'Boot': 0, 'Sandal': 1, 'Shoe': 2}\n",
      "Final tensors: (12000, 3, 224, 224) Classes: 3\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/kaggle/input/images-shoes\"\n",
    "data = np.load(os.path.join(data_dir, \"train_A.npz\"))  #changed to train_A.npz\n",
    "X, y = data[\"X\"], data[\"y\"]\n",
    "print(\"Loaded:\", X.shape, y.shape)\n",
    "\n",
    "#encode string labels to int values\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)            #e.g. Boot=0, Sandal=1, Shoe=2\n",
    "\n",
    "#save encoder\n",
    "with open('/kaggle/working/encoder_trainA.pkl', 'wb') as f:\n",
    "    pickle.dump(encoder, f)\n",
    "    \n",
    "print(\"Label mapping:\", dict(zip(encoder.classes_,\n",
    "                                 range(len(encoder.classes_)))))\n",
    "\n",
    "#normalise images\n",
    "X = X.astype(\"float32\") / 255.0\n",
    "\n",
    "#reshape for pytorch\n",
    "X = np.transpose(X, (0,3,1,2))\n",
    "y = y.astype(\"int64\")\n",
    "num_classes = len(np.unique(y))\n",
    "print(\"Final tensors:\", X.shape, \"Classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T22:34:10.744291Z",
     "iopub.status.busy": "2025-10-10T22:34:10.743997Z",
     "iopub.status.idle": "2025-10-10T23:57:45.941486Z",
     "shell.execute_reply": "2025-10-10T23:57:45.940801Z",
     "shell.execute_reply.started": "2025-10-10T22:34:10.744272Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Evaluating DeepCNN ###\n",
      "\n",
      "=== Outer Fold 1/10 ===\n",
      "  Epoch 1/2, Train loss=0.7664\n",
      "  Epoch 2/2, Train loss=0.5608\n",
      "  Epoch 1/2, Train loss=0.7626\n",
      "  Epoch 2/2, Train loss=0.5600\n",
      "  Epoch 1/2, Train loss=0.7324\n",
      "  Epoch 2/2, Train loss=0.5504\n",
      "  Epoch 1/2, Train loss=0.7799\n",
      "  Epoch 2/2, Train loss=0.6029\n",
      "  Epoch 1/2, Train loss=0.7233\n",
      "  Epoch 2/2, Train loss=0.5521\n",
      "  Epoch 1/2, Train loss=0.7769\n",
      "  Epoch 2/2, Train loss=0.5930\n",
      "  Epoch 1/2, Train loss=0.8542\n",
      "  Epoch 2/2, Train loss=0.6773\n",
      "  Epoch 1/2, Train loss=0.8415\n",
      "  Epoch 2/2, Train loss=0.6645\n",
      "  Epoch 1/2, Train loss=0.8234\n",
      "  Epoch 2/2, Train loss=0.6538\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.7051\n",
      "  Epoch 2/3, Train loss=0.5408\n",
      "  Epoch 3/3, Train loss=0.4639\n",
      "Fold 1: Acc=0.900, P=0.900, R=0.900, F1=0.900\n",
      "\n",
      "=== Outer Fold 2/10 ===\n",
      "  Epoch 1/2, Train loss=0.7760\n",
      "  Epoch 2/2, Train loss=0.5782\n",
      "  Epoch 1/2, Train loss=0.7593\n",
      "  Epoch 2/2, Train loss=0.5623\n",
      "  Epoch 1/2, Train loss=0.7432\n",
      "  Epoch 2/2, Train loss=0.5540\n",
      "  Epoch 1/2, Train loss=0.7675\n",
      "  Epoch 2/2, Train loss=0.5760\n",
      "  Epoch 1/2, Train loss=0.7758\n",
      "  Epoch 2/2, Train loss=0.5939\n",
      "  Epoch 1/2, Train loss=0.7447\n",
      "  Epoch 2/2, Train loss=0.5619\n",
      "  Epoch 1/2, Train loss=0.8127\n",
      "  Epoch 2/2, Train loss=0.6503\n",
      "  Epoch 1/2, Train loss=0.8231\n",
      "  Epoch 2/2, Train loss=0.6524\n",
      "  Epoch 1/2, Train loss=0.8242\n",
      "  Epoch 2/2, Train loss=0.6654\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.7254\n",
      "  Epoch 2/3, Train loss=0.5474\n",
      "  Epoch 3/3, Train loss=0.4812\n",
      "Fold 2: Acc=0.903, P=0.905, R=0.903, F1=0.904\n",
      "\n",
      "=== Outer Fold 3/10 ===\n",
      "  Epoch 1/2, Train loss=0.7262\n",
      "  Epoch 2/2, Train loss=0.5503\n",
      "  Epoch 1/2, Train loss=0.7592\n",
      "  Epoch 2/2, Train loss=0.5633\n",
      "  Epoch 1/2, Train loss=0.7475\n",
      "  Epoch 2/2, Train loss=0.5578\n",
      "  Epoch 1/2, Train loss=0.7758\n",
      "  Epoch 2/2, Train loss=0.5970\n",
      "  Epoch 1/2, Train loss=0.7762\n",
      "  Epoch 2/2, Train loss=0.5994\n",
      "  Epoch 1/2, Train loss=0.7191\n",
      "  Epoch 2/2, Train loss=0.5633\n",
      "  Epoch 1/2, Train loss=0.8216\n",
      "  Epoch 2/2, Train loss=0.6482\n",
      "  Epoch 1/2, Train loss=0.8641\n",
      "  Epoch 2/2, Train loss=0.6798\n",
      "  Epoch 1/2, Train loss=0.8286\n",
      "  Epoch 2/2, Train loss=0.6455\n",
      "Best LR = 3e-03\n",
      "  Epoch 1/3, Train loss=0.6838\n",
      "  Epoch 2/3, Train loss=0.4895\n",
      "  Epoch 3/3, Train loss=0.4193\n",
      "Fold 3: Acc=0.914, P=0.914, R=0.914, F1=0.914\n",
      "\n",
      "=== Outer Fold 4/10 ===\n",
      "  Epoch 1/2, Train loss=0.7191\n",
      "  Epoch 2/2, Train loss=0.5316\n",
      "  Epoch 1/2, Train loss=0.7510\n",
      "  Epoch 2/2, Train loss=0.5576\n",
      "  Epoch 1/2, Train loss=0.7401\n",
      "  Epoch 2/2, Train loss=0.5569\n",
      "  Epoch 1/2, Train loss=0.7751\n",
      "  Epoch 2/2, Train loss=0.5885\n",
      "  Epoch 1/2, Train loss=0.7515\n",
      "  Epoch 2/2, Train loss=0.5796\n",
      "  Epoch 1/2, Train loss=0.7644\n",
      "  Epoch 2/2, Train loss=0.5918\n",
      "  Epoch 1/2, Train loss=0.8066\n",
      "  Epoch 2/2, Train loss=0.6357\n",
      "  Epoch 1/2, Train loss=0.8705\n",
      "  Epoch 2/2, Train loss=0.6827\n",
      "  Epoch 1/2, Train loss=0.8428\n",
      "  Epoch 2/2, Train loss=0.6753\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.7323\n",
      "  Epoch 2/3, Train loss=0.5551\n",
      "  Epoch 3/3, Train loss=0.4723\n",
      "Fold 4: Acc=0.887, P=0.892, R=0.887, F1=0.889\n",
      "\n",
      "=== Outer Fold 5/10 ===\n",
      "  Epoch 1/2, Train loss=0.7505\n",
      "  Epoch 2/2, Train loss=0.5690\n",
      "  Epoch 1/2, Train loss=0.7268\n",
      "  Epoch 2/2, Train loss=0.5533\n",
      "  Epoch 1/2, Train loss=0.7313\n",
      "  Epoch 2/2, Train loss=0.5429\n",
      "  Epoch 1/2, Train loss=0.7574\n",
      "  Epoch 2/2, Train loss=0.5746\n",
      "  Epoch 1/2, Train loss=0.7452\n",
      "  Epoch 2/2, Train loss=0.5759\n",
      "  Epoch 1/2, Train loss=0.7745\n",
      "  Epoch 2/2, Train loss=0.5834\n",
      "  Epoch 1/2, Train loss=0.8313\n",
      "  Epoch 2/2, Train loss=0.6538\n",
      "  Epoch 1/2, Train loss=0.8427\n",
      "  Epoch 2/2, Train loss=0.6811\n",
      "  Epoch 1/2, Train loss=0.8365\n",
      "  Epoch 2/2, Train loss=0.6615\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.7200\n",
      "  Epoch 2/3, Train loss=0.5411\n",
      "  Epoch 3/3, Train loss=0.4655\n",
      "Fold 5: Acc=0.860, P=0.881, R=0.860, F1=0.870\n",
      "\n",
      "=== Outer Fold 6/10 ===\n",
      "  Epoch 1/2, Train loss=0.7529\n",
      "  Epoch 2/2, Train loss=0.5696\n",
      "  Epoch 1/2, Train loss=0.7579\n",
      "  Epoch 2/2, Train loss=0.5754\n",
      "  Epoch 1/2, Train loss=0.7381\n",
      "  Epoch 2/2, Train loss=0.5533\n",
      "  Epoch 1/2, Train loss=0.7516\n",
      "  Epoch 2/2, Train loss=0.5744\n",
      "  Epoch 1/2, Train loss=0.7569\n",
      "  Epoch 2/2, Train loss=0.5816\n",
      "  Epoch 1/2, Train loss=0.7791\n",
      "  Epoch 2/2, Train loss=0.5941\n",
      "  Epoch 1/2, Train loss=0.8369\n",
      "  Epoch 2/2, Train loss=0.6640\n",
      "  Epoch 1/2, Train loss=0.8197\n",
      "  Epoch 2/2, Train loss=0.6460\n",
      "  Epoch 1/2, Train loss=0.8234\n",
      "  Epoch 2/2, Train loss=0.6551\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.7283\n",
      "  Epoch 2/3, Train loss=0.5422\n",
      "  Epoch 3/3, Train loss=0.4660\n",
      "Fold 6: Acc=0.896, P=0.897, R=0.896, F1=0.897\n",
      "\n",
      "=== Outer Fold 7/10 ===\n",
      "  Epoch 1/2, Train loss=0.7165\n",
      "  Epoch 2/2, Train loss=0.5272\n",
      "  Epoch 1/2, Train loss=0.7429\n",
      "  Epoch 2/2, Train loss=0.5717\n",
      "  Epoch 1/2, Train loss=0.7682\n",
      "  Epoch 2/2, Train loss=0.5742\n",
      "  Epoch 1/2, Train loss=0.7581\n",
      "  Epoch 2/2, Train loss=0.5847\n",
      "  Epoch 1/2, Train loss=0.7869\n",
      "  Epoch 2/2, Train loss=0.5980\n",
      "  Epoch 1/2, Train loss=0.7958\n",
      "  Epoch 2/2, Train loss=0.6104\n",
      "  Epoch 1/2, Train loss=0.8572\n",
      "  Epoch 2/2, Train loss=0.6822\n",
      "  Epoch 1/2, Train loss=0.8691\n",
      "  Epoch 2/2, Train loss=0.6938\n",
      "  Epoch 1/2, Train loss=0.8322\n",
      "  Epoch 2/2, Train loss=0.6550\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.7375\n",
      "  Epoch 2/3, Train loss=0.5536\n",
      "  Epoch 3/3, Train loss=0.4818\n",
      "Fold 7: Acc=0.888, P=0.896, R=0.888, F1=0.892\n",
      "\n",
      "=== Outer Fold 8/10 ===\n",
      "  Epoch 1/2, Train loss=0.7675\n",
      "  Epoch 2/2, Train loss=0.5567\n",
      "  Epoch 1/2, Train loss=0.7820\n",
      "  Epoch 2/2, Train loss=0.5654\n",
      "  Epoch 1/2, Train loss=0.7374\n",
      "  Epoch 2/2, Train loss=0.5351\n",
      "  Epoch 1/2, Train loss=0.7307\n",
      "  Epoch 2/2, Train loss=0.5620\n",
      "  Epoch 1/2, Train loss=0.7647\n",
      "  Epoch 2/2, Train loss=0.5821\n",
      "  Epoch 1/2, Train loss=0.7559\n",
      "  Epoch 2/2, Train loss=0.5664\n",
      "  Epoch 1/2, Train loss=0.8373\n",
      "  Epoch 2/2, Train loss=0.6645\n",
      "  Epoch 1/2, Train loss=0.8612\n",
      "  Epoch 2/2, Train loss=0.6665\n",
      "  Epoch 1/2, Train loss=0.8244\n",
      "  Epoch 2/2, Train loss=0.6340\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.6937\n",
      "  Epoch 2/3, Train loss=0.5180\n",
      "  Epoch 3/3, Train loss=0.4488\n",
      "Fold 8: Acc=0.900, P=0.900, R=0.900, F1=0.900\n",
      "\n",
      "=== Outer Fold 9/10 ===\n",
      "  Epoch 1/2, Train loss=0.7216\n",
      "  Epoch 2/2, Train loss=0.5385\n",
      "  Epoch 1/2, Train loss=0.7265\n",
      "  Epoch 2/2, Train loss=0.5405\n",
      "  Epoch 1/2, Train loss=0.7099\n",
      "  Epoch 2/2, Train loss=0.5383\n",
      "  Epoch 1/2, Train loss=0.7964\n",
      "  Epoch 2/2, Train loss=0.6157\n",
      "  Epoch 1/2, Train loss=0.7862\n",
      "  Epoch 2/2, Train loss=0.6106\n",
      "  Epoch 1/2, Train loss=0.7744\n",
      "  Epoch 2/2, Train loss=0.5734\n",
      "  Epoch 1/2, Train loss=0.8367\n",
      "  Epoch 2/2, Train loss=0.6787\n",
      "  Epoch 1/2, Train loss=0.8470\n",
      "  Epoch 2/2, Train loss=0.6722\n",
      "  Epoch 1/2, Train loss=0.8120\n",
      "  Epoch 2/2, Train loss=0.6433\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.7196\n",
      "  Epoch 2/3, Train loss=0.5474\n",
      "  Epoch 3/3, Train loss=0.4809\n",
      "Fold 9: Acc=0.882, P=0.882, R=0.882, F1=0.882\n",
      "\n",
      "=== Outer Fold 10/10 ===\n",
      "  Epoch 1/2, Train loss=0.7573\n",
      "  Epoch 2/2, Train loss=0.5600\n",
      "  Epoch 1/2, Train loss=0.7362\n",
      "  Epoch 2/2, Train loss=0.5524\n",
      "  Epoch 1/2, Train loss=0.7617\n",
      "  Epoch 2/2, Train loss=0.5474\n",
      "  Epoch 1/2, Train loss=0.7578\n",
      "  Epoch 2/2, Train loss=0.5827\n",
      "  Epoch 1/2, Train loss=0.7705\n",
      "  Epoch 2/2, Train loss=0.5853\n",
      "  Epoch 1/2, Train loss=0.7773\n",
      "  Epoch 2/2, Train loss=0.6014\n",
      "  Epoch 1/2, Train loss=0.8633\n",
      "  Epoch 2/2, Train loss=0.7012\n",
      "  Epoch 1/2, Train loss=0.8461\n",
      "  Epoch 2/2, Train loss=0.6576\n",
      "  Epoch 1/2, Train loss=0.8695\n",
      "  Epoch 2/2, Train loss=0.6779\n",
      "Best LR = 3e-03\n",
      "  Epoch 1/3, Train loss=0.6487\n",
      "  Epoch 2/3, Train loss=0.4697\n",
      "  Epoch 3/3, Train loss=0.3977\n",
      "Fold 10: Acc=0.868, P=0.882, R=0.867, F1=0.875\n",
      "\n",
      "=== Nested CV Results ===\n",
      "Accuracy : 0.890 ± 0.016\n",
      "Precision: 0.895 ± 0.010\n",
      "Recall   : 0.890 ± 0.016\n",
      "F1-score : 0.892 ± 0.013\n",
      "CV results saved!\n"
     ]
    }
   ],
   "source": [
    "#cross validation, train A (clean images)\n",
    "print(\"\\n### Evaluating DeepCNN ###\")\n",
    "deep_mean_A, deep_std_A = evaluate_model_nested_cv(\n",
    "    X, y,\n",
    "    model_builder=lambda: DeepCNN(num_classes),\n",
    "    candidate_lr=[3e-3, 1e-3, 3e-4],\n",
    "    k_outer=10,\n",
    "    k_inner=3,\n",
    "    epochs=3,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "#saving results\n",
    "import pickle\n",
    "\n",
    "cv_results_A = {\n",
    "    'mean': deep_mean_A,\n",
    "    'std': deep_std_A,\n",
    "    'accuracy': deep_mean_A[0],\n",
    "    'precision': deep_mean_A[1],\n",
    "    'recall': deep_mean_A[2],\n",
    "    'f1': deep_mean_A[3]\n",
    "}\n",
    "\n",
    "with open('/kaggle/working/cv_results_trainA.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_results_A, f)\n",
    "\n",
    "print(\"CV results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T23:59:03.779472Z",
     "iopub.status.busy": "2025-10-10T23:59:03.779213Z",
     "iopub.status.idle": "2025-10-11T00:01:38.651060Z",
     "shell.execute_reply": "2025-10-11T00:01:38.650365Z",
     "shell.execute_reply.started": "2025-10-10T23:59:03.779454Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Final Model on Full Dataset (Clean Images) ===\n",
      "  Epoch 1/5, Train loss=0.6687\n",
      "  Epoch 2/5, Train loss=0.4961\n",
      "  Epoch 3/5, Train loss=0.4271\n",
      "  Epoch 4/5, Train loss=0.3816\n",
      "  Epoch 5/5, Train loss=0.3456\n",
      "Final model saved!\n"
     ]
    }
   ],
   "source": [
    "#training final model, using the whole training set\n",
    "#lr - set to the value determined via inner CV (see output above)\n",
    "print(\"\\n=== Training Final Model on Full Dataset (Clean Images) ===\")\n",
    "\n",
    "final_model_clean = train_final_model(\n",
    "    X, y,\n",
    "    model_builder=lambda: DeepCNN(num_classes),\n",
    "    lr=1e-3,\n",
    "    epochs=5,\n",
    "    batch=64,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "torch.save(final_model_clean.state_dict(), '/kaggle/working/deepcnn_final_clean.pth')\n",
    "print(\"Final model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T00:02:07.135531Z",
     "iopub.status.busy": "2025-10-11T00:02:07.134885Z",
     "iopub.status.idle": "2025-10-11T00:02:16.681311Z",
     "shell.execute_reply": "2025-10-11T00:02:16.680312Z",
     "shell.execute_reply.started": "2025-10-11T00:02:07.135506Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Test Data (for Train_A Model) ===\n",
      "Loaded test data: (3000, 224, 224, 3) (3000,)\n",
      "Test data prepared: (3000, 3, 224, 224)\n",
      "\n",
      "=== Evaluating Train_A Model on Test Set ===\n",
      "\n",
      "=== Test Set Results (Train_A Model - Clean Training) ===\n",
      "Accuracy : 0.812\n",
      "Precision: 0.816\n",
      "Recall   : 0.812\n",
      "F1-score : 0.814\n",
      "\n",
      "Confusion Matrix:\n",
      "[[844  92  64]\n",
      " [ 70 830 100]\n",
      " [ 52 185 763]]\n",
      "Classes: ['Boot' 'Sandal' 'Shoe']\n",
      "\n",
      "=== Performance Comparison (Train_A) ===\n",
      "CV Performance:   0.890 ± 0.016\n",
      "Test Performance: 0.812\n",
      "Difference:       -0.078\n"
     ]
    }
   ],
   "source": [
    "#loading/preparing test images\n",
    "print(\"\\n=== Loading Test Data (for Train_A Model) ===\")\n",
    "test_data = np.load(os.path.join(data_dir, \"test.npz\"))\n",
    "X_test, y_test = test_data[\"X\"], test_data[\"y\"]\n",
    "print(\"Loaded test data:\", X_test.shape, y_test.shape)\n",
    "\n",
    "#loading encoder\n",
    "with open('/kaggle/working/encoder_trainA.pkl', 'rb') as f:\n",
    "    encoder = pickle.load(f)\n",
    "    \n",
    "#encode labels\n",
    "y_test = encoder.transform(y_test)  # Make sure this is the encoder fitted on train_A\n",
    "\n",
    "#normalising and reshaping test set images\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "X_test = np.transpose(X_test, (0,3,1,2))\n",
    "y_test = y_test.astype(\"int64\")\n",
    "\n",
    "print(\"Test data prepared:\", X_test.shape)\n",
    "\n",
    "#making predictions\n",
    "print(\"\\n=== Evaluating Train_A Model on Test Set ===\")\n",
    "\n",
    "test_ds = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "final_model_clean.eval()  \n",
    "y_pred_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, _ in test_dl:\n",
    "        xb = xb.to(device)\n",
    "        probs = torch.softmax(final_model_clean(xb), 1)  \n",
    "        y_pred_test.append(torch.argmax(probs, 1).cpu().numpy())\n",
    "\n",
    "y_pred_test = np.concatenate(y_pred_test)\n",
    "\n",
    "#calculating metrics\n",
    "cm_test = confusion_matrix_manual(y_test, y_pred_test, labels=np.unique(y_test))\n",
    "acc_test, prec_test, rec_test, f1_test = calc_metrics(cm_test)\n",
    "\n",
    "print(f\"\\n=== Test Set Results (Train_A Model - Clean Training) ===\")\n",
    "print(f\"Accuracy : {acc_test:.3f}\")\n",
    "print(f\"Precision: {prec_test:.3f}\")\n",
    "print(f\"Recall   : {rec_test:.3f}\")\n",
    "print(f\"F1-score : {f1_test:.3f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm_test)\n",
    "print(f\"Classes: {encoder.classes_}\")\n",
    "\n",
    "#comparison with cv results\n",
    "#opening saved cv results\n",
    "with open('/kaggle/working/cv_results_trainA.pkl', 'rb') as f:\n",
    "    cv_results_A = pickle.load(f)\n",
    "\n",
    "#extracting relevant variables\n",
    "deep_mean_A = cv_results_A['mean']\n",
    "deep_std_A = cv_results_A['std']\n",
    "\n",
    "print(f\"\\n=== Performance Comparison (Train_A) ===\")\n",
    "print(f\"CV Performance:   {deep_mean_A[0]:.3f} ± {deep_std_A[0]:.3f}\")  # ← Use train_A CV results\n",
    "print(f\"Test Performance: {acc_test:.3f}\")\n",
    "print(f\"Difference:       {acc_test - deep_mean_A[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping, to get error bars for the test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T00:17:41.601998Z",
     "iopub.status.busy": "2025-10-11T00:17:41.601724Z",
     "iopub.status.idle": "2025-10-11T00:17:41.607481Z",
     "shell.execute_reply": "2025-10-11T00:17:41.606686Z",
     "shell.execute_reply.started": "2025-10-11T00:17:41.601978Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#bootstrapping to get error bars for test metrics\n",
    "#INPUTS:\n",
    "#y_true - true labels of test set\n",
    "#y_pred - predicted labels of test set\n",
    "#n_bootstrap - number of bootstrap iterations to perform\n",
    "#confidence - to calculate confidence interval, to determine statistical significance\n",
    "def bootstrap_metrics(y_true, y_pred, n_bootstrap=1000, confidence=0.95):\n",
    "    n_samples = len(y_true)\n",
    "    results = {\"acc\": [], \"prec\": [], \"rec\": [], \"f1\": []}\n",
    "    labels = np.unique(y_true)\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        y_t, y_p = y_true[idx], y_pred[idx]\n",
    "        cm = confusion_matrix_manual(y_t, y_p, labels=labels)\n",
    "        acc, prec, rec, f1 = calc_metrics(cm)\n",
    "        results[\"acc\"].append(acc)\n",
    "        results[\"prec\"].append(prec)\n",
    "        results[\"rec\"].append(rec)\n",
    "        results[\"f1\"].append(f1)\n",
    "\n",
    "    alpha = (1 - confidence) / 2\n",
    "    stats = {}\n",
    "    for k, v in results.items():\n",
    "        v = np.array(v)\n",
    "        stats[k] = (\n",
    "            np.mean(v),\n",
    "            np.percentile(v, 100 * alpha),\n",
    "            np.percentile(v, 100 * (1 - alpha)),\n",
    "        )\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-11T00:16:06.666201Z",
     "iopub.status.busy": "2025-10-11T00:16:06.665906Z",
     "iopub.status.idle": "2025-10-11T00:16:06.672566Z",
     "shell.execute_reply": "2025-10-11T00:16:06.671997Z",
     "shell.execute_reply.started": "2025-10-11T00:16:06.666180Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Train_A Model (Clean Images) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.2.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/var/folders/dv/bjx4hkg54x17symh1g3_3jf80000gq/T/ipykernel_49904/1957033298.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  final_model_A.load_state_dict(torch.load(os.path.join(results_dir, 'deepcnn_final_clean.pth'), map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bootstrap Analysis for Train_A Model ===\n",
      "Test Accuracy:  0.8124 (0.7987, 0.8263)\n",
      "Test Precision: 0.8157 (0.8019, 0.8291)\n",
      "Test Recall:    0.8124 (0.7981, 0.8259)\n",
      "Test F1:        0.8140 (0.8003, 0.8273)\n",
      "\n",
      "=== Bootstrap Analysis for Train_B Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.2.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/var/folders/dv/bjx4hkg54x17symh1g3_3jf80000gq/T/ipykernel_49904/1957033298.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  final_model_B.load_state_dict(torch.load(os.path.join(results_dir, 'deepcnn_final_B.pth'), map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.8469 (0.8333, 0.8597)\n",
      "Test Precision: 0.8484 (0.8348, 0.8613)\n",
      "Test Recall:    0.8469 (0.8335, 0.8596)\n",
      "Test F1:        0.8477 (0.8340, 0.8601)\n",
      "\n",
      "=== Final Comparison ===\n",
      "Train on Clean (A): 0.8124 (0.7987, 0.8263)\n",
      "Train on Dirty (B): 0.8469 (0.8333, 0.8597)\n",
      "\n",
      "Difference: 0.0344\n"
     ]
    }
   ],
   "source": [
    "#loading relevant paths \n",
    "results_dir = \"/Users/fungs4/Desktop/Skye/University of Melbourne/Year 1 2025/COMP90051 Statistical Machine Learning/COMP90051 Assignments/COMP90051 Assignment 2/COMP90051_A2/A2_DeepCNN_Results\"\n",
    "data_dir = \"/Users/fungs4/Desktop/Skye/University of Melbourne/Year 1 2025/COMP90051 Statistical Machine Learning/COMP90051 Assignments/COMP90051 Assignment 2/augmented_data\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#loading test data\n",
    "test_data = np.load(os.path.join(data_dir, \"test.npz\"))\n",
    "X_test_raw, y_test_raw = test_data[\"X\"], test_data[\"y\"]\n",
    "\n",
    "#loading train A model (clean images), and making predictions\n",
    "print(\"\\n=== Loading Train_A Model (Clean Images) ===\")\n",
    "\n",
    "#load encoder\n",
    "with open(os.path.join(results_dir, 'encoder_trainA.pkl'), 'rb') as f:\n",
    "    encoder_A = pickle.load(f)\n",
    "\n",
    "#transform labels\n",
    "y_test = encoder_A.transform(y_test_raw)\n",
    "\n",
    "#preprocessing images\n",
    "X_test = X_test_raw.astype(\"float32\") / 255.0\n",
    "X_test = np.transpose(X_test, (0, 3, 1, 2))\n",
    "y_test = y_test.astype(\"int64\")\n",
    "\n",
    "#loading train A\n",
    "num_classes = len(encoder_A.classes_)\n",
    "final_model_A = DeepCNN(num_classes).to(device)\n",
    "\n",
    "#dummy forward pass to initialize\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    _ = final_model_A(dummy_input)\n",
    "\n",
    "#load weights\n",
    "final_model_A.load_state_dict(torch.load(os.path.join(results_dir, 'deepcnn_final_clean.pth'), map_location=device))\n",
    "\n",
    "#predictions for Train_A\n",
    "test_ds_A = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "test_dl_A = DataLoader(test_ds_A, batch_size=64, shuffle=False)\n",
    "\n",
    "final_model_A.eval()\n",
    "y_pred_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, _ in test_dl_A:\n",
    "        xb = xb.to(device)\n",
    "        probs = torch.softmax(final_model_A(xb), 1)\n",
    "        y_pred_test.append(torch.argmax(probs, 1).cpu().numpy())\n",
    "\n",
    "y_pred_test = np.concatenate(y_pred_test)\n",
    "\n",
    "#bootstrap for train A\n",
    "print(\"\\n=== Bootstrap Analysis for Train_A Model ===\")\n",
    "\n",
    "stats_A = bootstrap_metrics(y_test, y_pred_test, n_bootstrap=1000)\n",
    "\n",
    "#extract metrics\n",
    "mean_acc_A, lower_acc_A, upper_acc_A = stats_A['acc']\n",
    "mean_prec_A, lower_prec_A, upper_prec_A = stats_A['prec']\n",
    "mean_rec_A, lower_rec_A, upper_rec_A = stats_A['rec']\n",
    "mean_f1_A, lower_f1_A, upper_f1_A = stats_A['f1']\n",
    "\n",
    "print(f\"Test Accuracy:  {mean_acc_A:.4f} ({lower_acc_A:.4f}, {upper_acc_A:.4f})\")\n",
    "print(f\"Test Precision: {mean_prec_A:.4f} ({lower_prec_A:.4f}, {upper_prec_A:.4f})\")\n",
    "print(f\"Test Recall:    {mean_rec_A:.4f} ({lower_rec_A:.4f}, {upper_rec_A:.4f})\")\n",
    "print(f\"Test F1:        {mean_f1_A:.4f} ({lower_f1_A:.4f}, {upper_f1_A:.4f})\")\n",
    "\n",
    "#bootstrap for train B\n",
    "print(\"\\n=== Bootstrap Analysis for Train_B Model ===\")\n",
    "\n",
    "#load encoder \n",
    "with open(os.path.join(results_dir, 'encoder_trainB.pkl'), 'rb') as f:\n",
    "    encoder_B = pickle.load(f)\n",
    "\n",
    "#transform labels \n",
    "y_test_B = encoder_B.transform(y_test_raw)\n",
    "\n",
    "#preprocess images\n",
    "X_test_B = X_test_raw.astype(\"float32\") / 255.0\n",
    "X_test_B = np.transpose(X_test_B, (0, 3, 1, 2))\n",
    "y_test_B = y_test_B.astype(\"int64\")\n",
    "\n",
    "#load train B model\n",
    "num_classes_B = len(encoder_B.classes_)\n",
    "final_model_B = DeepCNN(num_classes_B).to(device)\n",
    "\n",
    "#dummy forward pass to initialize\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    _ = final_model_B(dummy_input)\n",
    "\n",
    "#load weights\n",
    "final_model_B.load_state_dict(torch.load(os.path.join(results_dir, 'deepcnn_final_B.pth'), map_location=device))\n",
    "\n",
    "#get  predictions\n",
    "test_ds_B = TensorDataset(torch.tensor(X_test_B), torch.tensor(y_test_B))\n",
    "test_dl_B = DataLoader(test_ds_B, batch_size=64, shuffle=False)\n",
    "\n",
    "final_model_B.eval()\n",
    "y_pred_test_B = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, _ in test_dl_B:\n",
    "        xb = xb.to(device)\n",
    "        probs = torch.softmax(final_model_B(xb), 1)\n",
    "        y_pred_test_B.append(torch.argmax(probs, 1).cpu().numpy())\n",
    "\n",
    "y_pred_test_B = np.concatenate(y_pred_test_B)\n",
    "\n",
    "#bootstrap for Train_B\n",
    "stats_B = bootstrap_metrics(y_test_B, y_pred_test_B, n_bootstrap=1000)\n",
    "\n",
    "# Extract metrics\n",
    "mean_acc_B, lower_acc_B, upper_acc_B = stats_B['acc']\n",
    "mean_prec_B, lower_prec_B, upper_prec_B = stats_B['prec']\n",
    "mean_rec_B, lower_rec_B, upper_rec_B = stats_B['rec']\n",
    "mean_f1_B, lower_f1_B, upper_f1_B = stats_B['f1']\n",
    "\n",
    "print(f\"Test Accuracy:  {mean_acc_B:.4f} ({lower_acc_B:.4f}, {upper_acc_B:.4f})\")\n",
    "print(f\"Test Precision: {mean_prec_B:.4f} ({lower_prec_B:.4f}, {upper_prec_B:.4f})\")\n",
    "print(f\"Test Recall:    {mean_rec_B:.4f} ({lower_rec_B:.4f}, {upper_rec_B:.4f})\")\n",
    "print(f\"Test F1:        {mean_f1_B:.4f} ({lower_f1_B:.4f}, {upper_f1_B:.4f})\")\n",
    "\n",
    "\n",
    "#bootstrap test accuracy error bars comparison (is the test accuracy difference significant?)\n",
    "print(\"\\n=== Final Comparison ===\")\n",
    "print(f\"Train on Clean (A): {mean_acc_A:.4f} ({lower_acc_A:.4f}, {upper_acc_A:.4f})\")\n",
    "print(f\"Train on Dirty (B): {mean_acc_B:.4f} ({lower_acc_B:.4f}, {upper_acc_B:.4f})\")\n",
    "print(f\"\\nDifference: {mean_acc_B - mean_acc_A:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8443856,
     "sourceId": 13319577,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
