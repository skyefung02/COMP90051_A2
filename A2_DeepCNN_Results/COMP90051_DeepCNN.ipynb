{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13319577,"sourceType":"datasetVersion","datasetId":8443856}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-10-10T07:26:47.880734Z","iopub.execute_input":"2025-10-10T07:26:47.881128Z","iopub.status.idle":"2025-10-10T07:26:47.891093Z","shell.execute_reply.started":"2025-10-10T07:26:47.881104Z","shell.execute_reply":"2025-10-10T07:26:47.890467Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/images-shoes/train_B.npz\n/kaggle/input/images-shoes/test.npz\n/kaggle/input/images-shoes/train_A.npz\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os, random, numpy as np\nimport torch\nfrom torchvision import models\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport gc\nimport pickle\nfrom pathlib import Path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T00:15:27.871001Z","iopub.execute_input":"2025-10-11T00:15:27.871545Z","iopub.status.idle":"2025-10-11T00:15:27.875539Z","shell.execute_reply.started":"2025-10-11T00:15:27.871525Z","shell.execute_reply":"2025-10-11T00:15:27.874814Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T00:15:30.472186Z","iopub.execute_input":"2025-10-11T00:15:30.472819Z","iopub.status.idle":"2025-10-11T00:15:30.476818Z","shell.execute_reply.started":"2025-10-11T00:15:30.472796Z","shell.execute_reply":"2025-10-11T00:15:30.476261Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"def make_folds(n, k=10, seed=42, y=None):\n    \"\"\"\n    Create stratified k-fold splits that preserve class distribution.\n    \n    Parameters:\n    - n: Total number of samples\n    - k: Number of folds (default=10)\n    - seed: Random seed for reproducibility (default=42)\n    - y: Labels for stratification (required)\n    \n    Returns:\n    - List of k arrays containing indices for each fold\n    \"\"\"\n    np.random.seed(seed)\n    \n    unique_classes = np.unique(y)\n    folds = [[] for _ in range(k)]\n    \n    # For each class, split its samples across k folds\n    for cls in unique_classes:\n        cls_indices = np.where(y == cls)[0]\n        np.random.shuffle(cls_indices)\n        cls_splits = np.array_split(cls_indices, k)\n        \n        # Add class samples to each fold\n        for fold_idx, split in enumerate(cls_splits):\n            folds[fold_idx].extend(split)\n    \n    # Shuffle within each fold and convert to numpy arrays\n    for i in range(k):\n        np.random.shuffle(folds[i])\n        folds[i] = np.array(folds[i])\n    \n    return folds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T22:33:14.308677Z","iopub.execute_input":"2025-10-10T22:33:14.309188Z","iopub.status.idle":"2025-10-10T22:33:14.314834Z","shell.execute_reply.started":"2025-10-10T22:33:14.309165Z","shell.execute_reply":"2025-10-10T22:33:14.314072Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def confusion_matrix_manual(y_true, y_pred, labels):\n    n = len(labels)\n    label_to_idx = {lab: i for i, lab in enumerate(labels)}\n    cm = np.zeros((n, n), dtype=int)\n    for yt, yp in zip(y_true, y_pred):\n        i = label_to_idx[yt]\n        j = label_to_idx[yp]\n        cm[i, j] += 1\n    return cm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T22:33:18.148806Z","iopub.execute_input":"2025-10-10T22:33:18.149134Z","iopub.status.idle":"2025-10-10T22:33:18.154112Z","shell.execute_reply.started":"2025-10-10T22:33:18.149113Z","shell.execute_reply":"2025-10-10T22:33:18.153369Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def calc_metrics(cm):\n    TP = np.diag(cm)\n    FP = cm.sum(0) - TP\n    FN = cm.sum(1) - TP\n    precision = np.mean(TP / (TP + FP + 1e-9))\n    recall    = np.mean(TP / (TP + FN + 1e-9))\n    f1 = 2 * precision * recall / (precision + recall + 1e-9)\n    acc = TP.sum() / cm.sum()\n    return acc, precision, recall, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T22:33:19.987514Z","iopub.execute_input":"2025-10-10T22:33:19.988401Z","iopub.status.idle":"2025-10-10T22:33:19.992540Z","shell.execute_reply.started":"2025-10-10T22:33:19.988376Z","shell.execute_reply":"2025-10-10T22:33:19.991954Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train_one_fold(X_train, y_train, X_val, y_val, model_builder,\n                   lr=1e-3, epochs=5, batch=64, device=\"cpu\"):\n    \"\"\"Train one fold and return model + predictions on validation set.\"\"\"\n    \n    train_ds = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n    val_ds   = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n    train_dl = DataLoader(train_ds, batch_size=batch, shuffle=True)\n    val_dl   = DataLoader(val_ds, batch_size=batch, shuffle=False)\n\n    # note: difference here — build model dynamically\n    model = model_builder().to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    opt = optim.Adam(model.parameters(), lr=lr)\n\n    for ep in range(epochs):\n        model.train()\n        total_loss = 0.0\n        for xb, yb in train_dl:\n            xb, yb = xb.to(device), yb.to(device)\n            out = model(xb)\n            loss = loss_fn(out, yb)\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            total_loss += loss.item()\n        avg_loss = total_loss / len(train_dl)\n        print(f\"  Epoch {ep+1}/{epochs}, Train loss={avg_loss:.4f}\")\n\n    # ----- validation predictions -----\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for xb, _ in val_dl:\n            xb = xb.to(device)\n            probs = torch.softmax(model(xb), 1)\n            preds.append(torch.argmax(probs, 1).cpu().numpy())\n    preds = np.concatenate(preds)\n    \n    #memory cleanup\n    del model, loss_fn, opt, train_dl, val_dl\n    if device == \"cuda\":\n        torch.cuda.empty_cache()\n    gc.collect()\n    return None, preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T22:33:26.361758Z","iopub.execute_input":"2025-10-10T22:33:26.362085Z","iopub.status.idle":"2025-10-10T22:33:26.369437Z","shell.execute_reply.started":"2025-10-10T22:33:26.362062Z","shell.execute_reply":"2025-10-10T22:33:26.368628Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def evaluate_model_nested_cv(\n    X, y, model_builder,\n    candidate_lr=[1e-3, 3e-4, 1e-4],\n    k_outer=10, k_inner=3, epochs=5,\n    device=\"cpu\"\n):\n    \"\"\"Generic nested cross‑validation for any model.\"\"\"\n\n    folds = make_folds(len(X), k_outer, seed=42, y=y)\n    metrics_all = []\n\n    for i in range(k_outer):\n        print(f\"\\n=== Outer Fold {i+1}/{k_outer} ===\")\n\n        test_idx = folds[i]\n        train_idx = np.concatenate([folds[j] for j in range(k_outer) if j != i])\n        X_train, y_train = X[train_idx], y[train_idx]\n        X_test,  y_test  = X[test_idx],  y[test_idx]\n\n        # ---- inner loop: tuning learning rate ----\n        inner_folds = make_folds(len(X_train), k_inner, seed=42, y=y_train)\n        mean_accs = []\n\n        for lr in candidate_lr:\n            inner_scores = []\n            for j in range(k_inner):\n                val_idx = inner_folds[j]\n                tr_idx  = np.concatenate([inner_folds[m] for m in range(k_inner) if m != j])\n\n                _, y_pred_val = train_one_fold(\n                    X_train[tr_idx], y_train[tr_idx],\n                    X_train[val_idx], y_train[val_idx],\n                    model_builder=model_builder,\n                    lr=lr, epochs=2, device=device\n                )\n\n                cm = confusion_matrix_manual(y_train[val_idx], y_pred_val, labels=np.unique(y))\n                acc, prec, rec, f1 = calc_metrics(cm)\n                inner_scores.append(acc)\n\n            mean_accs.append(np.mean(inner_scores))\n\n        best_lr = candidate_lr[int(np.argmax(mean_accs))]\n        print(f\"Best LR = {best_lr:.0e}\")\n\n        # ---- outer test fold ----\n        _, y_pred = train_one_fold(\n            X_train, y_train, X_test, y_test,\n            model_builder=model_builder,\n            lr=best_lr, epochs=epochs, device=device\n        )\n\n        cm = confusion_matrix_manual(y_test, y_pred, labels=np.unique(y))\n        acc, prec, rec, f1 = calc_metrics(cm)\n        metrics_all.append([acc, prec, rec, f1])\n\n        print(f\"Fold {i+1}: Acc={acc:.3f}, P={prec:.3f}, R={rec:.3f}, F1={f1:.3f}\")\n        \n        #memory cleanup\n        if device == \"cuda\":\n            torch.cuda.empty_cache()\n        gc.collect()\n\n    # ---- summary ----\n    metrics_all = np.array(metrics_all)\n    mean, std = metrics_all.mean(0), metrics_all.std(0)\n\n    print(\"\\n=== Nested CV Results ===\")\n    print(f\"Accuracy : {mean[0]:.3f} ± {std[0]:.3f}\")\n    print(f\"Precision: {mean[1]:.3f} ± {std[1]:.3f}\")\n    print(f\"Recall   : {mean[2]:.3f} ± {std[2]:.3f}\")\n    print(f\"F1-score : {mean[3]:.3f} ± {std[3]:.3f}\")\n\n    return mean, std","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T22:33:28.116790Z","iopub.execute_input":"2025-10-10T22:33:28.117539Z","iopub.status.idle":"2025-10-10T22:33:28.126849Z","shell.execute_reply.started":"2025-10-10T22:33:28.117516Z","shell.execute_reply":"2025-10-10T22:33:28.126251Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class DeepCNN(nn.Module):\n    def __init__(self, n_classes):\n        super().__init__()\n        # Feature extractor\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1),  # 32 filters\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2),                 # Downsample\n\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Dropout(0.3),\n        )\n\n        self.flatten_dim = None\n        self.classifier = None\n        self.n_classes = n_classes\n\n    def _get_flatten_dim(self, x):\n        with torch.no_grad():\n            f = self.features(x)\n            return f.view(f.size(0), -1).shape[1]\n\n    def forward(self, x):\n        if self.classifier is None:\n            flat_dim = self._get_flatten_dim(x)\n            self.classifier = nn.Sequential(\n                nn.Flatten(),\n                nn.Linear(flat_dim, 128), nn.ReLU(),\n                nn.Dropout(0.4),\n                nn.Linear(128, self.n_classes)\n            ).to(x.device)\n        out = self.features(x)\n        out = self.classifier(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T22:33:30.727706Z","iopub.execute_input":"2025-10-10T22:33:30.727978Z","iopub.status.idle":"2025-10-10T22:33:30.734616Z","shell.execute_reply.started":"2025-10-10T22:33:30.727959Z","shell.execute_reply":"2025-10-10T22:33:30.734069Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"Training on train_B.npz (dirty images)","metadata":{}},{"cell_type":"code","source":"data_dir = \"/kaggle/input/images-shoes\"\ndata = np.load(os.path.join(data_dir, \"train_B.npz\"))  # or train_A.npz\nX, y = data[\"X\"], data[\"y\"]\nprint(\"Loaded:\", X.shape, y.shape)\n\n# ---------- encode string labels to ints ----------\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)            # e.g. Boot→0, Sandal→1, Shoe→2\n\n#save encoder\nwith open('/kaggle/working/encoder_trainB.pkl', 'wb') as f:\n    pickle.dump(encoder, f)\n\nprint(\"Label mapping:\", dict(zip(encoder.classes_,\n                                 range(len(encoder.classes_)))))\n\n# ---------- normalise images ----------\nX = X.astype(\"float32\") / 255.0\n\n# ---------- reshape for PyTorch (N,C,H,W) ----------\nX = np.transpose(X, (0,3,1,2))\ny = y.astype(\"int64\")\nnum_classes = len(np.unique(y))\nprint(\"Final tensors:\", X.shape, \"Classes:\", num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T00:04:18.594568Z","iopub.execute_input":"2025-10-11T00:04:18.594904Z","iopub.status.idle":"2025-10-11T00:04:37.621441Z","shell.execute_reply.started":"2025-10-11T00:04:18.594879Z","shell.execute_reply":"2025-10-11T00:04:37.620783Z"}},"outputs":[{"name":"stdout","text":"Loaded: (12000, 224, 224, 3) (12000,)\nLabel mapping: {'Boot': 0, 'Sandal': 1, 'Shoe': 2}\nFinal tensors: (12000, 3, 224, 224) Classes: 3\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# --- DeepCNN ---\nprint(\"\\n### Evaluating DeepCNN ###\")\ndeep_mean_B, deep_std_B = evaluate_model_nested_cv(\n    X, y,\n    model_builder=lambda: DeepCNN(num_classes),\n    candidate_lr=[3e-3, 1e-3, 3e-4],\n    k_outer=10,\n    k_inner=3,\n    epochs=3,\n    device=device\n)\n\n#saving results\nimport pickle\n\ncv_results_B = {\n    'mean': deep_mean_B,\n    'std': deep_std_B,\n    'accuracy': deep_mean_B[0],\n    'precision': deep_mean_B[1],\n    'recall': deep_mean_B[2],\n    'f1': deep_mean_B[3]\n}\n\nwith open('/kaggle/working/cv_results_trainB.pkl', 'wb') as f:\n    pickle.dump(cv_results_B, f)\n\nprint(\"CV results saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T07:27:06.871897Z","iopub.execute_input":"2025-10-10T07:27:06.872130Z","iopub.status.idle":"2025-10-10T08:50:45.892756Z","shell.execute_reply.started":"2025-10-10T07:27:06.872113Z","shell.execute_reply":"2025-10-10T08:50:45.892155Z"}},"outputs":[{"name":"stdout","text":"\n### Evaluating DeepCNN ###\n\n=== Outer Fold 1/10 ===\n  Epoch 1/2, Train loss=0.9186\n  Epoch 2/2, Train loss=0.7659\n  Epoch 1/2, Train loss=0.9090\n  Epoch 2/2, Train loss=0.7549\n  Epoch 1/2, Train loss=0.9340\n  Epoch 2/2, Train loss=0.7718\n  Epoch 1/2, Train loss=0.9266\n  Epoch 2/2, Train loss=0.7782\n  Epoch 1/2, Train loss=0.9270\n  Epoch 2/2, Train loss=0.7841\n  Epoch 1/2, Train loss=0.9262\n  Epoch 2/2, Train loss=0.7785\n  Epoch 1/2, Train loss=0.9811\n  Epoch 2/2, Train loss=0.8617\n  Epoch 1/2, Train loss=0.9634\n  Epoch 2/2, Train loss=0.8492\n  Epoch 1/2, Train loss=0.9903\n  Epoch 2/2, Train loss=0.8747\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.8735\n  Epoch 2/3, Train loss=0.7241\n  Epoch 3/3, Train loss=0.6613\nFold 1: Acc=0.816, P=0.819, R=0.816, F1=0.817\n\n=== Outer Fold 2/10 ===\n  Epoch 1/2, Train loss=0.9078\n  Epoch 2/2, Train loss=0.7662\n  Epoch 1/2, Train loss=0.9118\n  Epoch 2/2, Train loss=0.7604\n  Epoch 1/2, Train loss=0.9299\n  Epoch 2/2, Train loss=0.7572\n  Epoch 1/2, Train loss=0.9094\n  Epoch 2/2, Train loss=0.7652\n  Epoch 1/2, Train loss=0.9373\n  Epoch 2/2, Train loss=0.7936\n  Epoch 1/2, Train loss=0.9235\n  Epoch 2/2, Train loss=0.7740\n  Epoch 1/2, Train loss=0.9718\n  Epoch 2/2, Train loss=0.8392\n  Epoch 1/2, Train loss=0.9828\n  Epoch 2/2, Train loss=0.8678\n  Epoch 1/2, Train loss=0.9868\n  Epoch 2/2, Train loss=0.8601\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.8960\n  Epoch 2/3, Train loss=0.7360\n  Epoch 3/3, Train loss=0.6731\nFold 2: Acc=0.826, P=0.825, R=0.826, F1=0.826\n\n=== Outer Fold 3/10 ===\n  Epoch 1/2, Train loss=0.9284\n  Epoch 2/2, Train loss=0.7784\n  Epoch 1/2, Train loss=0.9397\n  Epoch 2/2, Train loss=0.7650\n  Epoch 1/2, Train loss=0.9170\n  Epoch 2/2, Train loss=0.7479\n  Epoch 1/2, Train loss=0.9276\n  Epoch 2/2, Train loss=0.7739\n  Epoch 1/2, Train loss=0.9087\n  Epoch 2/2, Train loss=0.7679\n  Epoch 1/2, Train loss=0.9245\n  Epoch 2/2, Train loss=0.7657\n  Epoch 1/2, Train loss=0.9487\n  Epoch 2/2, Train loss=0.8355\n  Epoch 1/2, Train loss=0.9909\n  Epoch 2/2, Train loss=0.8664\n  Epoch 1/2, Train loss=0.9691\n  Epoch 2/2, Train loss=0.8424\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.8998\n  Epoch 2/3, Train loss=0.7375\n  Epoch 3/3, Train loss=0.6757\nFold 3: Acc=0.800, P=0.806, R=0.800, F1=0.803\n\n=== Outer Fold 4/10 ===\n  Epoch 1/2, Train loss=0.9373\n  Epoch 2/2, Train loss=0.7715\n  Epoch 1/2, Train loss=0.9412\n  Epoch 2/2, Train loss=0.7820\n  Epoch 1/2, Train loss=0.9307\n  Epoch 2/2, Train loss=0.7690\n  Epoch 1/2, Train loss=0.9452\n  Epoch 2/2, Train loss=0.7950\n  Epoch 1/2, Train loss=0.9194\n  Epoch 2/2, Train loss=0.7708\n  Epoch 1/2, Train loss=0.8967\n  Epoch 2/2, Train loss=0.7586\n  Epoch 1/2, Train loss=0.9745\n  Epoch 2/2, Train loss=0.8620\n  Epoch 1/2, Train loss=0.9922\n  Epoch 2/2, Train loss=0.8700\n  Epoch 1/2, Train loss=0.9510\n  Epoch 2/2, Train loss=0.8183\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.8729\n  Epoch 2/3, Train loss=0.7257\n  Epoch 3/3, Train loss=0.6658\nFold 4: Acc=0.798, P=0.808, R=0.798, F1=0.803\n\n=== Outer Fold 5/10 ===\n  Epoch 1/2, Train loss=0.9280\n  Epoch 2/2, Train loss=0.7584\n  Epoch 1/2, Train loss=0.9158\n  Epoch 2/2, Train loss=0.7577\n  Epoch 1/2, Train loss=0.9149\n  Epoch 2/2, Train loss=0.7534\n  Epoch 1/2, Train loss=0.9379\n  Epoch 2/2, Train loss=0.7935\n  Epoch 1/2, Train loss=0.9407\n  Epoch 2/2, Train loss=0.7859\n  Epoch 1/2, Train loss=0.9230\n  Epoch 2/2, Train loss=0.7779\n  Epoch 1/2, Train loss=0.9937\n  Epoch 2/2, Train loss=0.8663\n  Epoch 1/2, Train loss=0.9807\n  Epoch 2/2, Train loss=0.8667\n  Epoch 1/2, Train loss=0.9713\n  Epoch 2/2, Train loss=0.8478\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.8990\n  Epoch 2/3, Train loss=0.7537\n  Epoch 3/3, Train loss=0.6842\nFold 5: Acc=0.811, P=0.815, R=0.811, F1=0.813\n\n=== Outer Fold 6/10 ===\n  Epoch 1/2, Train loss=0.9161\n  Epoch 2/2, Train loss=0.7621\n  Epoch 1/2, Train loss=0.9020\n  Epoch 2/2, Train loss=0.7513\n  Epoch 1/2, Train loss=0.9281\n  Epoch 2/2, Train loss=0.7737\n  Epoch 1/2, Train loss=0.9306\n  Epoch 2/2, Train loss=0.7866\n  Epoch 1/2, Train loss=0.9405\n  Epoch 2/2, Train loss=0.7973\n  Epoch 1/2, Train loss=0.9328\n  Epoch 2/2, Train loss=0.7832\n  Epoch 1/2, Train loss=0.9759\n  Epoch 2/2, Train loss=0.8432\n  Epoch 1/2, Train loss=0.9806\n  Epoch 2/2, Train loss=0.8575\n  Epoch 1/2, Train loss=0.9749\n  Epoch 2/2, Train loss=0.8674\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.8915\n  Epoch 2/3, Train loss=0.7462\n  Epoch 3/3, Train loss=0.6832\nFold 6: Acc=0.800, P=0.824, R=0.800, F1=0.812\n\n=== Outer Fold 7/10 ===\n  Epoch 1/2, Train loss=0.9096\n  Epoch 2/2, Train loss=0.7480\n  Epoch 1/2, Train loss=0.9573\n  Epoch 2/2, Train loss=0.8083\n  Epoch 1/2, Train loss=0.9400\n  Epoch 2/2, Train loss=0.7754\n  Epoch 1/2, Train loss=0.9423\n  Epoch 2/2, Train loss=0.7932\n  Epoch 1/2, Train loss=0.9311\n  Epoch 2/2, Train loss=0.7826\n  Epoch 1/2, Train loss=0.9015\n  Epoch 2/2, Train loss=0.7433\n  Epoch 1/2, Train loss=0.9817\n  Epoch 2/2, Train loss=0.8597\n  Epoch 1/2, Train loss=0.9957\n  Epoch 2/2, Train loss=0.8809\n  Epoch 1/2, Train loss=0.9806\n  Epoch 2/2, Train loss=0.8385\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.8899\n  Epoch 2/3, Train loss=0.7334\n  Epoch 3/3, Train loss=0.6693\nFold 7: Acc=0.792, P=0.799, R=0.792, F1=0.795\n\n=== Outer Fold 8/10 ===\n  Epoch 1/2, Train loss=0.9108\n  Epoch 2/2, Train loss=0.7549\n  Epoch 1/2, Train loss=0.9104\n  Epoch 2/2, Train loss=0.7448\n  Epoch 1/2, Train loss=0.9049\n  Epoch 2/2, Train loss=0.7423\n  Epoch 1/2, Train loss=0.9288\n  Epoch 2/2, Train loss=0.7794\n  Epoch 1/2, Train loss=0.9331\n  Epoch 2/2, Train loss=0.7861\n  Epoch 1/2, Train loss=0.9179\n  Epoch 2/2, Train loss=0.7766\n  Epoch 1/2, Train loss=0.9852\n  Epoch 2/2, Train loss=0.8619\n  Epoch 1/2, Train loss=0.9743\n  Epoch 2/2, Train loss=0.8637\n  Epoch 1/2, Train loss=0.9855\n  Epoch 2/2, Train loss=0.8506\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.8887\n  Epoch 2/3, Train loss=0.7375\n  Epoch 3/3, Train loss=0.6921\nFold 8: Acc=0.747, P=0.765, R=0.747, F1=0.756\n\n=== Outer Fold 9/10 ===\n  Epoch 1/2, Train loss=0.9296\n  Epoch 2/2, Train loss=0.7690\n  Epoch 1/2, Train loss=0.8945\n  Epoch 2/2, Train loss=0.7304\n  Epoch 1/2, Train loss=0.9410\n  Epoch 2/2, Train loss=0.7901\n  Epoch 1/2, Train loss=0.9279\n  Epoch 2/2, Train loss=0.7860\n  Epoch 1/2, Train loss=0.9133\n  Epoch 2/2, Train loss=0.7642\n  Epoch 1/2, Train loss=0.9565\n  Epoch 2/2, Train loss=0.8234\n  Epoch 1/2, Train loss=0.9638\n  Epoch 2/2, Train loss=0.8389\n  Epoch 1/2, Train loss=0.9606\n  Epoch 2/2, Train loss=0.8267\n  Epoch 1/2, Train loss=0.9699\n  Epoch 2/2, Train loss=0.8468\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.8813\n  Epoch 2/3, Train loss=0.7242\n  Epoch 3/3, Train loss=0.6602\nFold 9: Acc=0.808, P=0.815, R=0.808, F1=0.812\n\n=== Outer Fold 10/10 ===\n  Epoch 1/2, Train loss=0.8999\n  Epoch 2/2, Train loss=0.7530\n  Epoch 1/2, Train loss=0.9056\n  Epoch 2/2, Train loss=0.7406\n  Epoch 1/2, Train loss=0.9102\n  Epoch 2/2, Train loss=0.7433\n  Epoch 1/2, Train loss=0.9235\n  Epoch 2/2, Train loss=0.7729\n  Epoch 1/2, Train loss=0.9323\n  Epoch 2/2, Train loss=0.7769\n  Epoch 1/2, Train loss=0.9442\n  Epoch 2/2, Train loss=0.7979\n  Epoch 1/2, Train loss=0.9729\n  Epoch 2/2, Train loss=0.8379\n  Epoch 1/2, Train loss=0.9589\n  Epoch 2/2, Train loss=0.8242\n  Epoch 1/2, Train loss=0.9862\n  Epoch 2/2, Train loss=0.8468\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.9017\n  Epoch 2/3, Train loss=0.7497\n  Epoch 3/3, Train loss=0.6752\nFold 10: Acc=0.788, P=0.787, R=0.788, F1=0.788\n\n=== Nested CV Results ===\nAccuracy : 0.799 ± 0.020\nPrecision: 0.806 ± 0.018\nRecall   : 0.799 ± 0.020\nF1-score : 0.802 ± 0.019\nCV results saved!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"Training Final Model","metadata":{}},{"cell_type":"code","source":"def train_final_model(X, y, model_builder, lr=1e-3, epochs=5, batch=64, device=\"cpu\"):\n    \"\"\"Train final model on full dataset (no validation split).\"\"\"\n    \n    train_ds = TensorDataset(torch.tensor(X), torch.tensor(y))\n    train_dl = DataLoader(train_ds, batch_size=batch, shuffle=True)\n    \n    model = model_builder().to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    opt = optim.Adam(model.parameters(), lr=lr)\n    \n    for ep in range(epochs):\n        model.train()\n        total_loss = 0.0\n        for xb, yb in train_dl:\n            xb, yb = xb.to(device), yb.to(device)\n            out = model(xb)\n            loss = loss_fn(out, yb)\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            total_loss += loss.item()\n        avg_loss = total_loss / len(train_dl)\n        print(f\"  Epoch {ep+1}/{epochs}, Train loss={avg_loss:.4f}\")\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T23:58:58.423207Z","iopub.execute_input":"2025-10-10T23:58:58.423463Z","iopub.status.idle":"2025-10-10T23:58:58.429123Z","shell.execute_reply.started":"2025-10-10T23:58:58.423445Z","shell.execute_reply":"2025-10-10T23:58:58.428393Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# After nested CV\nprint(\"\\n=== Training Final Model on Full Dataset (Dirty Images) ===\")\n\nfinal_model = train_final_model(\n    X, y,\n    model_builder=lambda: DeepCNN(num_classes),\n    lr=1e-3,\n    epochs=5,\n    batch=64,\n    device=device\n)\n\ntorch.save(final_model.state_dict(), '/kaggle/working/deepcnn_final_B.pth')\nprint(\"Final model saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T00:05:21.354943Z","iopub.execute_input":"2025-10-11T00:05:21.355435Z","iopub.status.idle":"2025-10-11T00:07:56.533808Z","shell.execute_reply.started":"2025-10-11T00:05:21.355412Z","shell.execute_reply":"2025-10-11T00:07:56.533164Z"}},"outputs":[{"name":"stdout","text":"\n=== Training Final Model on Full Dataset (Dirty Images) ===\n  Epoch 1/5, Train loss=0.8796\n  Epoch 2/5, Train loss=0.7210\n  Epoch 3/5, Train loss=0.6518\n  Epoch 4/5, Train loss=0.6117\n  Epoch 5/5, Train loss=0.5837\nFinal model saved!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"Testing Model","metadata":{}},{"cell_type":"code","source":"# ===== LOAD AND PREPARE TEST DATA =====\nprint(\"\\n=== Loading Test Data ===\")\ntest_data = np.load(os.path.join(data_dir, \"test.npz\"))\nX_test, y_test = test_data[\"X\"], test_data[\"y\"]\nprint(\"Loaded test data:\", X_test.shape, y_test.shape)\n\n#loading correct encoder\nwith open('/kaggle/working/encoder_trainB.pkl', 'rb') as f:\n    encoder = pickle.load(f)\n\n# Encode labels (same encoder from training)\ny_test = encoder.transform(y_test)\n\n# Normalize images\nX_test = X_test.astype(\"float32\") / 255.0\n\n# Reshape for PyTorch (N,C,H,W)\nX_test = np.transpose(X_test, (0,3,1,2))\ny_test = y_test.astype(\"int64\")\n\nprint(\"Test data prepared:\", X_test.shape)\n\n# ===== MAKE PREDICTIONS =====\nprint(\"\\n=== Evaluating Final Model on Test Set ===\")\n\ntest_ds = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\ntest_dl = DataLoader(test_ds, batch_size=64, shuffle=False)\n\nfinal_model.eval()\ny_pred_test = []\n\nwith torch.no_grad():\n    for xb, _ in test_dl:\n        xb = xb.to(device)\n        probs = torch.softmax(final_model(xb), 1)\n        y_pred_test.append(torch.argmax(probs, 1).cpu().numpy())\n\ny_pred_test = np.concatenate(y_pred_test)\n\n# ===== CALCULATE METRICS =====\ncm_test = confusion_matrix_manual(y_test, y_pred_test, labels=np.unique(y_test))\nacc_test, prec_test, rec_test, f1_test = calc_metrics(cm_test)\n\nprint(f\"\\n=== Test Set Results ===\")\nprint(f\"Accuracy : {acc_test:.3f}\")\nprint(f\"Precision: {prec_test:.3f}\")\nprint(f\"Recall   : {rec_test:.3f}\")\nprint(f\"F1-score : {f1_test:.3f}\")\n\nprint(f\"\\nConfusion Matrix:\")\nprint(cm_test)\nprint(f\"Classes: {encoder.classes_}\")\n\n# ===== COMPARISON WITH CV =====\nwith open('/kaggle/working/cv_results_trainB.pkl', 'rb') as f:\n    cv_results_B = pickle.load(f)\n\n# Extract the variables you need\ndeep_mean_B = cv_results_B['mean']\ndeep_std_B = cv_results_B['std']\n\nprint(f\"\\n=== Performance Comparison ===\")\nprint(f\"CV Performance:   {deep_mean_B[0]:.3f} ± {deep_std_B[0]:.3f}\")  # ✅ Use index [0] for accuracy\nprint(f\"Test Performance: {acc_test:.3f}\")\nprint(f\"Difference:       {acc_test - deep_mean_B[0]:.3f}\")  # ✅ Use index [0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T09:16:20.905157Z","iopub.execute_input":"2025-10-10T09:16:20.905534Z","iopub.status.idle":"2025-10-10T09:16:28.554618Z","shell.execute_reply.started":"2025-10-10T09:16:20.905509Z","shell.execute_reply":"2025-10-10T09:16:28.553860Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading Test Data ===\nLoaded test data: (3000, 224, 224, 3) (3000,)\nTest data prepared: (3000, 3, 224, 224)\n\n=== Evaluating Final Model on Test Set ===\n\n=== Test Set Results ===\nAccuracy : 0.843\nPrecision: 0.842\nRecall   : 0.843\nF1-score : 0.842\n\nConfusion Matrix:\n[[946  30  24]\n [108 757 135]\n [ 46 128 826]]\nClasses: ['Boot' 'Sandal' 'Shoe']\n\n=== Performance Comparison ===\nCV Performance:   0.799 ± 0.020\nTest Performance: 0.843\nDifference:       0.044\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### Repeating the above, but this time using train_A (which is the image set with only clean images)","metadata":{}},{"cell_type":"code","source":"data_dir = \"/kaggle/input/images-shoes\"\ndata = np.load(os.path.join(data_dir, \"train_A.npz\"))  #changed to train_A.npz\nX, y = data[\"X\"], data[\"y\"]\nprint(\"Loaded:\", X.shape, y.shape)\n\n# ---------- encode string labels to ints ----------\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)            # e.g. Boot→0, Sandal→1, Shoe→2\n\n#save encoder\nwith open('/kaggle/working/encoder_trainA.pkl', 'wb') as f:\n    pickle.dump(encoder, f)\n    \nprint(\"Label mapping:\", dict(zip(encoder.classes_,\n                                 range(len(encoder.classes_)))))\n\n# ---------- normalise images ----------\nX = X.astype(\"float32\") / 255.0\n\n# ---------- reshape for PyTorch (N,C,H,W) ----------\nX = np.transpose(X, (0,3,1,2))\ny = y.astype(\"int64\")\nnum_classes = len(np.unique(y))\nprint(\"Final tensors:\", X.shape, \"Classes:\", num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T22:33:41.799934Z","iopub.execute_input":"2025-10-10T22:33:41.800682Z","iopub.status.idle":"2025-10-10T22:33:58.832988Z","shell.execute_reply.started":"2025-10-10T22:33:41.800660Z","shell.execute_reply":"2025-10-10T22:33:58.832215Z"}},"outputs":[{"name":"stdout","text":"Loaded: (12000, 224, 224, 3) (12000,)\nLabel mapping: {'Boot': 0, 'Sandal': 1, 'Shoe': 2}\nFinal tensors: (12000, 3, 224, 224) Classes: 3\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# --- DeepCNN --- \nprint(\"\\n### Evaluating DeepCNN ###\")\ndeep_mean_A, deep_std_A = evaluate_model_nested_cv(\n    X, y,\n    model_builder=lambda: DeepCNN(num_classes),\n    candidate_lr=[3e-3, 1e-3, 3e-4],\n    k_outer=10,\n    k_inner=3,\n    epochs=3,\n    device=device\n)\n\nimport pickle\n\ncv_results_A = {\n    'mean': deep_mean_A,\n    'std': deep_std_A,\n    'accuracy': deep_mean_A[0],\n    'precision': deep_mean_A[1],\n    'recall': deep_mean_A[2],\n    'f1': deep_mean_A[3]\n}\n\nwith open('/kaggle/working/cv_results_trainA.pkl', 'wb') as f:\n    pickle.dump(cv_results_A, f)\n\nprint(\"CV results saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T22:34:10.743997Z","iopub.execute_input":"2025-10-10T22:34:10.744291Z","iopub.status.idle":"2025-10-10T23:57:45.941486Z","shell.execute_reply.started":"2025-10-10T22:34:10.744272Z","shell.execute_reply":"2025-10-10T23:57:45.940801Z"}},"outputs":[{"name":"stdout","text":"\n### Evaluating DeepCNN ###\n\n=== Outer Fold 1/10 ===\n  Epoch 1/2, Train loss=0.7664\n  Epoch 2/2, Train loss=0.5608\n  Epoch 1/2, Train loss=0.7626\n  Epoch 2/2, Train loss=0.5600\n  Epoch 1/2, Train loss=0.7324\n  Epoch 2/2, Train loss=0.5504\n  Epoch 1/2, Train loss=0.7799\n  Epoch 2/2, Train loss=0.6029\n  Epoch 1/2, Train loss=0.7233\n  Epoch 2/2, Train loss=0.5521\n  Epoch 1/2, Train loss=0.7769\n  Epoch 2/2, Train loss=0.5930\n  Epoch 1/2, Train loss=0.8542\n  Epoch 2/2, Train loss=0.6773\n  Epoch 1/2, Train loss=0.8415\n  Epoch 2/2, Train loss=0.6645\n  Epoch 1/2, Train loss=0.8234\n  Epoch 2/2, Train loss=0.6538\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.7051\n  Epoch 2/3, Train loss=0.5408\n  Epoch 3/3, Train loss=0.4639\nFold 1: Acc=0.900, P=0.900, R=0.900, F1=0.900\n\n=== Outer Fold 2/10 ===\n  Epoch 1/2, Train loss=0.7760\n  Epoch 2/2, Train loss=0.5782\n  Epoch 1/2, Train loss=0.7593\n  Epoch 2/2, Train loss=0.5623\n  Epoch 1/2, Train loss=0.7432\n  Epoch 2/2, Train loss=0.5540\n  Epoch 1/2, Train loss=0.7675\n  Epoch 2/2, Train loss=0.5760\n  Epoch 1/2, Train loss=0.7758\n  Epoch 2/2, Train loss=0.5939\n  Epoch 1/2, Train loss=0.7447\n  Epoch 2/2, Train loss=0.5619\n  Epoch 1/2, Train loss=0.8127\n  Epoch 2/2, Train loss=0.6503\n  Epoch 1/2, Train loss=0.8231\n  Epoch 2/2, Train loss=0.6524\n  Epoch 1/2, Train loss=0.8242\n  Epoch 2/2, Train loss=0.6654\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.7254\n  Epoch 2/3, Train loss=0.5474\n  Epoch 3/3, Train loss=0.4812\nFold 2: Acc=0.903, P=0.905, R=0.903, F1=0.904\n\n=== Outer Fold 3/10 ===\n  Epoch 1/2, Train loss=0.7262\n  Epoch 2/2, Train loss=0.5503\n  Epoch 1/2, Train loss=0.7592\n  Epoch 2/2, Train loss=0.5633\n  Epoch 1/2, Train loss=0.7475\n  Epoch 2/2, Train loss=0.5578\n  Epoch 1/2, Train loss=0.7758\n  Epoch 2/2, Train loss=0.5970\n  Epoch 1/2, Train loss=0.7762\n  Epoch 2/2, Train loss=0.5994\n  Epoch 1/2, Train loss=0.7191\n  Epoch 2/2, Train loss=0.5633\n  Epoch 1/2, Train loss=0.8216\n  Epoch 2/2, Train loss=0.6482\n  Epoch 1/2, Train loss=0.8641\n  Epoch 2/2, Train loss=0.6798\n  Epoch 1/2, Train loss=0.8286\n  Epoch 2/2, Train loss=0.6455\nBest LR = 3e-03\n  Epoch 1/3, Train loss=0.6838\n  Epoch 2/3, Train loss=0.4895\n  Epoch 3/3, Train loss=0.4193\nFold 3: Acc=0.914, P=0.914, R=0.914, F1=0.914\n\n=== Outer Fold 4/10 ===\n  Epoch 1/2, Train loss=0.7191\n  Epoch 2/2, Train loss=0.5316\n  Epoch 1/2, Train loss=0.7510\n  Epoch 2/2, Train loss=0.5576\n  Epoch 1/2, Train loss=0.7401\n  Epoch 2/2, Train loss=0.5569\n  Epoch 1/2, Train loss=0.7751\n  Epoch 2/2, Train loss=0.5885\n  Epoch 1/2, Train loss=0.7515\n  Epoch 2/2, Train loss=0.5796\n  Epoch 1/2, Train loss=0.7644\n  Epoch 2/2, Train loss=0.5918\n  Epoch 1/2, Train loss=0.8066\n  Epoch 2/2, Train loss=0.6357\n  Epoch 1/2, Train loss=0.8705\n  Epoch 2/2, Train loss=0.6827\n  Epoch 1/2, Train loss=0.8428\n  Epoch 2/2, Train loss=0.6753\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.7323\n  Epoch 2/3, Train loss=0.5551\n  Epoch 3/3, Train loss=0.4723\nFold 4: Acc=0.887, P=0.892, R=0.887, F1=0.889\n\n=== Outer Fold 5/10 ===\n  Epoch 1/2, Train loss=0.7505\n  Epoch 2/2, Train loss=0.5690\n  Epoch 1/2, Train loss=0.7268\n  Epoch 2/2, Train loss=0.5533\n  Epoch 1/2, Train loss=0.7313\n  Epoch 2/2, Train loss=0.5429\n  Epoch 1/2, Train loss=0.7574\n  Epoch 2/2, Train loss=0.5746\n  Epoch 1/2, Train loss=0.7452\n  Epoch 2/2, Train loss=0.5759\n  Epoch 1/2, Train loss=0.7745\n  Epoch 2/2, Train loss=0.5834\n  Epoch 1/2, Train loss=0.8313\n  Epoch 2/2, Train loss=0.6538\n  Epoch 1/2, Train loss=0.8427\n  Epoch 2/2, Train loss=0.6811\n  Epoch 1/2, Train loss=0.8365\n  Epoch 2/2, Train loss=0.6615\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.7200\n  Epoch 2/3, Train loss=0.5411\n  Epoch 3/3, Train loss=0.4655\nFold 5: Acc=0.860, P=0.881, R=0.860, F1=0.870\n\n=== Outer Fold 6/10 ===\n  Epoch 1/2, Train loss=0.7529\n  Epoch 2/2, Train loss=0.5696\n  Epoch 1/2, Train loss=0.7579\n  Epoch 2/2, Train loss=0.5754\n  Epoch 1/2, Train loss=0.7381\n  Epoch 2/2, Train loss=0.5533\n  Epoch 1/2, Train loss=0.7516\n  Epoch 2/2, Train loss=0.5744\n  Epoch 1/2, Train loss=0.7569\n  Epoch 2/2, Train loss=0.5816\n  Epoch 1/2, Train loss=0.7791\n  Epoch 2/2, Train loss=0.5941\n  Epoch 1/2, Train loss=0.8369\n  Epoch 2/2, Train loss=0.6640\n  Epoch 1/2, Train loss=0.8197\n  Epoch 2/2, Train loss=0.6460\n  Epoch 1/2, Train loss=0.8234\n  Epoch 2/2, Train loss=0.6551\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.7283\n  Epoch 2/3, Train loss=0.5422\n  Epoch 3/3, Train loss=0.4660\nFold 6: Acc=0.896, P=0.897, R=0.896, F1=0.897\n\n=== Outer Fold 7/10 ===\n  Epoch 1/2, Train loss=0.7165\n  Epoch 2/2, Train loss=0.5272\n  Epoch 1/2, Train loss=0.7429\n  Epoch 2/2, Train loss=0.5717\n  Epoch 1/2, Train loss=0.7682\n  Epoch 2/2, Train loss=0.5742\n  Epoch 1/2, Train loss=0.7581\n  Epoch 2/2, Train loss=0.5847\n  Epoch 1/2, Train loss=0.7869\n  Epoch 2/2, Train loss=0.5980\n  Epoch 1/2, Train loss=0.7958\n  Epoch 2/2, Train loss=0.6104\n  Epoch 1/2, Train loss=0.8572\n  Epoch 2/2, Train loss=0.6822\n  Epoch 1/2, Train loss=0.8691\n  Epoch 2/2, Train loss=0.6938\n  Epoch 1/2, Train loss=0.8322\n  Epoch 2/2, Train loss=0.6550\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.7375\n  Epoch 2/3, Train loss=0.5536\n  Epoch 3/3, Train loss=0.4818\nFold 7: Acc=0.888, P=0.896, R=0.888, F1=0.892\n\n=== Outer Fold 8/10 ===\n  Epoch 1/2, Train loss=0.7675\n  Epoch 2/2, Train loss=0.5567\n  Epoch 1/2, Train loss=0.7820\n  Epoch 2/2, Train loss=0.5654\n  Epoch 1/2, Train loss=0.7374\n  Epoch 2/2, Train loss=0.5351\n  Epoch 1/2, Train loss=0.7307\n  Epoch 2/2, Train loss=0.5620\n  Epoch 1/2, Train loss=0.7647\n  Epoch 2/2, Train loss=0.5821\n  Epoch 1/2, Train loss=0.7559\n  Epoch 2/2, Train loss=0.5664\n  Epoch 1/2, Train loss=0.8373\n  Epoch 2/2, Train loss=0.6645\n  Epoch 1/2, Train loss=0.8612\n  Epoch 2/2, Train loss=0.6665\n  Epoch 1/2, Train loss=0.8244\n  Epoch 2/2, Train loss=0.6340\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.6937\n  Epoch 2/3, Train loss=0.5180\n  Epoch 3/3, Train loss=0.4488\nFold 8: Acc=0.900, P=0.900, R=0.900, F1=0.900\n\n=== Outer Fold 9/10 ===\n  Epoch 1/2, Train loss=0.7216\n  Epoch 2/2, Train loss=0.5385\n  Epoch 1/2, Train loss=0.7265\n  Epoch 2/2, Train loss=0.5405\n  Epoch 1/2, Train loss=0.7099\n  Epoch 2/2, Train loss=0.5383\n  Epoch 1/2, Train loss=0.7964\n  Epoch 2/2, Train loss=0.6157\n  Epoch 1/2, Train loss=0.7862\n  Epoch 2/2, Train loss=0.6106\n  Epoch 1/2, Train loss=0.7744\n  Epoch 2/2, Train loss=0.5734\n  Epoch 1/2, Train loss=0.8367\n  Epoch 2/2, Train loss=0.6787\n  Epoch 1/2, Train loss=0.8470\n  Epoch 2/2, Train loss=0.6722\n  Epoch 1/2, Train loss=0.8120\n  Epoch 2/2, Train loss=0.6433\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.7196\n  Epoch 2/3, Train loss=0.5474\n  Epoch 3/3, Train loss=0.4809\nFold 9: Acc=0.882, P=0.882, R=0.882, F1=0.882\n\n=== Outer Fold 10/10 ===\n  Epoch 1/2, Train loss=0.7573\n  Epoch 2/2, Train loss=0.5600\n  Epoch 1/2, Train loss=0.7362\n  Epoch 2/2, Train loss=0.5524\n  Epoch 1/2, Train loss=0.7617\n  Epoch 2/2, Train loss=0.5474\n  Epoch 1/2, Train loss=0.7578\n  Epoch 2/2, Train loss=0.5827\n  Epoch 1/2, Train loss=0.7705\n  Epoch 2/2, Train loss=0.5853\n  Epoch 1/2, Train loss=0.7773\n  Epoch 2/2, Train loss=0.6014\n  Epoch 1/2, Train loss=0.8633\n  Epoch 2/2, Train loss=0.7012\n  Epoch 1/2, Train loss=0.8461\n  Epoch 2/2, Train loss=0.6576\n  Epoch 1/2, Train loss=0.8695\n  Epoch 2/2, Train loss=0.6779\nBest LR = 3e-03\n  Epoch 1/3, Train loss=0.6487\n  Epoch 2/3, Train loss=0.4697\n  Epoch 3/3, Train loss=0.3977\nFold 10: Acc=0.868, P=0.882, R=0.867, F1=0.875\n\n=== Nested CV Results ===\nAccuracy : 0.890 ± 0.016\nPrecision: 0.895 ± 0.010\nRecall   : 0.890 ± 0.016\nF1-score : 0.892 ± 0.013\nCV results saved!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# After nested CV\nprint(\"\\n=== Training Final Model on Full Dataset (Clean Images) ===\")\n\nfinal_model_clean = train_final_model(\n    X, y,\n    model_builder=lambda: DeepCNN(num_classes),\n    lr=1e-3,\n    epochs=5,\n    batch=64,\n    device=device\n)\n\ntorch.save(final_model_clean.state_dict(), '/kaggle/working/deepcnn_final_clean.pth')\nprint(\"Final model saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-10T23:59:03.779213Z","iopub.execute_input":"2025-10-10T23:59:03.779472Z","iopub.status.idle":"2025-10-11T00:01:38.651060Z","shell.execute_reply.started":"2025-10-10T23:59:03.779454Z","shell.execute_reply":"2025-10-11T00:01:38.650365Z"}},"outputs":[{"name":"stdout","text":"\n=== Training Final Model on Full Dataset (Clean Images) ===\n  Epoch 1/5, Train loss=0.6687\n  Epoch 2/5, Train loss=0.4961\n  Epoch 3/5, Train loss=0.4271\n  Epoch 4/5, Train loss=0.3816\n  Epoch 5/5, Train loss=0.3456\nFinal model saved!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ===== LOAD AND PREPARE TEST DATA =====\nprint(\"\\n=== Loading Test Data (for Train_A Model) ===\")\ntest_data = np.load(os.path.join(data_dir, \"test.npz\"))\nX_test, y_test = test_data[\"X\"], test_data[\"y\"]\nprint(\"Loaded test data:\", X_test.shape, y_test.shape)\n\n#loading correct encoder\nwith open('/kaggle/working/encoder_trainA.pkl', 'rb') as f:\n    encoder = pickle.load(f)\n    \n# Encode labels\ny_test = encoder.transform(y_test)  # Make sure this is the encoder fitted on train_A\n\n# Normalize images\nX_test = X_test.astype(\"float32\") / 255.0\n\n# Reshape for PyTorch (N,C,H,W)\nX_test = np.transpose(X_test, (0,3,1,2))\ny_test = y_test.astype(\"int64\")\n\nprint(\"Test data prepared:\", X_test.shape)\n\n# ===== MAKE PREDICTIONS =====\nprint(\"\\n=== Evaluating Train_A Model on Test Set ===\")\n\ntest_ds = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\ntest_dl = DataLoader(test_ds, batch_size=64, shuffle=False)\n\nfinal_model_clean.eval()  # ← CHANGED: Use train_A model\ny_pred_test = []\n\nwith torch.no_grad():\n    for xb, _ in test_dl:\n        xb = xb.to(device)\n        probs = torch.softmax(final_model_clean(xb), 1)  # ← CHANGED\n        y_pred_test.append(torch.argmax(probs, 1).cpu().numpy())\n\ny_pred_test = np.concatenate(y_pred_test)\n\n# ===== CALCULATE METRICS =====\ncm_test = confusion_matrix_manual(y_test, y_pred_test, labels=np.unique(y_test))\nacc_test, prec_test, rec_test, f1_test = calc_metrics(cm_test)\n\nprint(f\"\\n=== Test Set Results (Train_A Model - Clean Training) ===\")\nprint(f\"Accuracy : {acc_test:.3f}\")\nprint(f\"Precision: {prec_test:.3f}\")\nprint(f\"Recall   : {rec_test:.3f}\")\nprint(f\"F1-score : {f1_test:.3f}\")\n\nprint(f\"\\nConfusion Matrix:\")\nprint(cm_test)\nprint(f\"Classes: {encoder.classes_}\")\n\n# ===== COMPARISON WITH CV =====\n\n# Load the saved CV results\nwith open('/kaggle/working/cv_results_trainA.pkl', 'rb') as f:\n    cv_results_A = pickle.load(f)\n\n# Extract the variables you need\ndeep_mean_A = cv_results_A['mean']\ndeep_std_A = cv_results_A['std']\n\nprint(f\"\\n=== Performance Comparison (Train_A) ===\")\nprint(f\"CV Performance:   {deep_mean_A[0]:.3f} ± {deep_std_A[0]:.3f}\")  # ← Use train_A CV results\nprint(f\"Test Performance: {acc_test:.3f}\")\nprint(f\"Difference:       {acc_test - deep_mean_A[0]:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T00:02:07.134885Z","iopub.execute_input":"2025-10-11T00:02:07.135531Z","iopub.status.idle":"2025-10-11T00:02:16.681311Z","shell.execute_reply.started":"2025-10-11T00:02:07.135506Z","shell.execute_reply":"2025-10-11T00:02:16.680312Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading Test Data (for Train_A Model) ===\nLoaded test data: (3000, 224, 224, 3) (3000,)\nTest data prepared: (3000, 3, 224, 224)\n\n=== Evaluating Train_A Model on Test Set ===\n\n=== Test Set Results (Train_A Model - Clean Training) ===\nAccuracy : 0.812\nPrecision: 0.816\nRecall   : 0.812\nF1-score : 0.814\n\nConfusion Matrix:\n[[844  92  64]\n [ 70 830 100]\n [ 52 185 763]]\nClasses: ['Boot' 'Sandal' 'Shoe']\n\n=== Performance Comparison (Train_A) ===\nCV Performance:   0.890 ± 0.016\nTest Performance: 0.812\nDifference:       -0.078\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"Bootstrapping, to get error bars for the test accuracy","metadata":{}},{"cell_type":"code","source":"def bootstrap_accuracy(y_true, y_pred, n_bootstrap=1000, confidence=0.95):\n    \"\"\"\n    Calculate bootstrap confidence intervals for accuracy\n    \n    Parameters:\n    -----------\n    y_true : array-like\n        True labels\n    y_pred : array-like\n        Predicted labels\n    n_bootstrap : int\n        Number of bootstrap samples\n    confidence : float\n        Confidence level (default 0.95 for 95% CI)\n    \n    Returns:\n    --------\n    mean_acc : float\n        Mean accuracy across bootstrap samples\n    lower : float\n        Lower bound of confidence interval\n    upper : float\n        Upper bound of confidence interval\n    \"\"\"\n    n_samples = len(y_true)\n    accuracies = []\n    \n    for _ in range(n_bootstrap):\n        # Resample with replacement\n        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        y_true_boot = y_true[indices]\n        y_pred_boot = y_pred[indices]\n        \n        # Calculate accuracy for this bootstrap sample\n        acc = np.mean(y_true_boot == y_pred_boot)\n        accuracies.append(acc)\n    \n    # Calculate confidence interval\n    alpha = (1 - confidence) / 2\n    lower = np.percentile(accuracies, alpha * 100)\n    upper = np.percentile(accuracies, (1 - alpha) * 100)\n    mean_acc = np.mean(accuracies)\n    \n    return mean_acc, lower, upper","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T00:17:41.601724Z","iopub.execute_input":"2025-10-11T00:17:41.601998Z","iopub.status.idle":"2025-10-11T00:17:41.607481Z","shell.execute_reply.started":"2025-10-11T00:17:41.601978Z","shell.execute_reply":"2025-10-11T00:17:41.606686Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Kaggle working directory\nmodel_path = Path('/kaggle/working')\n\n# List all files\nprint(\"Available files:\")\nfor file in model_path.glob('*'):\n    print(f\"  - {file.name}\")\n\n# Load encoder\nwith open(model_path / 'encoder_trainB.pkl', 'rb') as f:\n    encoder_trainB = pickle.load(f)\n\nprint(f\"\\nClasses (trainB): {encoder_trainB.classes_}\")\n\n# If you need encoder_trainA too:\nwith open(model_path / 'encoder_trainA.pkl', 'rb') as f:\n    encoder_trainA = pickle.load(f)\n\nprint(f\"Classes (trainA): {encoder_trainA.classes_}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T00:16:06.665906Z","iopub.execute_input":"2025-10-11T00:16:06.666201Z","iopub.status.idle":"2025-10-11T00:16:06.672566Z","shell.execute_reply.started":"2025-10-11T00:16:06.666180Z","shell.execute_reply":"2025-10-11T00:16:06.671997Z"}},"outputs":[{"name":"stdout","text":"Available files:\n  - deepcnn_final_clean.pth\n  - encoder_trainB.pkl\n  - deepcnn_final_B.pth\n  - .virtual_documents\n  - cv_results_trainA.pkl\n  - encoder_trainA.pkl\n\nClasses (trainB): ['Boot' 'Sandal' 'Shoe']\nClasses (trainA): ['Boot' 'Sandal' 'Shoe']\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# ===== BOOTSTRAP FOR TRAIN_A MODEL (Clean Images) =====\nprint(\"\\n=== Bootstrap Analysis for Train_A Model ===\")\n\n# You already have y_test and y_pred_test from your last cell\nmean_acc_A, lower_A, upper_A = bootstrap_accuracy(y_test, y_pred_test, n_bootstrap=1000)\n\nprint(f\"Test Accuracy: {mean_acc_A:.4f}\")\nprint(f\"95% CI: ({lower_A:.4f}, {upper_A:.4f})\")\nprint(f\"Margin of Error: ±{(upper_A - lower_A) / 2:.4f}\")\n\n\n# ===== NOW FOR TRAIN_B MODEL (Dirty Images) =====\nprint(\"\\n=== Bootstrap Analysis for Train_B Model ===\")\n\n# Load test data and encoder for Train_B first\ntest_data = np.load(os.path.join(data_dir, \"test.npz\"))\nX_test_B, y_test_B = test_data[\"X\"], test_data[\"y\"]\n\nwith open('/kaggle/working/encoder_trainB.pkl', 'rb') as f:\n    encoder_B = pickle.load(f)\n\ny_test_B = encoder_B.transform(y_test_B)\nX_test_B = X_test_B.astype(\"float32\") / 255.0\nX_test_B = np.transpose(X_test_B, (0,3,1,2))\ny_test_B = y_test_B.astype(\"int64\")\n\n# Load the Train_B model\nfinal_model_B = DeepCNN(num_classes).to(device)\n\n# CRITICAL FIX: Do a dummy forward pass to initialize the classifier\nwith torch.no_grad():\n    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n    _ = final_model_B(dummy_input)\n\n# NOW load the weights\nfinal_model_B.load_state_dict(torch.load('/kaggle/working/deepcnn_final_B.pth'))\n\n# Get predictions\ntest_ds_B = TensorDataset(torch.tensor(X_test_B), torch.tensor(y_test_B))\ntest_dl_B = DataLoader(test_ds_B, batch_size=64, shuffle=False)\n\nfinal_model_B.eval()\ny_pred_test_B = []\n\nwith torch.no_grad():\n    for xb, _ in test_dl_B:\n        xb = xb.to(device)\n        probs = torch.softmax(final_model_B(xb), 1)\n        y_pred_test_B.append(torch.argmax(probs, 1).cpu().numpy())\n\ny_pred_test_B = np.concatenate(y_pred_test_B)\n\n# Bootstrap for Train_B\nmean_acc_B, lower_B, upper_B = bootstrap_accuracy(y_test_B, y_pred_test_B, n_bootstrap=1000)\n\nprint(f\"Test Accuracy: {mean_acc_B:.4f}\")\nprint(f\"95% CI: ({lower_B:.4f}, {upper_B:.4f})\")\nprint(f\"Margin of Error: ±{(upper_B - lower_B) / 2:.4f}\")\n\n\n# ===== COMPARISON =====\nprint(\"\\n=== Final Comparison ===\")\nprint(f\"Train on Clean (A): {mean_acc_A:.4f} ({lower_A:.4f}, {upper_A:.4f})\")\nprint(f\"Train on Dirty (B): {mean_acc_B:.4f} ({lower_B:.4f}, {upper_B:.4f})\")\nprint(f\"\\nDifference: {mean_acc_B - mean_acc_A:.4f}\")\n\n# Check if confidence intervals overlap\nif lower_B > upper_A:\n    print(\"✓ CIs do NOT overlap - difference is statistically significant\")\nelif lower_A > upper_B:\n    print(\"✓ CIs do NOT overlap - difference is statistically significant\")  \nelse:\n    print(\"⚠ CIs overlap - difference may not be statistically significant\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T00:19:26.516943Z","iopub.execute_input":"2025-10-11T00:19:26.517631Z","iopub.status.idle":"2025-10-11T00:19:34.424929Z","shell.execute_reply.started":"2025-10-11T00:19:26.517607Z","shell.execute_reply":"2025-10-11T00:19:34.424187Z"}},"outputs":[{"name":"stdout","text":"\n=== Bootstrap Analysis for Train_A Model ===\nTest Accuracy: 0.8123\n95% CI: (0.7987, 0.8267)\nMargin of Error: ±0.0140\n\n=== Bootstrap Analysis for Train_B Model ===\nTest Accuracy: 0.8475\n95% CI: (0.8337, 0.8607)\nMargin of Error: ±0.0135\n\n=== Final Comparison ===\nTrain on Clean (A): 0.8123 (0.7987, 0.8267)\nTrain on Dirty (B): 0.8475 (0.8337, 0.8607)\n\nDifference: 0.0351\n✓ CIs do NOT overlap - difference is statistically significant\n","output_type":"stream"}],"execution_count":27}]}