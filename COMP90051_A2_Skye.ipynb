{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e304e8",
   "metadata": {},
   "source": [
    "# COMP90051 Assignment 2 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de005536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc9b550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fungs4/Desktop/Skye/University of Melbourne/Year 1 2025/COMP90051 Statistical Machine Learning/COMP90051 Assignments/COMP90051 Assignment 2/Shoe vs Sandal vs Boot Dataset\n"
     ]
    }
   ],
   "source": [
    "#setting working directory\n",
    "import os\n",
    "os.chdir('/Users/fungs4/Desktop/Skye/University of Melbourne/Year 1 2025/COMP90051 Statistical Machine Learning/COMP90051 Assignments/COMP90051 Assignment 2/Shoe vs Sandal vs Boot Dataset')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "375cbf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boot: 5000 images\n",
      "Sandal: 5000 images\n",
      "Shoe: 5000 images\n",
      "Total: 15000 images\n"
     ]
    }
   ],
   "source": [
    "#loading image paths\n",
    "data_dir = Path('.')\n",
    "\n",
    "boot_images = list((data_dir / 'Boot').glob('*.jpg')) + list((data_dir / 'Boot').glob('*.png'))\n",
    "sandal_images = list((data_dir / 'Sandal').glob('*.jpg')) + list((data_dir / 'Sandal').glob('*.png'))\n",
    "shoe_images = list((data_dir / 'Shoe').glob('*.jpg')) + list((data_dir / 'Shoe').glob('*.png'))\n",
    "\n",
    "print(f\"Boot: {len(boot_images)} images\")\n",
    "print(f\"Sandal: {len(sandal_images)} images\")\n",
    "print(f\"Shoe: {len(shoe_images)} images\")\n",
    "print(f\"Total: {len(boot_images) + len(sandal_images) + len(shoe_images)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffebda59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining image paths, creating labels\n",
    "image_paths = boot_images + sandal_images + shoe_images\n",
    "all_labels = ['Boot'] * len(boot_images) + ['Sandal'] * len(sandal_images) + ['Shoe'] * len(shoe_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fbe685",
   "metadata": {},
   "source": [
    "Creating train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3c6f330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 12000 images\n",
      "Test set: 3000 images\n",
      "\n",
      "Training set distribution: Counter({'Boot': 4000, 'Sandal': 4000, 'Shoe': 4000})\n",
      "Test set distribution: Counter({'Shoe': 1000, 'Sandal': 1000, 'Boot': 1000})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "    image_paths, \n",
    "    all_labels, \n",
    "    test_size = 0.2, \n",
    "    stratify = all_labels,\n",
    "    random_state = 66\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_paths)} images\")\n",
    "print(f\"Test set: {len(test_paths)} images\")\n",
    "\n",
    "#checking class distribution (making sure train/test sets have the same proportion of each class)\n",
    "from collections import Counter\n",
    "print(\"\\nTraining set distribution:\", Counter(train_labels))\n",
    "print(\"Test set distribution:\", Counter(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34103422",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de8a67",
   "metadata": {},
   "source": [
    "**\"When trained on augmented data, which machine learning architectures are most robust to progressively severe real-world distortions in footwear classification?\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00571fa",
   "metadata": {},
   "source": [
    "We want to look at which architectures handle distortions/noise best, when given augmented training data. The approach is as follows\n",
    "1. Apply moderate augmentation/distortions to the training set\n",
    "2. Create 5 severity levels for each distortion type, for the test set\n",
    "    * For example, our first test set will be clean. Second test set will have mild distortions,...,last test set will have extreme distortions\n",
    "3. Train 3 models on augmented data\n",
    "    * This phase will include CV/Hyperparameter tuning\n",
    "4. Testing\n",
    "    * Test each model on each of the 5 test sets, with the goal of exploring how model performance deviates, as the images that we feed in have increasing levels of distortion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f681ccb",
   "metadata": {},
   "source": [
    "An alternative approach, adds in the step of training another set of 3 models, on clean data only. This allows us to answer/investigate the following:\n",
    "* Real world images are noisy. We want our models to be robust, even when trained on/tested on images that have natural distortions/occlusions. The question we want to answer is \"is augmentation necessary for robustness\". In other words, is it possible to train good robust models for image classification, when they are trained only on \"clean\" images? We would essentially be investigating this by training models on clean images, and seeing how they perform when they are given noisy/distorted images to classify. If these models still perform well, than we can say that certain model architectures are able to learn relevant features from clean images, and are not distracted by the added noise from the test images, when actually classifying unseen images.\n",
    "Based on this, an alternative research question would be:\n",
    "\n",
    "**\"How does architectural choice affect robustness to real-world distortions: Is robustness inherent to the architecture or learned from augmented training?\"**\n",
    "\n",
    "In summary: \n",
    "* Condition A - Train on clean data -> Tests inherent robustness, which architectures naturally generalise\n",
    "* Condition B - Train on augmented data -> Tests learned robustness, which architectures learn distortion-invariance explicitly, and are able to benefit most from being trained on augmented images, that are more similar (in that they have noise/occlusions/distortions) to real-world images that we wish to classify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc667640",
   "metadata": {},
   "source": [
    "## What distortions/occlusions/noise to add? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5034ad",
   "metadata": {},
   "source": [
    "A realistic approach to adding distortions is as follows: The images that are currently in our dataset are very clean. More specifically, they depict footwear on a white background, with almost no noise. However, this is not a very realistic scenario. There may be many cases where we would want to classify footwear, where the images contains a lot of noise. Examples would be natural images of people wearing shoes. In these cases since we are only interested in classifying the footwear type, the other parts of the image (e.g. the background, what the individual is wearing, etc.) are all effectively noise. Hence, it makes sense to add noise to the images. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
