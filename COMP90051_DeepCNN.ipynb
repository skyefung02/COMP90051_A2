{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-08T07:06:11.882011Z",
     "iopub.status.busy": "2025-10-08T07:06:11.881275Z",
     "iopub.status.idle": "2025-10-08T07:06:12.692755Z",
     "shell.execute_reply": "2025-10-08T07:06:12.692118Z",
     "shell.execute_reply.started": "2025-10-08T07:06:11.881985Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/shoe-images/train_B.npz\n",
      "/kaggle/input/shoe-images/test.npz\n",
      "/kaggle/input/shoe-images/train_A.npz\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T08:33:59.655514Z",
     "iopub.status.busy": "2025-10-08T08:33:59.655087Z",
     "iopub.status.idle": "2025-10-08T08:34:06.988418Z",
     "shell.execute_reply": "2025-10-08T08:34:06.987616Z",
     "shell.execute_reply.started": "2025-10-08T08:33:59.655483Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os, random, numpy as np\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T08:34:06.989983Z",
     "iopub.status.busy": "2025-10-08T08:34:06.989589Z",
     "iopub.status.idle": "2025-10-08T08:34:07.072581Z",
     "shell.execute_reply": "2025-10-08T08:34:07.071702Z",
     "shell.execute_reply.started": "2025-10-08T08:34:06.989954Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T01:09:47.760894Z",
     "iopub.status.busy": "2025-10-08T01:09:47.760354Z",
     "iopub.status.idle": "2025-10-08T01:10:08.199880Z",
     "shell.execute_reply": "2025-10-08T01:10:08.199200Z",
     "shell.execute_reply.started": "2025-10-08T01:09:47.760871Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (12000, 224, 224, 3) (12000,)\n",
      "Label mapping: {'Boot': 0, 'Sandal': 1, 'Shoe': 2}\n",
      "Final tensors: (12000, 3, 224, 224) Classes: 3\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/kaggle/input/shoe-images\"\n",
    "data = np.load(os.path.join(data_dir, \"train_B.npz\"))  # or train_A.npz\n",
    "X, y = data[\"X\"], data[\"y\"]\n",
    "print(\"Loaded:\", X.shape, y.shape)\n",
    "\n",
    "# ---------- encode string labels to ints ----------\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)            # e.g. Boot→0, Sandal→1, Shoe→2\n",
    "print(\"Label mapping:\", dict(zip(encoder.classes_,\n",
    "                                 range(len(encoder.classes_)))))\n",
    "\n",
    "# ---------- normalise images ----------\n",
    "X = X.astype(\"float32\") / 255.0\n",
    "\n",
    "# ---------- reshape for PyTorch (N,C,H,W) ----------\n",
    "X = np.transpose(X, (0,3,1,2))\n",
    "y = y.astype(\"int64\")\n",
    "num_classes = len(np.unique(y))\n",
    "print(\"Final tensors:\", X.shape, \"Classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T08:35:33.901098Z",
     "iopub.status.busy": "2025-10-08T08:35:33.900781Z",
     "iopub.status.idle": "2025-10-08T08:35:33.907385Z",
     "shell.execute_reply": "2025-10-08T08:35:33.906555Z",
     "shell.execute_reply.started": "2025-10-08T08:35:33.901075Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_folds(n, k=10, seed=42, y=None):\n",
    "    \"\"\"\n",
    "    Create stratified k-fold splits that preserve class distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    - n: Total number of samples\n",
    "    - k: Number of folds (default=10)\n",
    "    - seed: Random seed for reproducibility (default=42)\n",
    "    - y: Labels for stratification (required)\n",
    "    \n",
    "    Returns:\n",
    "    - List of k arrays containing indices for each fold\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    unique_classes = np.unique(y)\n",
    "    folds = [[] for _ in range(k)]\n",
    "    \n",
    "    # For each class, split its samples across k folds\n",
    "    for cls in unique_classes:\n",
    "        cls_indices = np.where(y == cls)[0]\n",
    "        np.random.shuffle(cls_indices)\n",
    "        cls_splits = np.array_split(cls_indices, k)\n",
    "        \n",
    "        # Add class samples to each fold\n",
    "        for fold_idx, split in enumerate(cls_splits):\n",
    "            folds[fold_idx].extend(split)\n",
    "    \n",
    "    # Shuffle within each fold and convert to numpy arrays\n",
    "    for i in range(k):\n",
    "        np.random.shuffle(folds[i])\n",
    "        folds[i] = np.array(folds[i])\n",
    "    \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T08:35:35.726290Z",
     "iopub.status.busy": "2025-10-08T08:35:35.725752Z",
     "iopub.status.idle": "2025-10-08T08:35:35.730570Z",
     "shell.execute_reply": "2025-10-08T08:35:35.729823Z",
     "shell.execute_reply.started": "2025-10-08T08:35:35.726267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix_manual(y_true, y_pred, labels):\n",
    "    n = len(labels)\n",
    "    label_to_idx = {lab: i for i, lab in enumerate(labels)}\n",
    "    cm = np.zeros((n, n), dtype=int)\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        i = label_to_idx[yt]\n",
    "        j = label_to_idx[yp]\n",
    "        cm[i, j] += 1\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T08:35:37.172173Z",
     "iopub.status.busy": "2025-10-08T08:35:37.171707Z",
     "iopub.status.idle": "2025-10-08T08:35:37.176624Z",
     "shell.execute_reply": "2025-10-08T08:35:37.175878Z",
     "shell.execute_reply.started": "2025-10-08T08:35:37.172151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calc_metrics(cm):\n",
    "    TP = np.diag(cm)\n",
    "    FP = cm.sum(0) - TP\n",
    "    FN = cm.sum(1) - TP\n",
    "    precision = np.mean(TP / (TP + FP + 1e-9))\n",
    "    recall    = np.mean(TP / (TP + FN + 1e-9))\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "    acc = TP.sum() / cm.sum()\n",
    "    return acc, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T08:35:47.335861Z",
     "iopub.status.busy": "2025-10-08T08:35:47.335620Z",
     "iopub.status.idle": "2025-10-08T08:35:47.343289Z",
     "shell.execute_reply": "2025-10-08T08:35:47.342518Z",
     "shell.execute_reply.started": "2025-10-08T08:35:47.335845Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_fold(X_train, y_train, X_val, y_val, model_builder,\n",
    "                   lr=1e-3, epochs=5, batch=64, device=\"cpu\"):\n",
    "    \"\"\"Train one fold and return model + predictions on validation set.\"\"\"\n",
    "    \n",
    "    train_ds = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "    val_ds   = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch, shuffle=True)\n",
    "    val_dl   = DataLoader(val_ds, batch_size=batch, shuffle=False)\n",
    "\n",
    "    # note: difference here — build model dynamically\n",
    "    model = model_builder().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            loss = loss_fn(out, yb)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_dl)\n",
    "        print(f\"  Epoch {ep+1}/{epochs}, Train loss={avg_loss:.4f}\")\n",
    "\n",
    "    # ----- validation predictions -----\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in val_dl:\n",
    "            xb = xb.to(device)\n",
    "            probs = torch.softmax(model(xb), 1)\n",
    "            preds.append(torch.argmax(probs, 1).cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return model, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T08:35:51.703243Z",
     "iopub.status.busy": "2025-10-08T08:35:51.702962Z",
     "iopub.status.idle": "2025-10-08T08:35:51.712602Z",
     "shell.execute_reply": "2025-10-08T08:35:51.711764Z",
     "shell.execute_reply.started": "2025-10-08T08:35:51.703221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model_nested_cv(\n",
    "    X, y, model_builder,\n",
    "    candidate_lr=[1e-3, 3e-4, 1e-4],\n",
    "    k_outer=10, k_inner=3, epochs=5,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"Generic nested cross‑validation for any model.\"\"\"\n",
    "\n",
    "    folds = make_folds(len(X), k_outer, seed=42, y=y)\n",
    "    metrics_all = []\n",
    "\n",
    "    for i in range(k_outer):\n",
    "        print(f\"\\n=== Outer Fold {i+1}/{k_outer} ===\")\n",
    "\n",
    "        test_idx = folds[i]\n",
    "        train_idx = np.concatenate([folds[j] for j in range(k_outer) if j != i])\n",
    "        X_train, y_train = X[train_idx], y[train_idx]\n",
    "        X_test,  y_test  = X[test_idx],  y[test_idx]\n",
    "\n",
    "        # ---- inner loop: tuning learning rate ----\n",
    "        inner_folds = make_folds(len(X_train), k_inner, seed=42, y=y_train)\n",
    "        mean_accs = []\n",
    "\n",
    "        for lr in candidate_lr:\n",
    "            inner_scores = []\n",
    "            for j in range(k_inner):\n",
    "                val_idx = inner_folds[j]\n",
    "                tr_idx  = np.concatenate([inner_folds[m] for m in range(k_inner) if m != j])\n",
    "\n",
    "                _, y_pred_val = train_one_fold(\n",
    "                    X_train[tr_idx], y_train[tr_idx],\n",
    "                    X_train[val_idx], y_train[val_idx],\n",
    "                    model_builder=model_builder,\n",
    "                    lr=lr, epochs=2, device=device\n",
    "                )\n",
    "\n",
    "                cm = confusion_matrix_manual(y_train[val_idx], y_pred_val, labels=np.unique(y))\n",
    "                acc, prec, rec, f1 = calc_metrics(cm)\n",
    "                inner_scores.append(acc)\n",
    "\n",
    "            mean_accs.append(np.mean(inner_scores))\n",
    "\n",
    "        best_lr = candidate_lr[int(np.argmax(mean_accs))]\n",
    "        print(f\"Best LR = {best_lr:.0e}\")\n",
    "\n",
    "        # ---- outer test fold ----\n",
    "        model, y_pred = train_one_fold(\n",
    "            X_train, y_train, X_test, y_test,\n",
    "            model_builder=model_builder,\n",
    "            lr=best_lr, epochs=epochs, device=device\n",
    "        )\n",
    "\n",
    "        cm = confusion_matrix_manual(y_test, y_pred, labels=np.unique(y))\n",
    "        acc, prec, rec, f1 = calc_metrics(cm)\n",
    "        metrics_all.append([acc, prec, rec, f1])\n",
    "\n",
    "        print(f\"Fold {i+1}: Acc={acc:.3f}, P={prec:.3f}, R={rec:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "    # ---- summary ----\n",
    "    metrics_all = np.array(metrics_all)\n",
    "    mean, std = metrics_all.mean(0), metrics_all.std(0)\n",
    "\n",
    "    print(\"\\n=== Nested CV Results ===\")\n",
    "    print(f\"Accuracy : {mean[0]:.3f} ± {std[0]:.3f}\")\n",
    "    print(f\"Precision: {mean[1]:.3f} ± {std[1]:.3f}\")\n",
    "    print(f\"Recall   : {mean[2]:.3f} ± {std[2]:.3f}\")\n",
    "    print(f\"F1-score : {mean[3]:.3f} ± {std[3]:.3f}\")\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T08:35:57.658158Z",
     "iopub.status.busy": "2025-10-08T08:35:57.657868Z",
     "iopub.status.idle": "2025-10-08T08:35:57.664750Z",
     "shell.execute_reply": "2025-10-08T08:35:57.664085Z",
     "shell.execute_reply.started": "2025-10-08T08:35:57.658137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DeepCNN(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        # Feature extractor\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),  #32 filters, padding=1 to maintain size of filters\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                 # Downsample\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)) #GAP instead of MaxPool2d, to increase inherent robustness\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, n_classes)        # ← CHANGED: Direct 128 → n_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T01:10:08.293154Z",
     "iopub.status.busy": "2025-10-08T01:10:08.292924Z",
     "iopub.status.idle": "2025-10-08T02:23:47.727635Z",
     "shell.execute_reply": "2025-10-08T02:23:47.726762Z",
     "shell.execute_reply.started": "2025-10-08T01:10:08.293139Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Evaluating DeepCNN ###\n",
      "\n",
      "=== Outer Fold 1/10 ===\n",
      "  Epoch 1/2, Train loss=0.8530\n",
      "  Epoch 2/2, Train loss=0.6977\n",
      "  Epoch 1/2, Train loss=0.8345\n",
      "  Epoch 2/2, Train loss=0.6829\n",
      "  Epoch 1/2, Train loss=0.8199\n",
      "  Epoch 2/2, Train loss=0.6747\n",
      "  Epoch 1/2, Train loss=0.8483\n",
      "  Epoch 2/2, Train loss=0.6708\n",
      "  Epoch 1/2, Train loss=0.8495\n",
      "  Epoch 2/2, Train loss=0.6716\n",
      "  Epoch 1/2, Train loss=0.8362\n",
      "  Epoch 2/2, Train loss=0.6569\n",
      "  Epoch 1/2, Train loss=0.9176\n",
      "  Epoch 2/2, Train loss=0.7366\n",
      "  Epoch 1/2, Train loss=0.9269\n",
      "  Epoch 2/2, Train loss=0.7455\n",
      "  Epoch 1/2, Train loss=0.9199\n",
      "  Epoch 2/2, Train loss=0.7238\n",
      "Best LR = 3e-04\n",
      "  Epoch 1/3, Train loss=0.8641\n",
      "  Epoch 2/3, Train loss=0.6765\n",
      "  Epoch 3/3, Train loss=0.5997\n",
      "Fold 1: Acc=0.787, P=0.791, R=0.787, F1=0.789\n",
      "\n",
      "=== Outer Fold 2/10 ===\n",
      "  Epoch 1/2, Train loss=0.8492\n",
      "  Epoch 2/2, Train loss=0.7184\n",
      "  Epoch 1/2, Train loss=0.8304\n",
      "  Epoch 2/2, Train loss=0.6707\n",
      "  Epoch 1/2, Train loss=0.8230\n",
      "  Epoch 2/2, Train loss=0.6967\n",
      "  Epoch 1/2, Train loss=0.8165\n",
      "  Epoch 2/2, Train loss=0.6529\n",
      "  Epoch 1/2, Train loss=0.8184\n",
      "  Epoch 2/2, Train loss=0.6290\n",
      "  Epoch 1/2, Train loss=0.8256\n",
      "  Epoch 2/2, Train loss=0.6594\n",
      "  Epoch 1/2, Train loss=0.9489\n",
      "  Epoch 2/2, Train loss=0.7691\n",
      "  Epoch 1/2, Train loss=0.9097\n",
      "  Epoch 2/2, Train loss=0.7042\n",
      "  Epoch 1/2, Train loss=0.9059\n",
      "  Epoch 2/2, Train loss=0.7114\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.7923\n",
      "  Epoch 2/3, Train loss=0.6231\n",
      "  Epoch 3/3, Train loss=0.5553\n",
      "Fold 2: Acc=0.814, P=0.822, R=0.814, F1=0.818\n",
      "\n",
      "=== Outer Fold 3/10 ===\n",
      "  Epoch 1/2, Train loss=0.8512\n",
      "  Epoch 2/2, Train loss=0.7248\n",
      "  Epoch 1/2, Train loss=0.8571\n",
      "  Epoch 2/2, Train loss=0.7061\n",
      "  Epoch 1/2, Train loss=0.8114\n",
      "  Epoch 2/2, Train loss=0.6635\n",
      "  Epoch 1/2, Train loss=0.8087\n",
      "  Epoch 2/2, Train loss=0.6426\n",
      "  Epoch 1/2, Train loss=0.8785\n",
      "  Epoch 2/2, Train loss=0.7053\n",
      "  Epoch 1/2, Train loss=0.8484\n",
      "  Epoch 2/2, Train loss=0.6639\n",
      "  Epoch 1/2, Train loss=0.9098\n",
      "  Epoch 2/2, Train loss=0.7443\n",
      "  Epoch 1/2, Train loss=0.8913\n",
      "  Epoch 2/2, Train loss=0.7217\n",
      "  Epoch 1/2, Train loss=0.9280\n",
      "  Epoch 2/2, Train loss=0.7364\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.7904\n",
      "  Epoch 2/3, Train loss=0.6109\n",
      "  Epoch 3/3, Train loss=0.5385\n",
      "Fold 3: Acc=0.810, P=0.810, R=0.810, F1=0.810\n",
      "\n",
      "=== Outer Fold 4/10 ===\n",
      "  Epoch 1/2, Train loss=0.8879\n",
      "  Epoch 2/2, Train loss=0.7050\n",
      "  Epoch 1/2, Train loss=0.8330\n",
      "  Epoch 2/2, Train loss=0.6843\n",
      "  Epoch 1/2, Train loss=0.8132\n",
      "  Epoch 2/2, Train loss=0.6582\n",
      "  Epoch 1/2, Train loss=0.8381\n",
      "  Epoch 2/2, Train loss=0.6601\n",
      "  Epoch 1/2, Train loss=0.8590\n",
      "  Epoch 2/2, Train loss=0.6805\n",
      "  Epoch 1/2, Train loss=0.8153\n",
      "  Epoch 2/2, Train loss=0.6486\n",
      "  Epoch 1/2, Train loss=0.8942\n",
      "  Epoch 2/2, Train loss=0.7146\n",
      "  Epoch 1/2, Train loss=0.9089\n",
      "  Epoch 2/2, Train loss=0.7430\n",
      "  Epoch 1/2, Train loss=0.9115\n",
      "  Epoch 2/2, Train loss=0.7178\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.7880\n",
      "  Epoch 2/3, Train loss=0.6187\n",
      "  Epoch 3/3, Train loss=0.5520\n",
      "Fold 4: Acc=0.807, P=0.810, R=0.807, F1=0.809\n",
      "\n",
      "=== Outer Fold 5/10 ===\n",
      "  Epoch 1/2, Train loss=0.8638\n",
      "  Epoch 2/2, Train loss=0.7108\n",
      "  Epoch 1/2, Train loss=0.8623\n",
      "  Epoch 2/2, Train loss=0.7248\n",
      "  Epoch 1/2, Train loss=0.8141\n",
      "  Epoch 2/2, Train loss=0.6774\n",
      "  Epoch 1/2, Train loss=0.8329\n",
      "  Epoch 2/2, Train loss=0.6610\n",
      "  Epoch 1/2, Train loss=0.8349\n",
      "  Epoch 2/2, Train loss=0.6487\n",
      "  Epoch 1/2, Train loss=0.8163\n",
      "  Epoch 2/2, Train loss=0.6497\n",
      "  Epoch 1/2, Train loss=0.9311\n",
      "  Epoch 2/2, Train loss=0.7478\n",
      "  Epoch 1/2, Train loss=0.9083\n",
      "  Epoch 2/2, Train loss=0.7217\n",
      "  Epoch 1/2, Train loss=0.9063\n",
      "  Epoch 2/2, Train loss=0.7229\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.7849\n",
      "  Epoch 2/3, Train loss=0.6189\n",
      "  Epoch 3/3, Train loss=0.5506\n",
      "Fold 5: Acc=0.821, P=0.833, R=0.821, F1=0.827\n",
      "\n",
      "=== Outer Fold 6/10 ===\n",
      "  Epoch 1/2, Train loss=0.8827\n",
      "  Epoch 2/2, Train loss=0.7388\n",
      "  Epoch 1/2, Train loss=0.8320\n",
      "  Epoch 2/2, Train loss=0.6956\n",
      "  Epoch 1/2, Train loss=0.8227\n",
      "  Epoch 2/2, Train loss=0.6983\n",
      "  Epoch 1/2, Train loss=0.8216\n",
      "  Epoch 2/2, Train loss=0.6486\n",
      "  Epoch 1/2, Train loss=0.8548\n",
      "  Epoch 2/2, Train loss=0.6811\n",
      "  Epoch 1/2, Train loss=0.8531\n",
      "  Epoch 2/2, Train loss=0.6867\n",
      "  Epoch 1/2, Train loss=0.9211\n",
      "  Epoch 2/2, Train loss=0.7332\n",
      "  Epoch 1/2, Train loss=0.9492\n",
      "  Epoch 2/2, Train loss=0.7415\n",
      "  Epoch 1/2, Train loss=0.9422\n",
      "  Epoch 2/2, Train loss=0.7614\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.7807\n",
      "  Epoch 2/3, Train loss=0.6111\n",
      "  Epoch 3/3, Train loss=0.5476\n",
      "Fold 6: Acc=0.833, P=0.838, R=0.833, F1=0.836\n",
      "\n",
      "=== Outer Fold 7/10 ===\n",
      "  Epoch 1/2, Train loss=0.8531\n",
      "  Epoch 2/2, Train loss=0.7113\n",
      "  Epoch 1/2, Train loss=0.8602\n",
      "  Epoch 2/2, Train loss=0.6953\n",
      "  Epoch 1/2, Train loss=0.8374\n",
      "  Epoch 2/2, Train loss=0.7119\n",
      "  Epoch 1/2, Train loss=0.8524\n",
      "  Epoch 2/2, Train loss=0.6854\n",
      "  Epoch 1/2, Train loss=0.8502\n",
      "  Epoch 2/2, Train loss=0.6761\n",
      "  Epoch 1/2, Train loss=0.8540\n",
      "  Epoch 2/2, Train loss=0.6584\n",
      "  Epoch 1/2, Train loss=0.9335\n",
      "  Epoch 2/2, Train loss=0.7545\n",
      "  Epoch 1/2, Train loss=0.9150\n",
      "  Epoch 2/2, Train loss=0.7222\n",
      "  Epoch 1/2, Train loss=0.9021\n",
      "  Epoch 2/2, Train loss=0.7195\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.7857\n",
      "  Epoch 2/3, Train loss=0.6136\n",
      "  Epoch 3/3, Train loss=0.5588\n",
      "Fold 7: Acc=0.838, P=0.838, R=0.838, F1=0.838\n",
      "\n",
      "=== Outer Fold 8/10 ===\n",
      "  Epoch 1/2, Train loss=0.8670\n",
      "  Epoch 2/2, Train loss=0.7096\n",
      "  Epoch 1/2, Train loss=0.8522\n",
      "  Epoch 2/2, Train loss=0.7276\n",
      "  Epoch 1/2, Train loss=0.8184\n",
      "  Epoch 2/2, Train loss=0.6813\n",
      "  Epoch 1/2, Train loss=0.8378\n",
      "  Epoch 2/2, Train loss=0.6543\n",
      "  Epoch 1/2, Train loss=0.8377\n",
      "  Epoch 2/2, Train loss=0.6695\n",
      "  Epoch 1/2, Train loss=0.8660\n",
      "  Epoch 2/2, Train loss=0.6910\n",
      "  Epoch 1/2, Train loss=0.9198\n",
      "  Epoch 2/2, Train loss=0.7419\n",
      "  Epoch 1/2, Train loss=0.9030\n",
      "  Epoch 2/2, Train loss=0.7155\n",
      "  Epoch 1/2, Train loss=0.9080\n",
      "  Epoch 2/2, Train loss=0.7354\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.7886\n",
      "  Epoch 2/3, Train loss=0.6189\n",
      "  Epoch 3/3, Train loss=0.5468\n",
      "Fold 8: Acc=0.818, P=0.821, R=0.817, F1=0.819\n",
      "\n",
      "=== Outer Fold 9/10 ===\n",
      "  Epoch 1/2, Train loss=0.8624\n",
      "  Epoch 2/2, Train loss=0.7035\n",
      "  Epoch 1/2, Train loss=0.8444\n",
      "  Epoch 2/2, Train loss=0.6868\n",
      "  Epoch 1/2, Train loss=0.8515\n",
      "  Epoch 2/2, Train loss=0.6875\n",
      "  Epoch 1/2, Train loss=0.8535\n",
      "  Epoch 2/2, Train loss=0.6630\n",
      "  Epoch 1/2, Train loss=0.8419\n",
      "  Epoch 2/2, Train loss=0.6740\n",
      "  Epoch 1/2, Train loss=0.8159\n",
      "  Epoch 2/2, Train loss=0.6409\n",
      "  Epoch 1/2, Train loss=0.9078\n",
      "  Epoch 2/2, Train loss=0.7318\n",
      "  Epoch 1/2, Train loss=0.9236\n",
      "  Epoch 2/2, Train loss=0.7369\n",
      "  Epoch 1/2, Train loss=0.9272\n",
      "  Epoch 2/2, Train loss=0.7383\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.7840\n",
      "  Epoch 2/3, Train loss=0.6046\n",
      "  Epoch 3/3, Train loss=0.5469\n",
      "Fold 9: Acc=0.750, P=0.783, R=0.750, F1=0.766\n",
      "\n",
      "=== Outer Fold 10/10 ===\n",
      "  Epoch 1/2, Train loss=0.8408\n",
      "  Epoch 2/2, Train loss=0.7243\n",
      "  Epoch 1/2, Train loss=0.8555\n",
      "  Epoch 2/2, Train loss=0.7010\n",
      "  Epoch 1/2, Train loss=0.8251\n",
      "  Epoch 2/2, Train loss=0.7082\n",
      "  Epoch 1/2, Train loss=0.8487\n",
      "  Epoch 2/2, Train loss=0.6616\n",
      "  Epoch 1/2, Train loss=0.8270\n",
      "  Epoch 2/2, Train loss=0.6463\n",
      "  Epoch 1/2, Train loss=0.8265\n",
      "  Epoch 2/2, Train loss=0.6437\n",
      "  Epoch 1/2, Train loss=0.9216\n",
      "  Epoch 2/2, Train loss=0.7195\n",
      "  Epoch 1/2, Train loss=0.9166\n",
      "  Epoch 2/2, Train loss=0.7345\n",
      "  Epoch 1/2, Train loss=0.9295\n",
      "  Epoch 2/2, Train loss=0.7561\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.7774\n",
      "  Epoch 2/3, Train loss=0.5962\n",
      "  Epoch 3/3, Train loss=0.5333\n",
      "Fold 10: Acc=0.802, P=0.815, R=0.802, F1=0.808\n",
      "\n",
      "=== Nested CV Results ===\n",
      "Accuracy : 0.808 ± 0.024\n",
      "Precision: 0.816 ± 0.018\n",
      "Recall   : 0.808 ± 0.024\n",
      "F1-score : 0.812 ± 0.021\n"
     ]
    }
   ],
   "source": [
    "# --- DeepCNN ---\n",
    "print(\"\\n### Evaluating DeepCNN ###\")\n",
    "deep_mean_B, deep_std_B = evaluate_model_nested_cv(\n",
    "    X, y,\n",
    "    model_builder=lambda: DeepCNN(num_classes),\n",
    "    candidate_lr=[3e-3, 1e-3, 3e-4],\n",
    "    k_outer=10,\n",
    "    k_inner=3,\n",
    "    epochs=3,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cv_results_B = {\n",
    "    'mean': deep_mean_B,\n",
    "    'std': deep_std_B,\n",
    "    'accuracy': deep_mean_B[0],\n",
    "    'precision': deep_mean_B[1],\n",
    "    'recall': deep_mean_B[2],\n",
    "    'f1': deep_mean_B[3]\n",
    "}\n",
    "\n",
    "with open('/kaggle/working/cv_results_trainB.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_results_B, f)\n",
    "\n",
    "print(\"CV results saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T08:35:19.449035Z",
     "iopub.status.busy": "2025-10-08T08:35:19.448761Z",
     "iopub.status.idle": "2025-10-08T08:35:19.455100Z",
     "shell.execute_reply": "2025-10-08T08:35:19.454363Z",
     "shell.execute_reply.started": "2025-10-08T08:35:19.449014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_final_model(X, y, model_builder, lr=1e-3, epochs=5, batch=64, device=\"cpu\"):\n",
    "    \"\"\"Train final model on full dataset (no validation split).\"\"\"\n",
    "    \n",
    "    train_ds = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch, shuffle=True)\n",
    "    \n",
    "    model = model_builder().to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, yb in train_dl:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            out = model(xb)\n",
    "            loss = loss_fn(out, yb)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_dl)\n",
    "        print(f\"  Epoch {ep+1}/{epochs}, Train loss={avg_loss:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T02:32:14.450095Z",
     "iopub.status.busy": "2025-10-08T02:32:14.449468Z",
     "iopub.status.idle": "2025-10-08T02:34:27.782237Z",
     "shell.execute_reply": "2025-10-08T02:34:27.781371Z",
     "shell.execute_reply.started": "2025-10-08T02:32:14.450070Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Final Model on Full Dataset ===\n",
      "  Epoch 1/5, Train loss=0.7666\n",
      "  Epoch 2/5, Train loss=0.5881\n",
      "  Epoch 3/5, Train loss=0.5281\n",
      "  Epoch 4/5, Train loss=0.4872\n",
      "  Epoch 5/5, Train loss=0.4560\n",
      "Final model saved!\n"
     ]
    }
   ],
   "source": [
    "# After nested CV\n",
    "print(\"\\n=== Training Final Model on Full Dataset ===\")\n",
    "\n",
    "final_model = train_final_model(\n",
    "    X, y,\n",
    "    model_builder=lambda: DeepCNN(num_classes),\n",
    "    lr=1e-3,\n",
    "    epochs=5,\n",
    "    batch=64,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "torch.save(final_model.state_dict(), '/kaggle/working/deepcnn_final.pth')\n",
    "print(\"Final model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T02:40:32.054733Z",
     "iopub.status.busy": "2025-10-08T02:40:32.053827Z",
     "iopub.status.idle": "2025-10-08T02:40:42.315145Z",
     "shell.execute_reply": "2025-10-08T02:40:42.314311Z",
     "shell.execute_reply.started": "2025-10-08T02:40:32.054703Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Test Data ===\n",
      "Loaded test data: (3000, 224, 224, 3) (3000,)\n",
      "Test data prepared: (3000, 3, 224, 224)\n",
      "\n",
      "=== Evaluating Final Model on Test Set ===\n",
      "\n",
      "=== Test Set Results ===\n",
      "Accuracy : 0.856\n",
      "Precision: 0.860\n",
      "Recall   : 0.856\n",
      "F1-score : 0.858\n",
      "\n",
      "Confusion Matrix:\n",
      "[[874  67  59]\n",
      " [ 25 866 109]\n",
      " [ 22 150 828]]\n",
      "Classes: ['Boot' 'Sandal' 'Shoe']\n",
      "\n",
      "=== Performance Comparison ===\n",
      "CV Performance:   0.808 ± 0.024\n",
      "Test Performance: 0.856\n",
      "Difference:       0.048\n"
     ]
    }
   ],
   "source": [
    "# ===== LOAD AND PREPARE TEST DATA =====\n",
    "print(\"\\n=== Loading Test Data ===\")\n",
    "test_data = np.load(os.path.join(data_dir, \"test.npz\"))\n",
    "X_test, y_test = test_data[\"X\"], test_data[\"y\"]\n",
    "print(\"Loaded test data:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Encode labels (same encoder from training)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "# Normalize images\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Reshape for PyTorch (N,C,H,W)\n",
    "X_test = np.transpose(X_test, (0,3,1,2))\n",
    "y_test = y_test.astype(\"int64\")\n",
    "\n",
    "print(\"Test data prepared:\", X_test.shape)\n",
    "\n",
    "# ===== MAKE PREDICTIONS =====\n",
    "print(\"\\n=== Evaluating Final Model on Test Set ===\")\n",
    "\n",
    "test_ds = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "final_model.eval()\n",
    "y_pred_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, _ in test_dl:\n",
    "        xb = xb.to(device)\n",
    "        probs = torch.softmax(final_model(xb), 1)\n",
    "        y_pred_test.append(torch.argmax(probs, 1).cpu().numpy())\n",
    "\n",
    "y_pred_test = np.concatenate(y_pred_test)\n",
    "\n",
    "# ===== CALCULATE METRICS =====\n",
    "cm_test = confusion_matrix_manual(y_test, y_pred_test, labels=np.unique(y_test))\n",
    "acc_test, prec_test, rec_test, f1_test = calc_metrics(cm_test)\n",
    "\n",
    "print(f\"\\n=== Test Set Results ===\")\n",
    "print(f\"Accuracy : {acc_test:.3f}\")\n",
    "print(f\"Precision: {prec_test:.3f}\")\n",
    "print(f\"Recall   : {rec_test:.3f}\")\n",
    "print(f\"F1-score : {f1_test:.3f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm_test)\n",
    "print(f\"Classes: {encoder.classes_}\")\n",
    "\n",
    "# ===== COMPARISON WITH CV =====\n",
    "print(f\"\\n=== Performance Comparison ===\")\n",
    "print(f\"CV Performance:   {0.808:.3f} ± {0.024:.3f}\")\n",
    "print(f\"Test Performance: {acc_test:.3f}\")\n",
    "print(f\"Difference:       {acc_test - 0.808:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating the above, but this time using train_A (which is the image set with only clean images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T08:34:24.440159Z",
     "iopub.status.busy": "2025-10-08T08:34:24.439877Z",
     "iopub.status.idle": "2025-10-08T08:34:39.749610Z",
     "shell.execute_reply": "2025-10-08T08:34:39.748773Z",
     "shell.execute_reply.started": "2025-10-08T08:34:24.440136Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (12000, 224, 224, 3) (12000,)\n",
      "Label mapping: {'Boot': 0, 'Sandal': 1, 'Shoe': 2}\n",
      "Final tensors: (12000, 3, 224, 224) Classes: 3\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/kaggle/input/shoe-images\"\n",
    "data = np.load(os.path.join(data_dir, \"train_A.npz\"))  #changed to train_A.npz\n",
    "X, y = data[\"X\"], data[\"y\"]\n",
    "print(\"Loaded:\", X.shape, y.shape)\n",
    "\n",
    "# ---------- encode string labels to ints ----------\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(y)            # e.g. Boot→0, Sandal→1, Shoe→2\n",
    "print(\"Label mapping:\", dict(zip(encoder.classes_,\n",
    "                                 range(len(encoder.classes_)))))\n",
    "\n",
    "# ---------- normalise images ----------\n",
    "X = X.astype(\"float32\") / 255.0\n",
    "\n",
    "# ---------- reshape for PyTorch (N,C,H,W) ----------\n",
    "X = np.transpose(X, (0,3,1,2))\n",
    "y = y.astype(\"int64\")\n",
    "num_classes = len(np.unique(y))\n",
    "print(\"Final tensors:\", X.shape, \"Classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T07:07:16.334907Z",
     "iopub.status.busy": "2025-10-08T07:07:16.334280Z",
     "iopub.status.idle": "2025-10-08T08:20:48.201173Z",
     "shell.execute_reply": "2025-10-08T08:20:48.200379Z",
     "shell.execute_reply.started": "2025-10-08T07:07:16.334880Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Evaluating DeepCNN ###\n",
      "\n",
      "=== Outer Fold 1/10 ===\n",
      "  Epoch 1/2, Train loss=0.5666\n",
      "  Epoch 2/2, Train loss=0.4016\n",
      "  Epoch 1/2, Train loss=0.6100\n",
      "  Epoch 2/2, Train loss=0.4277\n",
      "  Epoch 1/2, Train loss=0.5683\n",
      "  Epoch 2/2, Train loss=0.3877\n",
      "  Epoch 1/2, Train loss=0.5922\n",
      "  Epoch 2/2, Train loss=0.3824\n",
      "  Epoch 1/2, Train loss=0.6348\n",
      "  Epoch 2/2, Train loss=0.4321\n",
      "  Epoch 1/2, Train loss=0.6062\n",
      "  Epoch 2/2, Train loss=0.4086\n",
      "  Epoch 1/2, Train loss=0.7221\n",
      "  Epoch 2/2, Train loss=0.4885\n",
      "  Epoch 1/2, Train loss=0.7342\n",
      "  Epoch 2/2, Train loss=0.4893\n",
      "  Epoch 1/2, Train loss=0.7527\n",
      "  Epoch 2/2, Train loss=0.5159\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.5930\n",
      "  Epoch 2/3, Train loss=0.3864\n",
      "  Epoch 3/3, Train loss=0.3236\n",
      "Fold 1: Acc=0.929, P=0.929, R=0.929, F1=0.929\n",
      "\n",
      "=== Outer Fold 2/10 ===\n",
      "  Epoch 1/2, Train loss=0.5528\n",
      "  Epoch 2/2, Train loss=0.4022\n",
      "  Epoch 1/2, Train loss=0.5897\n",
      "  Epoch 2/2, Train loss=0.4258\n",
      "  Epoch 1/2, Train loss=0.5813\n",
      "  Epoch 2/2, Train loss=0.4219\n",
      "  Epoch 1/2, Train loss=0.6391\n",
      "  Epoch 2/2, Train loss=0.4135\n",
      "  Epoch 1/2, Train loss=0.6361\n",
      "  Epoch 2/2, Train loss=0.4187\n",
      "  Epoch 1/2, Train loss=0.6171\n",
      "  Epoch 2/2, Train loss=0.3987\n",
      "  Epoch 1/2, Train loss=0.7643\n",
      "  Epoch 2/2, Train loss=0.5098\n",
      "  Epoch 1/2, Train loss=0.7455\n",
      "  Epoch 2/2, Train loss=0.5117\n",
      "  Epoch 1/2, Train loss=0.7599\n",
      "  Epoch 2/2, Train loss=0.5290\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.5938\n",
      "  Epoch 2/3, Train loss=0.3858\n",
      "  Epoch 3/3, Train loss=0.3216\n",
      "Fold 2: Acc=0.880, P=0.894, R=0.880, F1=0.887\n",
      "\n",
      "=== Outer Fold 3/10 ===\n",
      "  Epoch 1/2, Train loss=0.6266\n",
      "  Epoch 2/2, Train loss=0.4474\n",
      "  Epoch 1/2, Train loss=0.6234\n",
      "  Epoch 2/2, Train loss=0.4432\n",
      "  Epoch 1/2, Train loss=0.6154\n",
      "  Epoch 2/2, Train loss=0.4452\n",
      "  Epoch 1/2, Train loss=0.6206\n",
      "  Epoch 2/2, Train loss=0.4254\n",
      "  Epoch 1/2, Train loss=0.6511\n",
      "  Epoch 2/2, Train loss=0.4153\n",
      "  Epoch 1/2, Train loss=0.6190\n",
      "  Epoch 2/2, Train loss=0.4014\n",
      "  Epoch 1/2, Train loss=0.7749\n",
      "  Epoch 2/2, Train loss=0.5356\n",
      "  Epoch 1/2, Train loss=0.7294\n",
      "  Epoch 2/2, Train loss=0.4960\n",
      "  Epoch 1/2, Train loss=0.7417\n",
      "  Epoch 2/2, Train loss=0.5028\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.5956\n",
      "  Epoch 2/3, Train loss=0.3987\n",
      "  Epoch 3/3, Train loss=0.3421\n",
      "Fold 3: Acc=0.904, P=0.909, R=0.904, F1=0.907\n",
      "\n",
      "=== Outer Fold 4/10 ===\n",
      "  Epoch 1/2, Train loss=0.5968\n",
      "  Epoch 2/2, Train loss=0.4408\n",
      "  Epoch 1/2, Train loss=0.6008\n",
      "  Epoch 2/2, Train loss=0.4217\n",
      "  Epoch 1/2, Train loss=0.5772\n",
      "  Epoch 2/2, Train loss=0.3969\n",
      "  Epoch 1/2, Train loss=0.6639\n",
      "  Epoch 2/2, Train loss=0.4522\n",
      "  Epoch 1/2, Train loss=0.6175\n",
      "  Epoch 2/2, Train loss=0.4048\n",
      "  Epoch 1/2, Train loss=0.6232\n",
      "  Epoch 2/2, Train loss=0.4196\n",
      "  Epoch 1/2, Train loss=0.7532\n",
      "  Epoch 2/2, Train loss=0.5090\n",
      "  Epoch 1/2, Train loss=0.7779\n",
      "  Epoch 2/2, Train loss=0.5552\n",
      "  Epoch 1/2, Train loss=0.7408\n",
      "  Epoch 2/2, Train loss=0.5079\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.5580\n",
      "  Epoch 2/3, Train loss=0.3590\n",
      "  Epoch 3/3, Train loss=0.3080\n",
      "Fold 4: Acc=0.907, P=0.907, R=0.907, F1=0.907\n",
      "\n",
      "=== Outer Fold 5/10 ===\n",
      "  Epoch 1/2, Train loss=0.5674\n",
      "  Epoch 2/2, Train loss=0.4207\n",
      "  Epoch 1/2, Train loss=0.5998\n",
      "  Epoch 2/2, Train loss=0.4183\n",
      "  Epoch 1/2, Train loss=0.5776\n",
      "  Epoch 2/2, Train loss=0.4086\n",
      "  Epoch 1/2, Train loss=0.6378\n",
      "  Epoch 2/2, Train loss=0.4365\n",
      "  Epoch 1/2, Train loss=0.6351\n",
      "  Epoch 2/2, Train loss=0.4271\n",
      "  Epoch 1/2, Train loss=0.6078\n",
      "  Epoch 2/2, Train loss=0.3789\n",
      "  Epoch 1/2, Train loss=0.7692\n",
      "  Epoch 2/2, Train loss=0.5133\n",
      "  Epoch 1/2, Train loss=0.7538\n",
      "  Epoch 2/2, Train loss=0.5007\n",
      "  Epoch 1/2, Train loss=0.7633\n",
      "  Epoch 2/2, Train loss=0.4946\n",
      "Best LR = 3e-04\n",
      "  Epoch 1/3, Train loss=0.6855\n",
      "  Epoch 2/3, Train loss=0.4440\n",
      "  Epoch 3/3, Train loss=0.3663\n",
      "Fold 5: Acc=0.927, P=0.927, R=0.927, F1=0.927\n",
      "\n",
      "=== Outer Fold 6/10 ===\n",
      "  Epoch 1/2, Train loss=0.6277\n",
      "  Epoch 2/2, Train loss=0.4394\n",
      "  Epoch 1/2, Train loss=0.6120\n",
      "  Epoch 2/2, Train loss=0.4255\n",
      "  Epoch 1/2, Train loss=0.5544\n",
      "  Epoch 2/2, Train loss=0.4193\n",
      "  Epoch 1/2, Train loss=0.6421\n",
      "  Epoch 2/2, Train loss=0.4411\n",
      "  Epoch 1/2, Train loss=0.6244\n",
      "  Epoch 2/2, Train loss=0.4063\n",
      "  Epoch 1/2, Train loss=0.6066\n",
      "  Epoch 2/2, Train loss=0.4092\n",
      "  Epoch 1/2, Train loss=0.7630\n",
      "  Epoch 2/2, Train loss=0.5410\n",
      "  Epoch 1/2, Train loss=0.7187\n",
      "  Epoch 2/2, Train loss=0.4688\n",
      "  Epoch 1/2, Train loss=0.7698\n",
      "  Epoch 2/2, Train loss=0.5005\n",
      "Best LR = 3e-03\n",
      "  Epoch 1/3, Train loss=0.5469\n",
      "  Epoch 2/3, Train loss=0.3940\n",
      "  Epoch 3/3, Train loss=0.3459\n",
      "Fold 6: Acc=0.902, P=0.908, R=0.902, F1=0.905\n",
      "\n",
      "=== Outer Fold 7/10 ===\n",
      "  Epoch 1/2, Train loss=0.6093\n",
      "  Epoch 2/2, Train loss=0.4422\n",
      "  Epoch 1/2, Train loss=0.5725\n",
      "  Epoch 2/2, Train loss=0.4164\n",
      "  Epoch 1/2, Train loss=0.5809\n",
      "  Epoch 2/2, Train loss=0.4291\n",
      "  Epoch 1/2, Train loss=0.6088\n",
      "  Epoch 2/2, Train loss=0.3971\n",
      "  Epoch 1/2, Train loss=0.6182\n",
      "  Epoch 2/2, Train loss=0.4110\n",
      "  Epoch 1/2, Train loss=0.6190\n",
      "  Epoch 2/2, Train loss=0.4207\n",
      "  Epoch 1/2, Train loss=0.7398\n",
      "  Epoch 2/2, Train loss=0.5003\n",
      "  Epoch 1/2, Train loss=0.8073\n",
      "  Epoch 2/2, Train loss=0.5323\n",
      "  Epoch 1/2, Train loss=0.7233\n",
      "  Epoch 2/2, Train loss=0.4874\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.5576\n",
      "  Epoch 2/3, Train loss=0.3738\n",
      "  Epoch 3/3, Train loss=0.3244\n",
      "Fold 7: Acc=0.895, P=0.903, R=0.895, F1=0.899\n",
      "\n",
      "=== Outer Fold 8/10 ===\n",
      "  Epoch 1/2, Train loss=0.5896\n",
      "  Epoch 2/2, Train loss=0.4226\n",
      "  Epoch 1/2, Train loss=0.6212\n",
      "  Epoch 2/2, Train loss=0.4139\n",
      "  Epoch 1/2, Train loss=0.6418\n",
      "  Epoch 2/2, Train loss=0.4659\n",
      "  Epoch 1/2, Train loss=0.6305\n",
      "  Epoch 2/2, Train loss=0.4250\n",
      "  Epoch 1/2, Train loss=0.6255\n",
      "  Epoch 2/2, Train loss=0.4073\n",
      "  Epoch 1/2, Train loss=0.6134\n",
      "  Epoch 2/2, Train loss=0.3988\n",
      "  Epoch 1/2, Train loss=0.8082\n",
      "  Epoch 2/2, Train loss=0.5475\n",
      "  Epoch 1/2, Train loss=0.7675\n",
      "  Epoch 2/2, Train loss=0.5025\n",
      "  Epoch 1/2, Train loss=0.7363\n",
      "  Epoch 2/2, Train loss=0.5074\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.5426\n",
      "  Epoch 2/3, Train loss=0.3541\n",
      "  Epoch 3/3, Train loss=0.3018\n",
      "Fold 8: Acc=0.926, P=0.926, R=0.926, F1=0.926\n",
      "\n",
      "=== Outer Fold 9/10 ===\n",
      "  Epoch 1/2, Train loss=0.5928\n",
      "  Epoch 2/2, Train loss=0.3975\n",
      "  Epoch 1/2, Train loss=0.5930\n",
      "  Epoch 2/2, Train loss=0.4249\n",
      "  Epoch 1/2, Train loss=0.5625\n",
      "  Epoch 2/2, Train loss=0.3890\n",
      "  Epoch 1/2, Train loss=0.5979\n",
      "  Epoch 2/2, Train loss=0.3937\n",
      "  Epoch 1/2, Train loss=0.5989\n",
      "  Epoch 2/2, Train loss=0.3856\n",
      "  Epoch 1/2, Train loss=0.6485\n",
      "  Epoch 2/2, Train loss=0.4255\n",
      "  Epoch 1/2, Train loss=0.8245\n",
      "  Epoch 2/2, Train loss=0.5686\n",
      "  Epoch 1/2, Train loss=0.7106\n",
      "  Epoch 2/2, Train loss=0.5002\n",
      "  Epoch 1/2, Train loss=0.7666\n",
      "  Epoch 2/2, Train loss=0.5033\n",
      "Best LR = 3e-03\n",
      "  Epoch 1/3, Train loss=0.5573\n",
      "  Epoch 2/3, Train loss=0.4023\n",
      "  Epoch 3/3, Train loss=0.3584\n",
      "Fold 9: Acc=0.921, P=0.922, R=0.921, F1=0.921\n",
      "\n",
      "=== Outer Fold 10/10 ===\n",
      "  Epoch 1/2, Train loss=0.6010\n",
      "  Epoch 2/2, Train loss=0.4094\n",
      "  Epoch 1/2, Train loss=0.6610\n",
      "  Epoch 2/2, Train loss=0.4729\n",
      "  Epoch 1/2, Train loss=0.6212\n",
      "  Epoch 2/2, Train loss=0.4477\n",
      "  Epoch 1/2, Train loss=0.5882\n",
      "  Epoch 2/2, Train loss=0.3957\n",
      "  Epoch 1/2, Train loss=0.6130\n",
      "  Epoch 2/2, Train loss=0.3931\n",
      "  Epoch 1/2, Train loss=0.6172\n",
      "  Epoch 2/2, Train loss=0.4067\n",
      "  Epoch 1/2, Train loss=0.7607\n",
      "  Epoch 2/2, Train loss=0.4874\n",
      "  Epoch 1/2, Train loss=0.7773\n",
      "  Epoch 2/2, Train loss=0.5372\n",
      "  Epoch 1/2, Train loss=0.7711\n",
      "  Epoch 2/2, Train loss=0.5328\n",
      "Best LR = 1e-03\n",
      "  Epoch 1/3, Train loss=0.5504\n",
      "  Epoch 2/3, Train loss=0.3685\n",
      "  Epoch 3/3, Train loss=0.3088\n",
      "Fold 10: Acc=0.883, P=0.891, R=0.883, F1=0.887\n",
      "\n",
      "=== Nested CV Results ===\n",
      "Accuracy : 0.907 ± 0.017\n",
      "Precision: 0.912 ± 0.013\n",
      "Recall   : 0.907 ± 0.017\n",
      "F1-score : 0.909 ± 0.015\n"
     ]
    }
   ],
   "source": [
    "# --- DeepCNN --- \n",
    "print(\"\\n### Evaluating DeepCNN ###\")\n",
    "deep_mean_A, deep_std_A = evaluate_model_nested_cv(\n",
    "    X, y,\n",
    "    model_builder=lambda: DeepCNN(num_classes),\n",
    "    candidate_lr=[3e-3, 1e-3, 3e-4],\n",
    "    k_outer=10,\n",
    "    k_inner=3,\n",
    "    epochs=3,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T08:32:44.578537Z",
     "iopub.status.busy": "2025-10-08T08:32:44.577792Z",
     "iopub.status.idle": "2025-10-08T08:32:44.600646Z",
     "shell.execute_reply": "2025-10-08T08:32:44.599856Z",
     "shell.execute_reply.started": "2025-10-08T08:32:44.578510Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV results saved!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "cv_results_A = {\n",
    "    'mean': deep_mean_A,\n",
    "    'std': deep_std_A,\n",
    "    'accuracy': deep_mean_A[0],\n",
    "    'precision': deep_mean_A[1],\n",
    "    'recall': deep_mean_A[2],\n",
    "    'f1': deep_mean_A[3]\n",
    "}\n",
    "\n",
    "with open('/kaggle/working/cv_results_trainA.pkl', 'wb') as f:\n",
    "    pickle.dump(cv_results_A, f)\n",
    "\n",
    "print(\"CV results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T08:36:15.045019Z",
     "iopub.status.busy": "2025-10-08T08:36:15.044454Z",
     "iopub.status.idle": "2025-10-08T08:38:28.789110Z",
     "shell.execute_reply": "2025-10-08T08:38:28.788431Z",
     "shell.execute_reply.started": "2025-10-08T08:36:15.044993Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training Final Model on Full Dataset (Clean Images) ===\n",
      "  Epoch 1/5, Train loss=0.5589\n",
      "  Epoch 2/5, Train loss=0.3616\n",
      "  Epoch 3/5, Train loss=0.3032\n",
      "  Epoch 4/5, Train loss=0.2716\n",
      "  Epoch 5/5, Train loss=0.2443\n",
      "Final model saved!\n"
     ]
    }
   ],
   "source": [
    "# After nested CV\n",
    "print(\"\\n=== Training Final Model on Full Dataset (Clean Images) ===\")\n",
    "\n",
    "final_model_clean = train_final_model(\n",
    "    X, y,\n",
    "    model_builder=lambda: DeepCNN(num_classes),\n",
    "    lr=1e-3,\n",
    "    epochs=5,\n",
    "    batch=64,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "torch.save(final_model_clean.state_dict(), '/kaggle/working/deepcnn_final_clean.pth')\n",
    "print(\"Final model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T08:43:07.694219Z",
     "iopub.status.busy": "2025-10-08T08:43:07.693682Z",
     "iopub.status.idle": "2025-10-08T08:43:15.157880Z",
     "shell.execute_reply": "2025-10-08T08:43:15.157132Z",
     "shell.execute_reply.started": "2025-10-08T08:43:07.694197Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Test Data (for Train_A Model) ===\n",
      "Loaded test data: (3000, 224, 224, 3) (3000,)\n",
      "Test data prepared: (3000, 3, 224, 224)\n",
      "\n",
      "=== Evaluating Train_A Model on Test Set ===\n",
      "\n",
      "=== Test Set Results (Train_A Model - Clean Training) ===\n",
      "Accuracy : 0.696\n",
      "Precision: 0.711\n",
      "Recall   : 0.696\n",
      "F1-score : 0.704\n",
      "\n",
      "Confusion Matrix:\n",
      "[[661 251  88]\n",
      " [100 764 136]\n",
      " [ 84 252 664]]\n",
      "Classes: ['Boot' 'Sandal' 'Shoe']\n",
      "\n",
      "=== Performance Comparison (Train_A) ===\n",
      "CV Performance:   0.907 ± 0.017\n",
      "Test Performance: 0.696\n",
      "Difference:       -0.211\n"
     ]
    }
   ],
   "source": [
    "# ===== LOAD AND PREPARE TEST DATA =====\n",
    "print(\"\\n=== Loading Test Data (for Train_A Model) ===\")\n",
    "test_data = np.load(os.path.join(data_dir, \"test.npz\"))\n",
    "X_test, y_test = test_data[\"X\"], test_data[\"y\"]\n",
    "print(\"Loaded test data:\", X_test.shape, y_test.shape)\n",
    "\n",
    "# Encode labels (use encoder from train_A)\n",
    "y_test = encoder.transform(y_test)  # Make sure this is the encoder fitted on train_A\n",
    "\n",
    "# Normalize images\n",
    "X_test = X_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Reshape for PyTorch (N,C,H,W)\n",
    "X_test = np.transpose(X_test, (0,3,1,2))\n",
    "y_test = y_test.astype(\"int64\")\n",
    "\n",
    "print(\"Test data prepared:\", X_test.shape)\n",
    "\n",
    "# ===== MAKE PREDICTIONS =====\n",
    "print(\"\\n=== Evaluating Train_A Model on Test Set ===\")\n",
    "\n",
    "test_ds = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "final_model_clean.eval()  # ← CHANGED: Use train_A model\n",
    "y_pred_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, _ in test_dl:\n",
    "        xb = xb.to(device)\n",
    "        probs = torch.softmax(final_model_clean(xb), 1)  # ← CHANGED\n",
    "        y_pred_test.append(torch.argmax(probs, 1).cpu().numpy())\n",
    "\n",
    "y_pred_test = np.concatenate(y_pred_test)\n",
    "\n",
    "# ===== CALCULATE METRICS =====\n",
    "cm_test = confusion_matrix_manual(y_test, y_pred_test, labels=np.unique(y_test))\n",
    "acc_test, prec_test, rec_test, f1_test = calc_metrics(cm_test)\n",
    "\n",
    "print(f\"\\n=== Test Set Results (Train_A Model - Clean Training) ===\")\n",
    "print(f\"Accuracy : {acc_test:.3f}\")\n",
    "print(f\"Precision: {prec_test:.3f}\")\n",
    "print(f\"Recall   : {rec_test:.3f}\")\n",
    "print(f\"F1-score : {f1_test:.3f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(cm_test)\n",
    "print(f\"Classes: {encoder.classes_}\")\n",
    "\n",
    "# ===== COMPARISON WITH CV =====\n",
    "\n",
    "# Load the saved CV results\n",
    "import pickle\n",
    "with open('/kaggle/working/cv_results_trainA.pkl', 'rb') as f:\n",
    "    cv_results_A = pickle.load(f)\n",
    "\n",
    "# Extract the variables you need\n",
    "deep_mean_A = cv_results_A['mean']\n",
    "deep_std_A = cv_results_A['std']\n",
    "\n",
    "print(f\"\\n=== Performance Comparison (Train_A) ===\")\n",
    "print(f\"CV Performance:   {deep_mean_A[0]:.3f} ± {deep_std_A[0]:.3f}\")  # ← Use train_A CV results\n",
    "print(f\"Test Performance: {acc_test:.3f}\")\n",
    "print(f\"Difference:       {acc_test - deep_mean_A[0]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8420823,
     "sourceId": 13286972,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
