{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13286972,"sourceType":"datasetVersion","datasetId":8420823}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-08T07:06:11.881275Z","iopub.execute_input":"2025-10-08T07:06:11.882011Z","iopub.status.idle":"2025-10-08T07:06:12.692755Z","shell.execute_reply.started":"2025-10-08T07:06:11.881985Z","shell.execute_reply":"2025-10-08T07:06:12.692118Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/shoe-images/train_B.npz\n/kaggle/input/shoe-images/test.npz\n/kaggle/input/shoe-images/train_A.npz\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os, random, numpy as np\nimport torch\nfrom torchvision import models\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport gc","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T08:33:59.655087Z","iopub.execute_input":"2025-10-08T08:33:59.655514Z","iopub.status.idle":"2025-10-08T08:34:06.988418Z","shell.execute_reply.started":"2025-10-08T08:33:59.655483Z","shell.execute_reply":"2025-10-08T08:34:06.987616Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T08:34:06.989589Z","iopub.execute_input":"2025-10-08T08:34:06.989983Z","iopub.status.idle":"2025-10-08T08:34:07.072581Z","shell.execute_reply.started":"2025-10-08T08:34:06.989954Z","shell.execute_reply":"2025-10-08T08:34:07.071702Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"data_dir = \"/kaggle/input/shoe-images\"\ndata = np.load(os.path.join(data_dir, \"train_B.npz\"))  # or train_A.npz\nX, y = data[\"X\"], data[\"y\"]\nprint(\"Loaded:\", X.shape, y.shape)\n\n# ---------- encode string labels to ints ----------\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)            # e.g. Boot→0, Sandal→1, Shoe→2\nprint(\"Label mapping:\", dict(zip(encoder.classes_,\n                                 range(len(encoder.classes_)))))\n\n# ---------- normalise images ----------\nX = X.astype(\"float32\") / 255.0\n\n# ---------- reshape for PyTorch (N,C,H,W) ----------\nX = np.transpose(X, (0,3,1,2))\ny = y.astype(\"int64\")\nnum_classes = len(np.unique(y))\nprint(\"Final tensors:\", X.shape, \"Classes:\", num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T01:09:47.760354Z","iopub.execute_input":"2025-10-08T01:09:47.760894Z","iopub.status.idle":"2025-10-08T01:10:08.199880Z","shell.execute_reply.started":"2025-10-08T01:09:47.760871Z","shell.execute_reply":"2025-10-08T01:10:08.199200Z"}},"outputs":[{"name":"stdout","text":"Loaded: (12000, 224, 224, 3) (12000,)\nLabel mapping: {'Boot': 0, 'Sandal': 1, 'Shoe': 2}\nFinal tensors: (12000, 3, 224, 224) Classes: 3\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def make_folds(n, k=10, seed=42, y=None):\n    \"\"\"\n    Create stratified k-fold splits that preserve class distribution.\n    \n    Parameters:\n    - n: Total number of samples\n    - k: Number of folds (default=10)\n    - seed: Random seed for reproducibility (default=42)\n    - y: Labels for stratification (required)\n    \n    Returns:\n    - List of k arrays containing indices for each fold\n    \"\"\"\n    np.random.seed(seed)\n    \n    unique_classes = np.unique(y)\n    folds = [[] for _ in range(k)]\n    \n    # For each class, split its samples across k folds\n    for cls in unique_classes:\n        cls_indices = np.where(y == cls)[0]\n        np.random.shuffle(cls_indices)\n        cls_splits = np.array_split(cls_indices, k)\n        \n        # Add class samples to each fold\n        for fold_idx, split in enumerate(cls_splits):\n            folds[fold_idx].extend(split)\n    \n    # Shuffle within each fold and convert to numpy arrays\n    for i in range(k):\n        np.random.shuffle(folds[i])\n        folds[i] = np.array(folds[i])\n    \n    return folds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T08:35:33.900781Z","iopub.execute_input":"2025-10-08T08:35:33.901098Z","iopub.status.idle":"2025-10-08T08:35:33.907385Z","shell.execute_reply.started":"2025-10-08T08:35:33.901075Z","shell.execute_reply":"2025-10-08T08:35:33.906555Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def confusion_matrix_manual(y_true, y_pred, labels):\n    n = len(labels)\n    label_to_idx = {lab: i for i, lab in enumerate(labels)}\n    cm = np.zeros((n, n), dtype=int)\n    for yt, yp in zip(y_true, y_pred):\n        i = label_to_idx[yt]\n        j = label_to_idx[yp]\n        cm[i, j] += 1\n    return cm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T08:35:35.725752Z","iopub.execute_input":"2025-10-08T08:35:35.726290Z","iopub.status.idle":"2025-10-08T08:35:35.730570Z","shell.execute_reply.started":"2025-10-08T08:35:35.726267Z","shell.execute_reply":"2025-10-08T08:35:35.729823Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def calc_metrics(cm):\n    TP = np.diag(cm)\n    FP = cm.sum(0) - TP\n    FN = cm.sum(1) - TP\n    precision = np.mean(TP / (TP + FP + 1e-9))\n    recall    = np.mean(TP / (TP + FN + 1e-9))\n    f1 = 2 * precision * recall / (precision + recall + 1e-9)\n    acc = TP.sum() / cm.sum()\n    return acc, precision, recall, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T08:35:37.171707Z","iopub.execute_input":"2025-10-08T08:35:37.172173Z","iopub.status.idle":"2025-10-08T08:35:37.176624Z","shell.execute_reply.started":"2025-10-08T08:35:37.172151Z","shell.execute_reply":"2025-10-08T08:35:37.175878Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def train_one_fold(X_train, y_train, X_val, y_val, model_builder,\n                   lr=1e-3, epochs=5, batch=64, device=\"cpu\"):\n    \"\"\"Train one fold and return model + predictions on validation set.\"\"\"\n    \n    train_ds = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n    val_ds   = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n    train_dl = DataLoader(train_ds, batch_size=batch, shuffle=True)\n    val_dl   = DataLoader(val_ds, batch_size=batch, shuffle=False)\n\n    # note: difference here — build model dynamically\n    model = model_builder().to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    opt = optim.Adam(model.parameters(), lr=lr)\n\n    for ep in range(epochs):\n        model.train()\n        total_loss = 0.0\n        for xb, yb in train_dl:\n            xb, yb = xb.to(device), yb.to(device)\n            out = model(xb)\n            loss = loss_fn(out, yb)\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            total_loss += loss.item()\n        avg_loss = total_loss / len(train_dl)\n        print(f\"  Epoch {ep+1}/{epochs}, Train loss={avg_loss:.4f}\")\n\n    # ----- validation predictions -----\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for xb, _ in val_dl:\n            xb = xb.to(device)\n            probs = torch.softmax(model(xb), 1)\n            preds.append(torch.argmax(probs, 1).cpu().numpy())\n    preds = np.concatenate(preds)\n    return model, preds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T08:35:47.335620Z","iopub.execute_input":"2025-10-08T08:35:47.335861Z","iopub.status.idle":"2025-10-08T08:35:47.343289Z","shell.execute_reply.started":"2025-10-08T08:35:47.335845Z","shell.execute_reply":"2025-10-08T08:35:47.342518Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def evaluate_model_nested_cv(\n    X, y, model_builder,\n    candidate_lr=[1e-3, 3e-4, 1e-4],\n    k_outer=10, k_inner=3, epochs=5,\n    device=\"cpu\"\n):\n    \"\"\"Generic nested cross‑validation for any model.\"\"\"\n\n    folds = make_folds(len(X), k_outer, seed=42, y=y)\n    metrics_all = []\n\n    for i in range(k_outer):\n        print(f\"\\n=== Outer Fold {i+1}/{k_outer} ===\")\n\n        test_idx = folds[i]\n        train_idx = np.concatenate([folds[j] for j in range(k_outer) if j != i])\n        X_train, y_train = X[train_idx], y[train_idx]\n        X_test,  y_test  = X[test_idx],  y[test_idx]\n\n        # ---- inner loop: tuning learning rate ----\n        inner_folds = make_folds(len(X_train), k_inner, seed=42, y=y_train)\n        mean_accs = []\n\n        for lr in candidate_lr:\n            inner_scores = []\n            for j in range(k_inner):\n                val_idx = inner_folds[j]\n                tr_idx  = np.concatenate([inner_folds[m] for m in range(k_inner) if m != j])\n\n                _, y_pred_val = train_one_fold(\n                    X_train[tr_idx], y_train[tr_idx],\n                    X_train[val_idx], y_train[val_idx],\n                    model_builder=model_builder,\n                    lr=lr, epochs=2, device=device\n                )\n\n                cm = confusion_matrix_manual(y_train[val_idx], y_pred_val, labels=np.unique(y))\n                acc, prec, rec, f1 = calc_metrics(cm)\n                inner_scores.append(acc)\n\n            mean_accs.append(np.mean(inner_scores))\n\n        best_lr = candidate_lr[int(np.argmax(mean_accs))]\n        print(f\"Best LR = {best_lr:.0e}\")\n\n        # ---- outer test fold ----\n        model, y_pred = train_one_fold(\n            X_train, y_train, X_test, y_test,\n            model_builder=model_builder,\n            lr=best_lr, epochs=epochs, device=device\n        )\n\n        cm = confusion_matrix_manual(y_test, y_pred, labels=np.unique(y))\n        acc, prec, rec, f1 = calc_metrics(cm)\n        metrics_all.append([acc, prec, rec, f1])\n\n        print(f\"Fold {i+1}: Acc={acc:.3f}, P={prec:.3f}, R={rec:.3f}, F1={f1:.3f}\")\n\n    # ---- summary ----\n    metrics_all = np.array(metrics_all)\n    mean, std = metrics_all.mean(0), metrics_all.std(0)\n\n    print(\"\\n=== Nested CV Results ===\")\n    print(f\"Accuracy : {mean[0]:.3f} ± {std[0]:.3f}\")\n    print(f\"Precision: {mean[1]:.3f} ± {std[1]:.3f}\")\n    print(f\"Recall   : {mean[2]:.3f} ± {std[2]:.3f}\")\n    print(f\"F1-score : {mean[3]:.3f} ± {std[3]:.3f}\")\n\n    return mean, std","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T08:35:51.702962Z","iopub.execute_input":"2025-10-08T08:35:51.703243Z","iopub.status.idle":"2025-10-08T08:35:51.712602Z","shell.execute_reply.started":"2025-10-08T08:35:51.703221Z","shell.execute_reply":"2025-10-08T08:35:51.711764Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class DeepCNN(nn.Module):\n    def __init__(self, n_classes):\n        super().__init__()\n        # Feature extractor\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1),  # 32 filters\n            nn.ReLU(),\n            nn.MaxPool2d(2),                 # Downsample\n\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.MaxPool2d(2)\n        )\n\n        self.flatten_dim = None\n        self.classifier = None\n        self.n_classes = n_classes\n\n    def _get_flatten_dim(self, x):\n        with torch.no_grad():\n            f = self.features(x)\n            return f.view(f.size(0), -1).shape[1]\n\n    def forward(self, x):\n        if self.classifier is None:\n            flat_dim = self._get_flatten_dim(x)\n            self.classifier = nn.Sequential(\n                nn.Flatten(),\n                nn.Linear(flat_dim, 128), nn.ReLU(),\n                nn.Dropout(0.4),\n                nn.Linear(128, self.n_classes)\n            ).to(x.device)\n        out = self.features(x)\n        out = self.classifier(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T08:35:57.657868Z","iopub.execute_input":"2025-10-08T08:35:57.658158Z","iopub.status.idle":"2025-10-08T08:35:57.664750Z","shell.execute_reply.started":"2025-10-08T08:35:57.658137Z","shell.execute_reply":"2025-10-08T08:35:57.664085Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# --- DeepCNN ---\nprint(\"\\n### Evaluating DeepCNN ###\")\ndeep_mean_B, deep_std_B = evaluate_model_nested_cv(\n    X, y,\n    model_builder=lambda: DeepCNN(num_classes),\n    candidate_lr=[3e-3, 1e-3, 3e-4],\n    k_outer=10,\n    k_inner=3,\n    epochs=3,\n    device=device\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T01:10:08.292924Z","iopub.execute_input":"2025-10-08T01:10:08.293154Z","iopub.status.idle":"2025-10-08T02:23:47.727635Z","shell.execute_reply.started":"2025-10-08T01:10:08.293139Z","shell.execute_reply":"2025-10-08T02:23:47.726762Z"}},"outputs":[{"name":"stdout","text":"\n### Evaluating DeepCNN ###\n\n=== Outer Fold 1/10 ===\n  Epoch 1/2, Train loss=0.8530\n  Epoch 2/2, Train loss=0.6977\n  Epoch 1/2, Train loss=0.8345\n  Epoch 2/2, Train loss=0.6829\n  Epoch 1/2, Train loss=0.8199\n  Epoch 2/2, Train loss=0.6747\n  Epoch 1/2, Train loss=0.8483\n  Epoch 2/2, Train loss=0.6708\n  Epoch 1/2, Train loss=0.8495\n  Epoch 2/2, Train loss=0.6716\n  Epoch 1/2, Train loss=0.8362\n  Epoch 2/2, Train loss=0.6569\n  Epoch 1/2, Train loss=0.9176\n  Epoch 2/2, Train loss=0.7366\n  Epoch 1/2, Train loss=0.9269\n  Epoch 2/2, Train loss=0.7455\n  Epoch 1/2, Train loss=0.9199\n  Epoch 2/2, Train loss=0.7238\nBest LR = 3e-04\n  Epoch 1/3, Train loss=0.8641\n  Epoch 2/3, Train loss=0.6765\n  Epoch 3/3, Train loss=0.5997\nFold 1: Acc=0.787, P=0.791, R=0.787, F1=0.789\n\n=== Outer Fold 2/10 ===\n  Epoch 1/2, Train loss=0.8492\n  Epoch 2/2, Train loss=0.7184\n  Epoch 1/2, Train loss=0.8304\n  Epoch 2/2, Train loss=0.6707\n  Epoch 1/2, Train loss=0.8230\n  Epoch 2/2, Train loss=0.6967\n  Epoch 1/2, Train loss=0.8165\n  Epoch 2/2, Train loss=0.6529\n  Epoch 1/2, Train loss=0.8184\n  Epoch 2/2, Train loss=0.6290\n  Epoch 1/2, Train loss=0.8256\n  Epoch 2/2, Train loss=0.6594\n  Epoch 1/2, Train loss=0.9489\n  Epoch 2/2, Train loss=0.7691\n  Epoch 1/2, Train loss=0.9097\n  Epoch 2/2, Train loss=0.7042\n  Epoch 1/2, Train loss=0.9059\n  Epoch 2/2, Train loss=0.7114\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.7923\n  Epoch 2/3, Train loss=0.6231\n  Epoch 3/3, Train loss=0.5553\nFold 2: Acc=0.814, P=0.822, R=0.814, F1=0.818\n\n=== Outer Fold 3/10 ===\n  Epoch 1/2, Train loss=0.8512\n  Epoch 2/2, Train loss=0.7248\n  Epoch 1/2, Train loss=0.8571\n  Epoch 2/2, Train loss=0.7061\n  Epoch 1/2, Train loss=0.8114\n  Epoch 2/2, Train loss=0.6635\n  Epoch 1/2, Train loss=0.8087\n  Epoch 2/2, Train loss=0.6426\n  Epoch 1/2, Train loss=0.8785\n  Epoch 2/2, Train loss=0.7053\n  Epoch 1/2, Train loss=0.8484\n  Epoch 2/2, Train loss=0.6639\n  Epoch 1/2, Train loss=0.9098\n  Epoch 2/2, Train loss=0.7443\n  Epoch 1/2, Train loss=0.8913\n  Epoch 2/2, Train loss=0.7217\n  Epoch 1/2, Train loss=0.9280\n  Epoch 2/2, Train loss=0.7364\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.7904\n  Epoch 2/3, Train loss=0.6109\n  Epoch 3/3, Train loss=0.5385\nFold 3: Acc=0.810, P=0.810, R=0.810, F1=0.810\n\n=== Outer Fold 4/10 ===\n  Epoch 1/2, Train loss=0.8879\n  Epoch 2/2, Train loss=0.7050\n  Epoch 1/2, Train loss=0.8330\n  Epoch 2/2, Train loss=0.6843\n  Epoch 1/2, Train loss=0.8132\n  Epoch 2/2, Train loss=0.6582\n  Epoch 1/2, Train loss=0.8381\n  Epoch 2/2, Train loss=0.6601\n  Epoch 1/2, Train loss=0.8590\n  Epoch 2/2, Train loss=0.6805\n  Epoch 1/2, Train loss=0.8153\n  Epoch 2/2, Train loss=0.6486\n  Epoch 1/2, Train loss=0.8942\n  Epoch 2/2, Train loss=0.7146\n  Epoch 1/2, Train loss=0.9089\n  Epoch 2/2, Train loss=0.7430\n  Epoch 1/2, Train loss=0.9115\n  Epoch 2/2, Train loss=0.7178\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.7880\n  Epoch 2/3, Train loss=0.6187\n  Epoch 3/3, Train loss=0.5520\nFold 4: Acc=0.807, P=0.810, R=0.807, F1=0.809\n\n=== Outer Fold 5/10 ===\n  Epoch 1/2, Train loss=0.8638\n  Epoch 2/2, Train loss=0.7108\n  Epoch 1/2, Train loss=0.8623\n  Epoch 2/2, Train loss=0.7248\n  Epoch 1/2, Train loss=0.8141\n  Epoch 2/2, Train loss=0.6774\n  Epoch 1/2, Train loss=0.8329\n  Epoch 2/2, Train loss=0.6610\n  Epoch 1/2, Train loss=0.8349\n  Epoch 2/2, Train loss=0.6487\n  Epoch 1/2, Train loss=0.8163\n  Epoch 2/2, Train loss=0.6497\n  Epoch 1/2, Train loss=0.9311\n  Epoch 2/2, Train loss=0.7478\n  Epoch 1/2, Train loss=0.9083\n  Epoch 2/2, Train loss=0.7217\n  Epoch 1/2, Train loss=0.9063\n  Epoch 2/2, Train loss=0.7229\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.7849\n  Epoch 2/3, Train loss=0.6189\n  Epoch 3/3, Train loss=0.5506\nFold 5: Acc=0.821, P=0.833, R=0.821, F1=0.827\n\n=== Outer Fold 6/10 ===\n  Epoch 1/2, Train loss=0.8827\n  Epoch 2/2, Train loss=0.7388\n  Epoch 1/2, Train loss=0.8320\n  Epoch 2/2, Train loss=0.6956\n  Epoch 1/2, Train loss=0.8227\n  Epoch 2/2, Train loss=0.6983\n  Epoch 1/2, Train loss=0.8216\n  Epoch 2/2, Train loss=0.6486\n  Epoch 1/2, Train loss=0.8548\n  Epoch 2/2, Train loss=0.6811\n  Epoch 1/2, Train loss=0.8531\n  Epoch 2/2, Train loss=0.6867\n  Epoch 1/2, Train loss=0.9211\n  Epoch 2/2, Train loss=0.7332\n  Epoch 1/2, Train loss=0.9492\n  Epoch 2/2, Train loss=0.7415\n  Epoch 1/2, Train loss=0.9422\n  Epoch 2/2, Train loss=0.7614\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.7807\n  Epoch 2/3, Train loss=0.6111\n  Epoch 3/3, Train loss=0.5476\nFold 6: Acc=0.833, P=0.838, R=0.833, F1=0.836\n\n=== Outer Fold 7/10 ===\n  Epoch 1/2, Train loss=0.8531\n  Epoch 2/2, Train loss=0.7113\n  Epoch 1/2, Train loss=0.8602\n  Epoch 2/2, Train loss=0.6953\n  Epoch 1/2, Train loss=0.8374\n  Epoch 2/2, Train loss=0.7119\n  Epoch 1/2, Train loss=0.8524\n  Epoch 2/2, Train loss=0.6854\n  Epoch 1/2, Train loss=0.8502\n  Epoch 2/2, Train loss=0.6761\n  Epoch 1/2, Train loss=0.8540\n  Epoch 2/2, Train loss=0.6584\n  Epoch 1/2, Train loss=0.9335\n  Epoch 2/2, Train loss=0.7545\n  Epoch 1/2, Train loss=0.9150\n  Epoch 2/2, Train loss=0.7222\n  Epoch 1/2, Train loss=0.9021\n  Epoch 2/2, Train loss=0.7195\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.7857\n  Epoch 2/3, Train loss=0.6136\n  Epoch 3/3, Train loss=0.5588\nFold 7: Acc=0.838, P=0.838, R=0.838, F1=0.838\n\n=== Outer Fold 8/10 ===\n  Epoch 1/2, Train loss=0.8670\n  Epoch 2/2, Train loss=0.7096\n  Epoch 1/2, Train loss=0.8522\n  Epoch 2/2, Train loss=0.7276\n  Epoch 1/2, Train loss=0.8184\n  Epoch 2/2, Train loss=0.6813\n  Epoch 1/2, Train loss=0.8378\n  Epoch 2/2, Train loss=0.6543\n  Epoch 1/2, Train loss=0.8377\n  Epoch 2/2, Train loss=0.6695\n  Epoch 1/2, Train loss=0.8660\n  Epoch 2/2, Train loss=0.6910\n  Epoch 1/2, Train loss=0.9198\n  Epoch 2/2, Train loss=0.7419\n  Epoch 1/2, Train loss=0.9030\n  Epoch 2/2, Train loss=0.7155\n  Epoch 1/2, Train loss=0.9080\n  Epoch 2/2, Train loss=0.7354\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.7886\n  Epoch 2/3, Train loss=0.6189\n  Epoch 3/3, Train loss=0.5468\nFold 8: Acc=0.818, P=0.821, R=0.817, F1=0.819\n\n=== Outer Fold 9/10 ===\n  Epoch 1/2, Train loss=0.8624\n  Epoch 2/2, Train loss=0.7035\n  Epoch 1/2, Train loss=0.8444\n  Epoch 2/2, Train loss=0.6868\n  Epoch 1/2, Train loss=0.8515\n  Epoch 2/2, Train loss=0.6875\n  Epoch 1/2, Train loss=0.8535\n  Epoch 2/2, Train loss=0.6630\n  Epoch 1/2, Train loss=0.8419\n  Epoch 2/2, Train loss=0.6740\n  Epoch 1/2, Train loss=0.8159\n  Epoch 2/2, Train loss=0.6409\n  Epoch 1/2, Train loss=0.9078\n  Epoch 2/2, Train loss=0.7318\n  Epoch 1/2, Train loss=0.9236\n  Epoch 2/2, Train loss=0.7369\n  Epoch 1/2, Train loss=0.9272\n  Epoch 2/2, Train loss=0.7383\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.7840\n  Epoch 2/3, Train loss=0.6046\n  Epoch 3/3, Train loss=0.5469\nFold 9: Acc=0.750, P=0.783, R=0.750, F1=0.766\n\n=== Outer Fold 10/10 ===\n  Epoch 1/2, Train loss=0.8408\n  Epoch 2/2, Train loss=0.7243\n  Epoch 1/2, Train loss=0.8555\n  Epoch 2/2, Train loss=0.7010\n  Epoch 1/2, Train loss=0.8251\n  Epoch 2/2, Train loss=0.7082\n  Epoch 1/2, Train loss=0.8487\n  Epoch 2/2, Train loss=0.6616\n  Epoch 1/2, Train loss=0.8270\n  Epoch 2/2, Train loss=0.6463\n  Epoch 1/2, Train loss=0.8265\n  Epoch 2/2, Train loss=0.6437\n  Epoch 1/2, Train loss=0.9216\n  Epoch 2/2, Train loss=0.7195\n  Epoch 1/2, Train loss=0.9166\n  Epoch 2/2, Train loss=0.7345\n  Epoch 1/2, Train loss=0.9295\n  Epoch 2/2, Train loss=0.7561\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.7774\n  Epoch 2/3, Train loss=0.5962\n  Epoch 3/3, Train loss=0.5333\nFold 10: Acc=0.802, P=0.815, R=0.802, F1=0.808\n\n=== Nested CV Results ===\nAccuracy : 0.808 ± 0.024\nPrecision: 0.816 ± 0.018\nRecall   : 0.808 ± 0.024\nF1-score : 0.812 ± 0.021\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pickle\n\ncv_results_B = {\n    'mean': deep_mean_B,\n    'std': deep_std_B,\n    'accuracy': deep_mean_B[0],\n    'precision': deep_mean_B[1],\n    'recall': deep_mean_B[2],\n    'f1': deep_mean_B[3]\n}\n\nwith open('/kaggle/working/cv_results_trainB.pkl', 'wb') as f:\n    pickle.dump(cv_results_B, f)\n\nprint(\"CV results saved!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training Final Model","metadata":{}},{"cell_type":"code","source":"def train_final_model(X, y, model_builder, lr=1e-3, epochs=5, batch=64, device=\"cpu\"):\n    \"\"\"Train final model on full dataset (no validation split).\"\"\"\n    \n    train_ds = TensorDataset(torch.tensor(X), torch.tensor(y))\n    train_dl = DataLoader(train_ds, batch_size=batch, shuffle=True)\n    \n    model = model_builder().to(device)\n    loss_fn = nn.CrossEntropyLoss()\n    opt = optim.Adam(model.parameters(), lr=lr)\n    \n    for ep in range(epochs):\n        model.train()\n        total_loss = 0.0\n        for xb, yb in train_dl:\n            xb, yb = xb.to(device), yb.to(device)\n            out = model(xb)\n            loss = loss_fn(out, yb)\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            total_loss += loss.item()\n        avg_loss = total_loss / len(train_dl)\n        print(f\"  Epoch {ep+1}/{epochs}, Train loss={avg_loss:.4f}\")\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T08:35:19.448761Z","iopub.execute_input":"2025-10-08T08:35:19.449035Z","iopub.status.idle":"2025-10-08T08:35:19.455100Z","shell.execute_reply.started":"2025-10-08T08:35:19.449014Z","shell.execute_reply":"2025-10-08T08:35:19.454363Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# After nested CV\nprint(\"\\n=== Training Final Model on Full Dataset ===\")\n\nfinal_model = train_final_model(\n    X, y,\n    model_builder=lambda: DeepCNN(num_classes),\n    lr=1e-3,\n    epochs=5,\n    batch=64,\n    device=device\n)\n\ntorch.save(final_model.state_dict(), '/kaggle/working/deepcnn_final.pth')\nprint(\"Final model saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T02:32:14.449468Z","iopub.execute_input":"2025-10-08T02:32:14.450095Z","iopub.status.idle":"2025-10-08T02:34:27.782237Z","shell.execute_reply.started":"2025-10-08T02:32:14.450070Z","shell.execute_reply":"2025-10-08T02:34:27.781371Z"}},"outputs":[{"name":"stdout","text":"\n=== Training Final Model on Full Dataset ===\n  Epoch 1/5, Train loss=0.7666\n  Epoch 2/5, Train loss=0.5881\n  Epoch 3/5, Train loss=0.5281\n  Epoch 4/5, Train loss=0.4872\n  Epoch 5/5, Train loss=0.4560\nFinal model saved!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"Testing Model","metadata":{}},{"cell_type":"code","source":"# ===== LOAD AND PREPARE TEST DATA =====\nprint(\"\\n=== Loading Test Data ===\")\ntest_data = np.load(os.path.join(data_dir, \"test.npz\"))\nX_test, y_test = test_data[\"X\"], test_data[\"y\"]\nprint(\"Loaded test data:\", X_test.shape, y_test.shape)\n\n# Encode labels (same encoder from training)\ny_test = encoder.transform(y_test)\n\n# Normalize images\nX_test = X_test.astype(\"float32\") / 255.0\n\n# Reshape for PyTorch (N,C,H,W)\nX_test = np.transpose(X_test, (0,3,1,2))\ny_test = y_test.astype(\"int64\")\n\nprint(\"Test data prepared:\", X_test.shape)\n\n# ===== MAKE PREDICTIONS =====\nprint(\"\\n=== Evaluating Final Model on Test Set ===\")\n\ntest_ds = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\ntest_dl = DataLoader(test_ds, batch_size=64, shuffle=False)\n\nfinal_model.eval()\ny_pred_test = []\n\nwith torch.no_grad():\n    for xb, _ in test_dl:\n        xb = xb.to(device)\n        probs = torch.softmax(final_model(xb), 1)\n        y_pred_test.append(torch.argmax(probs, 1).cpu().numpy())\n\ny_pred_test = np.concatenate(y_pred_test)\n\n# ===== CALCULATE METRICS =====\ncm_test = confusion_matrix_manual(y_test, y_pred_test, labels=np.unique(y_test))\nacc_test, prec_test, rec_test, f1_test = calc_metrics(cm_test)\n\nprint(f\"\\n=== Test Set Results ===\")\nprint(f\"Accuracy : {acc_test:.3f}\")\nprint(f\"Precision: {prec_test:.3f}\")\nprint(f\"Recall   : {rec_test:.3f}\")\nprint(f\"F1-score : {f1_test:.3f}\")\n\nprint(f\"\\nConfusion Matrix:\")\nprint(cm_test)\nprint(f\"Classes: {encoder.classes_}\")\n\n# ===== COMPARISON WITH CV =====\nprint(f\"\\n=== Performance Comparison ===\")\nprint(f\"CV Performance:   {0.808:.3f} ± {0.024:.3f}\")\nprint(f\"Test Performance: {acc_test:.3f}\")\nprint(f\"Difference:       {acc_test - 0.808:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T02:40:32.053827Z","iopub.execute_input":"2025-10-08T02:40:32.054733Z","iopub.status.idle":"2025-10-08T02:40:42.315145Z","shell.execute_reply.started":"2025-10-08T02:40:32.054703Z","shell.execute_reply":"2025-10-08T02:40:42.314311Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading Test Data ===\nLoaded test data: (3000, 224, 224, 3) (3000,)\nTest data prepared: (3000, 3, 224, 224)\n\n=== Evaluating Final Model on Test Set ===\n\n=== Test Set Results ===\nAccuracy : 0.856\nPrecision: 0.860\nRecall   : 0.856\nF1-score : 0.858\n\nConfusion Matrix:\n[[874  67  59]\n [ 25 866 109]\n [ 22 150 828]]\nClasses: ['Boot' 'Sandal' 'Shoe']\n\n=== Performance Comparison ===\nCV Performance:   0.808 ± 0.024\nTest Performance: 0.856\nDifference:       0.048\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"### Repeating the above, but this time using train_A (which is the image set with only clean images)","metadata":{}},{"cell_type":"code","source":"data_dir = \"/kaggle/input/shoe-images\"\ndata = np.load(os.path.join(data_dir, \"train_A.npz\"))  #changed to train_A.npz\nX, y = data[\"X\"], data[\"y\"]\nprint(\"Loaded:\", X.shape, y.shape)\n\n# ---------- encode string labels to ints ----------\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)            # e.g. Boot→0, Sandal→1, Shoe→2\nprint(\"Label mapping:\", dict(zip(encoder.classes_,\n                                 range(len(encoder.classes_)))))\n\n# ---------- normalise images ----------\nX = X.astype(\"float32\") / 255.0\n\n# ---------- reshape for PyTorch (N,C,H,W) ----------\nX = np.transpose(X, (0,3,1,2))\ny = y.astype(\"int64\")\nnum_classes = len(np.unique(y))\nprint(\"Final tensors:\", X.shape, \"Classes:\", num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T08:34:24.439877Z","iopub.execute_input":"2025-10-08T08:34:24.440159Z","iopub.status.idle":"2025-10-08T08:34:39.749610Z","shell.execute_reply.started":"2025-10-08T08:34:24.440136Z","shell.execute_reply":"2025-10-08T08:34:39.748773Z"}},"outputs":[{"name":"stdout","text":"Loaded: (12000, 224, 224, 3) (12000,)\nLabel mapping: {'Boot': 0, 'Sandal': 1, 'Shoe': 2}\nFinal tensors: (12000, 3, 224, 224) Classes: 3\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --- DeepCNN --- \nprint(\"\\n### Evaluating DeepCNN ###\")\ndeep_mean_A, deep_std_A = evaluate_model_nested_cv(\n    X, y,\n    model_builder=lambda: DeepCNN(num_classes),\n    candidate_lr=[3e-3, 1e-3, 3e-4],\n    k_outer=10,\n    k_inner=3,\n    epochs=3,\n    device=device\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T07:07:16.334280Z","iopub.execute_input":"2025-10-08T07:07:16.334907Z","iopub.status.idle":"2025-10-08T08:20:48.201173Z","shell.execute_reply.started":"2025-10-08T07:07:16.334880Z","shell.execute_reply":"2025-10-08T08:20:48.200379Z"}},"outputs":[{"name":"stdout","text":"\n### Evaluating DeepCNN ###\n\n=== Outer Fold 1/10 ===\n  Epoch 1/2, Train loss=0.5666\n  Epoch 2/2, Train loss=0.4016\n  Epoch 1/2, Train loss=0.6100\n  Epoch 2/2, Train loss=0.4277\n  Epoch 1/2, Train loss=0.5683\n  Epoch 2/2, Train loss=0.3877\n  Epoch 1/2, Train loss=0.5922\n  Epoch 2/2, Train loss=0.3824\n  Epoch 1/2, Train loss=0.6348\n  Epoch 2/2, Train loss=0.4321\n  Epoch 1/2, Train loss=0.6062\n  Epoch 2/2, Train loss=0.4086\n  Epoch 1/2, Train loss=0.7221\n  Epoch 2/2, Train loss=0.4885\n  Epoch 1/2, Train loss=0.7342\n  Epoch 2/2, Train loss=0.4893\n  Epoch 1/2, Train loss=0.7527\n  Epoch 2/2, Train loss=0.5159\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.5930\n  Epoch 2/3, Train loss=0.3864\n  Epoch 3/3, Train loss=0.3236\nFold 1: Acc=0.929, P=0.929, R=0.929, F1=0.929\n\n=== Outer Fold 2/10 ===\n  Epoch 1/2, Train loss=0.5528\n  Epoch 2/2, Train loss=0.4022\n  Epoch 1/2, Train loss=0.5897\n  Epoch 2/2, Train loss=0.4258\n  Epoch 1/2, Train loss=0.5813\n  Epoch 2/2, Train loss=0.4219\n  Epoch 1/2, Train loss=0.6391\n  Epoch 2/2, Train loss=0.4135\n  Epoch 1/2, Train loss=0.6361\n  Epoch 2/2, Train loss=0.4187\n  Epoch 1/2, Train loss=0.6171\n  Epoch 2/2, Train loss=0.3987\n  Epoch 1/2, Train loss=0.7643\n  Epoch 2/2, Train loss=0.5098\n  Epoch 1/2, Train loss=0.7455\n  Epoch 2/2, Train loss=0.5117\n  Epoch 1/2, Train loss=0.7599\n  Epoch 2/2, Train loss=0.5290\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.5938\n  Epoch 2/3, Train loss=0.3858\n  Epoch 3/3, Train loss=0.3216\nFold 2: Acc=0.880, P=0.894, R=0.880, F1=0.887\n\n=== Outer Fold 3/10 ===\n  Epoch 1/2, Train loss=0.6266\n  Epoch 2/2, Train loss=0.4474\n  Epoch 1/2, Train loss=0.6234\n  Epoch 2/2, Train loss=0.4432\n  Epoch 1/2, Train loss=0.6154\n  Epoch 2/2, Train loss=0.4452\n  Epoch 1/2, Train loss=0.6206\n  Epoch 2/2, Train loss=0.4254\n  Epoch 1/2, Train loss=0.6511\n  Epoch 2/2, Train loss=0.4153\n  Epoch 1/2, Train loss=0.6190\n  Epoch 2/2, Train loss=0.4014\n  Epoch 1/2, Train loss=0.7749\n  Epoch 2/2, Train loss=0.5356\n  Epoch 1/2, Train loss=0.7294\n  Epoch 2/2, Train loss=0.4960\n  Epoch 1/2, Train loss=0.7417\n  Epoch 2/2, Train loss=0.5028\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.5956\n  Epoch 2/3, Train loss=0.3987\n  Epoch 3/3, Train loss=0.3421\nFold 3: Acc=0.904, P=0.909, R=0.904, F1=0.907\n\n=== Outer Fold 4/10 ===\n  Epoch 1/2, Train loss=0.5968\n  Epoch 2/2, Train loss=0.4408\n  Epoch 1/2, Train loss=0.6008\n  Epoch 2/2, Train loss=0.4217\n  Epoch 1/2, Train loss=0.5772\n  Epoch 2/2, Train loss=0.3969\n  Epoch 1/2, Train loss=0.6639\n  Epoch 2/2, Train loss=0.4522\n  Epoch 1/2, Train loss=0.6175\n  Epoch 2/2, Train loss=0.4048\n  Epoch 1/2, Train loss=0.6232\n  Epoch 2/2, Train loss=0.4196\n  Epoch 1/2, Train loss=0.7532\n  Epoch 2/2, Train loss=0.5090\n  Epoch 1/2, Train loss=0.7779\n  Epoch 2/2, Train loss=0.5552\n  Epoch 1/2, Train loss=0.7408\n  Epoch 2/2, Train loss=0.5079\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.5580\n  Epoch 2/3, Train loss=0.3590\n  Epoch 3/3, Train loss=0.3080\nFold 4: Acc=0.907, P=0.907, R=0.907, F1=0.907\n\n=== Outer Fold 5/10 ===\n  Epoch 1/2, Train loss=0.5674\n  Epoch 2/2, Train loss=0.4207\n  Epoch 1/2, Train loss=0.5998\n  Epoch 2/2, Train loss=0.4183\n  Epoch 1/2, Train loss=0.5776\n  Epoch 2/2, Train loss=0.4086\n  Epoch 1/2, Train loss=0.6378\n  Epoch 2/2, Train loss=0.4365\n  Epoch 1/2, Train loss=0.6351\n  Epoch 2/2, Train loss=0.4271\n  Epoch 1/2, Train loss=0.6078\n  Epoch 2/2, Train loss=0.3789\n  Epoch 1/2, Train loss=0.7692\n  Epoch 2/2, Train loss=0.5133\n  Epoch 1/2, Train loss=0.7538\n  Epoch 2/2, Train loss=0.5007\n  Epoch 1/2, Train loss=0.7633\n  Epoch 2/2, Train loss=0.4946\nBest LR = 3e-04\n  Epoch 1/3, Train loss=0.6855\n  Epoch 2/3, Train loss=0.4440\n  Epoch 3/3, Train loss=0.3663\nFold 5: Acc=0.927, P=0.927, R=0.927, F1=0.927\n\n=== Outer Fold 6/10 ===\n  Epoch 1/2, Train loss=0.6277\n  Epoch 2/2, Train loss=0.4394\n  Epoch 1/2, Train loss=0.6120\n  Epoch 2/2, Train loss=0.4255\n  Epoch 1/2, Train loss=0.5544\n  Epoch 2/2, Train loss=0.4193\n  Epoch 1/2, Train loss=0.6421\n  Epoch 2/2, Train loss=0.4411\n  Epoch 1/2, Train loss=0.6244\n  Epoch 2/2, Train loss=0.4063\n  Epoch 1/2, Train loss=0.6066\n  Epoch 2/2, Train loss=0.4092\n  Epoch 1/2, Train loss=0.7630\n  Epoch 2/2, Train loss=0.5410\n  Epoch 1/2, Train loss=0.7187\n  Epoch 2/2, Train loss=0.4688\n  Epoch 1/2, Train loss=0.7698\n  Epoch 2/2, Train loss=0.5005\nBest LR = 3e-03\n  Epoch 1/3, Train loss=0.5469\n  Epoch 2/3, Train loss=0.3940\n  Epoch 3/3, Train loss=0.3459\nFold 6: Acc=0.902, P=0.908, R=0.902, F1=0.905\n\n=== Outer Fold 7/10 ===\n  Epoch 1/2, Train loss=0.6093\n  Epoch 2/2, Train loss=0.4422\n  Epoch 1/2, Train loss=0.5725\n  Epoch 2/2, Train loss=0.4164\n  Epoch 1/2, Train loss=0.5809\n  Epoch 2/2, Train loss=0.4291\n  Epoch 1/2, Train loss=0.6088\n  Epoch 2/2, Train loss=0.3971\n  Epoch 1/2, Train loss=0.6182\n  Epoch 2/2, Train loss=0.4110\n  Epoch 1/2, Train loss=0.6190\n  Epoch 2/2, Train loss=0.4207\n  Epoch 1/2, Train loss=0.7398\n  Epoch 2/2, Train loss=0.5003\n  Epoch 1/2, Train loss=0.8073\n  Epoch 2/2, Train loss=0.5323\n  Epoch 1/2, Train loss=0.7233\n  Epoch 2/2, Train loss=0.4874\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.5576\n  Epoch 2/3, Train loss=0.3738\n  Epoch 3/3, Train loss=0.3244\nFold 7: Acc=0.895, P=0.903, R=0.895, F1=0.899\n\n=== Outer Fold 8/10 ===\n  Epoch 1/2, Train loss=0.5896\n  Epoch 2/2, Train loss=0.4226\n  Epoch 1/2, Train loss=0.6212\n  Epoch 2/2, Train loss=0.4139\n  Epoch 1/2, Train loss=0.6418\n  Epoch 2/2, Train loss=0.4659\n  Epoch 1/2, Train loss=0.6305\n  Epoch 2/2, Train loss=0.4250\n  Epoch 1/2, Train loss=0.6255\n  Epoch 2/2, Train loss=0.4073\n  Epoch 1/2, Train loss=0.6134\n  Epoch 2/2, Train loss=0.3988\n  Epoch 1/2, Train loss=0.8082\n  Epoch 2/2, Train loss=0.5475\n  Epoch 1/2, Train loss=0.7675\n  Epoch 2/2, Train loss=0.5025\n  Epoch 1/2, Train loss=0.7363\n  Epoch 2/2, Train loss=0.5074\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.5426\n  Epoch 2/3, Train loss=0.3541\n  Epoch 3/3, Train loss=0.3018\nFold 8: Acc=0.926, P=0.926, R=0.926, F1=0.926\n\n=== Outer Fold 9/10 ===\n  Epoch 1/2, Train loss=0.5928\n  Epoch 2/2, Train loss=0.3975\n  Epoch 1/2, Train loss=0.5930\n  Epoch 2/2, Train loss=0.4249\n  Epoch 1/2, Train loss=0.5625\n  Epoch 2/2, Train loss=0.3890\n  Epoch 1/2, Train loss=0.5979\n  Epoch 2/2, Train loss=0.3937\n  Epoch 1/2, Train loss=0.5989\n  Epoch 2/2, Train loss=0.3856\n  Epoch 1/2, Train loss=0.6485\n  Epoch 2/2, Train loss=0.4255\n  Epoch 1/2, Train loss=0.8245\n  Epoch 2/2, Train loss=0.5686\n  Epoch 1/2, Train loss=0.7106\n  Epoch 2/2, Train loss=0.5002\n  Epoch 1/2, Train loss=0.7666\n  Epoch 2/2, Train loss=0.5033\nBest LR = 3e-03\n  Epoch 1/3, Train loss=0.5573\n  Epoch 2/3, Train loss=0.4023\n  Epoch 3/3, Train loss=0.3584\nFold 9: Acc=0.921, P=0.922, R=0.921, F1=0.921\n\n=== Outer Fold 10/10 ===\n  Epoch 1/2, Train loss=0.6010\n  Epoch 2/2, Train loss=0.4094\n  Epoch 1/2, Train loss=0.6610\n  Epoch 2/2, Train loss=0.4729\n  Epoch 1/2, Train loss=0.6212\n  Epoch 2/2, Train loss=0.4477\n  Epoch 1/2, Train loss=0.5882\n  Epoch 2/2, Train loss=0.3957\n  Epoch 1/2, Train loss=0.6130\n  Epoch 2/2, Train loss=0.3931\n  Epoch 1/2, Train loss=0.6172\n  Epoch 2/2, Train loss=0.4067\n  Epoch 1/2, Train loss=0.7607\n  Epoch 2/2, Train loss=0.4874\n  Epoch 1/2, Train loss=0.7773\n  Epoch 2/2, Train loss=0.5372\n  Epoch 1/2, Train loss=0.7711\n  Epoch 2/2, Train loss=0.5328\nBest LR = 1e-03\n  Epoch 1/3, Train loss=0.5504\n  Epoch 2/3, Train loss=0.3685\n  Epoch 3/3, Train loss=0.3088\nFold 10: Acc=0.883, P=0.891, R=0.883, F1=0.887\n\n=== Nested CV Results ===\nAccuracy : 0.907 ± 0.017\nPrecision: 0.912 ± 0.013\nRecall   : 0.907 ± 0.017\nF1-score : 0.909 ± 0.015\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pickle\n\ncv_results_A = {\n    'mean': deep_mean_A,\n    'std': deep_std_A,\n    'accuracy': deep_mean_A[0],\n    'precision': deep_mean_A[1],\n    'recall': deep_mean_A[2],\n    'f1': deep_mean_A[3]\n}\n\nwith open('/kaggle/working/cv_results_trainA.pkl', 'wb') as f:\n    pickle.dump(cv_results_A, f)\n\nprint(\"CV results saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T08:32:44.577792Z","iopub.execute_input":"2025-10-08T08:32:44.578537Z","iopub.status.idle":"2025-10-08T08:32:44.600646Z","shell.execute_reply.started":"2025-10-08T08:32:44.578510Z","shell.execute_reply":"2025-10-08T08:32:44.599856Z"}},"outputs":[{"name":"stdout","text":"CV results saved!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# After nested CV\nprint(\"\\n=== Training Final Model on Full Dataset (Clean Images) ===\")\n\nfinal_model_clean = train_final_model(\n    X, y,\n    model_builder=lambda: DeepCNN(num_classes),\n    lr=1e-3,\n    epochs=5,\n    batch=64,\n    device=device\n)\n\ntorch.save(final_model_clean.state_dict(), '/kaggle/working/deepcnn_final_clean.pth')\nprint(\"Final model saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T08:36:15.044454Z","iopub.execute_input":"2025-10-08T08:36:15.045019Z","iopub.status.idle":"2025-10-08T08:38:28.789110Z","shell.execute_reply.started":"2025-10-08T08:36:15.044993Z","shell.execute_reply":"2025-10-08T08:38:28.788431Z"}},"outputs":[{"name":"stdout","text":"\n=== Training Final Model on Full Dataset (Clean Images) ===\n  Epoch 1/5, Train loss=0.5589\n  Epoch 2/5, Train loss=0.3616\n  Epoch 3/5, Train loss=0.3032\n  Epoch 4/5, Train loss=0.2716\n  Epoch 5/5, Train loss=0.2443\nFinal model saved!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ===== LOAD AND PREPARE TEST DATA =====\nprint(\"\\n=== Loading Test Data (for Train_A Model) ===\")\ntest_data = np.load(os.path.join(data_dir, \"test.npz\"))\nX_test, y_test = test_data[\"X\"], test_data[\"y\"]\nprint(\"Loaded test data:\", X_test.shape, y_test.shape)\n\n# Encode labels (use encoder from train_A)\ny_test = encoder.transform(y_test)  # Make sure this is the encoder fitted on train_A\n\n# Normalize images\nX_test = X_test.astype(\"float32\") / 255.0\n\n# Reshape for PyTorch (N,C,H,W)\nX_test = np.transpose(X_test, (0,3,1,2))\ny_test = y_test.astype(\"int64\")\n\nprint(\"Test data prepared:\", X_test.shape)\n\n# ===== MAKE PREDICTIONS =====\nprint(\"\\n=== Evaluating Train_A Model on Test Set ===\")\n\ntest_ds = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\ntest_dl = DataLoader(test_ds, batch_size=64, shuffle=False)\n\nfinal_model_clean.eval()  # ← CHANGED: Use train_A model\ny_pred_test = []\n\nwith torch.no_grad():\n    for xb, _ in test_dl:\n        xb = xb.to(device)\n        probs = torch.softmax(final_model_clean(xb), 1)  # ← CHANGED\n        y_pred_test.append(torch.argmax(probs, 1).cpu().numpy())\n\ny_pred_test = np.concatenate(y_pred_test)\n\n# ===== CALCULATE METRICS =====\ncm_test = confusion_matrix_manual(y_test, y_pred_test, labels=np.unique(y_test))\nacc_test, prec_test, rec_test, f1_test = calc_metrics(cm_test)\n\nprint(f\"\\n=== Test Set Results (Train_A Model - Clean Training) ===\")\nprint(f\"Accuracy : {acc_test:.3f}\")\nprint(f\"Precision: {prec_test:.3f}\")\nprint(f\"Recall   : {rec_test:.3f}\")\nprint(f\"F1-score : {f1_test:.3f}\")\n\nprint(f\"\\nConfusion Matrix:\")\nprint(cm_test)\nprint(f\"Classes: {encoder.classes_}\")\n\n# ===== COMPARISON WITH CV =====\n\n# Load the saved CV results\nimport pickle\nwith open('/kaggle/working/cv_results_trainA.pkl', 'rb') as f:\n    cv_results_A = pickle.load(f)\n\n# Extract the variables you need\ndeep_mean_A = cv_results_A['mean']\ndeep_std_A = cv_results_A['std']\n\nprint(f\"\\n=== Performance Comparison (Train_A) ===\")\nprint(f\"CV Performance:   {deep_mean_A[0]:.3f} ± {deep_std_A[0]:.3f}\")  # ← Use train_A CV results\nprint(f\"Test Performance: {acc_test:.3f}\")\nprint(f\"Difference:       {acc_test - deep_mean_A[0]:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T08:43:07.693682Z","iopub.execute_input":"2025-10-08T08:43:07.694219Z","iopub.status.idle":"2025-10-08T08:43:15.157880Z","shell.execute_reply.started":"2025-10-08T08:43:07.694197Z","shell.execute_reply":"2025-10-08T08:43:15.157132Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading Test Data (for Train_A Model) ===\nLoaded test data: (3000, 224, 224, 3) (3000,)\nTest data prepared: (3000, 3, 224, 224)\n\n=== Evaluating Train_A Model on Test Set ===\n\n=== Test Set Results (Train_A Model - Clean Training) ===\nAccuracy : 0.696\nPrecision: 0.711\nRecall   : 0.696\nF1-score : 0.704\n\nConfusion Matrix:\n[[661 251  88]\n [100 764 136]\n [ 84 252 664]]\nClasses: ['Boot' 'Sandal' 'Shoe']\n\n=== Performance Comparison (Train_A) ===\nCV Performance:   0.907 ± 0.017\nTest Performance: 0.696\nDifference:       -0.211\n","output_type":"stream"}],"execution_count":19}]}